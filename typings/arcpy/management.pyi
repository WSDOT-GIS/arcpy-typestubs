"""
This type stub file was generated by pyright.
"""

from arcpy import Result
from arcpy.geoprocessing._base import gptooldoc

r"""The Data Management toolbox contains tools that are used to develop,
manage, and maintain feature classes, datasets, layers, and raster
data structures."""
__all__ = [
    "Add3DFormats",
    "AddAttachments",
    "AddAttributeRule",
    "AddCodedValueToDomain",
    "AddColormap",
    "AddContingentValue",
    "AddDataToTrajectoryDataset",
    "AddFeatureClassToTopology",
    "AddField",
    "AddFieldConflictFilter",
    "AddFields",
    "AddFilesToLasDataset",
    "AddGPSMetadataFields",
    "AddGeometryAttributes",
    "AddGlobalIDs",
    "AddIncrementingIDField",
    "AddIndex",
    "AddItemsToCatalogDataset",
    "AddJoin",
    "AddPortalItemsToCatalogDataset",
    "AddRastersToMosaicDataset",
    "AddRelate",
    "AddRuleToRelationshipClass",
    "AddRuleToTopology",
    "AddSpatialIndex",
    "AddSpatialJoin",
    "AddSubtype",
    "AddXY",
    "Adjust3DZ",
    "AlterAttributeRule",
    "AlterDomain",
    "AlterField",
    "AlterFieldGroup",
    "AlterMosaicDatasetSchema",
    "AlterVersion",
    "Analyze",
    "AnalyzeControlPoints",
    "AnalyzeDatasets",
    "AnalyzeMosaicDataset",
    "AnalyzeToolboxForVersion",
    "AnalyzeToolsForPro",
    "Append",
    "AppendAnnotation",
    "AppendControlPoints",
    "ApplyBlockAdjustment",
    "ApplySymbologyFromLayer",
    "AssignDefaultToField",
    "AssignDomainToField",
    "BatchBuildPyramids",
    "BatchCalculateStatistics",
    "BatchProject",
    "BatchUpdateFields",
    "BearingDistanceToLine",
    "BuildBoundary",
    "BuildFootprints",
    "BuildLasDatasetPyramid",
    "BuildMosaicDatasetItemCache",
    "BuildMultidimensionalInfo",
    "BuildMultidimensionalTranspose",
    "BuildOverviews",
    "BuildPyramids",
    "BuildPyramidsandStatistics",
    "BuildRasterAttributeTable",
    "BuildSeamlines",
    "BuildStereoModel",
    "CalculateCellSizeRanges",
    "CalculateDefaultClusterTolerance",
    "CalculateDefaultGridIndex",
    "CalculateEndTime",
    "CalculateField",
    "CalculateFields",
    "CalculateGeometryAttributes",
    "CalculateStatistics",
    "CalculateValue",
    "ChangePrivileges",
    "ChangeVersion",
    "CheckGeometry",
    "ClearPixelCache",
    "ClearWorkspaceCache",
    "Clip",
    "ColorBalanceMosaicDataset",
    "Compact",
    "CompareReplicaSchema",
    "CompositeBands",
    "Compress",
    "CompressFileGeodatabaseData",
    "ComputeBlockAdjustment",
    "ComputeCameraModel",
    "ComputeControlPoints",
    "ComputeDirtyArea",
    "ComputeFiducials",
    "ComputeMosaicCandidates",
    "ComputePansharpenWeights",
    "ComputeTiePoints",
    "ConfigureGeodatabaseLogFileTables",
    "ConsolidateLayer",
    "ConsolidateLocator",
    "ConsolidateMap",
    "ConsolidateProject",
    "ConsolidateToolbox",
    "ConvertCoordinateNotation",
    "ConvertRasterFunctionTemplate",
    "ConvertTimeField",
    "ConvertTimeZone",
    "Copy",
    "CopyFeatures",
    "CopyRaster",
    "CopyRows",
    "Create3DObjectSceneLayerPackage",
    "CreateBuildingSceneLayerPackage",
    "CreateCatalogDataset",
    "CreateCloudStorageConnectionFile",
    "CreateColorComposite",
    "CreateConnectionString",
    "CreateCustomGeoTransformation",
    "CreateDatabaseConnection",
    "CreateDatabaseConnectionString",
    "CreateDatabaseSequence",
    "CreateDatabaseUser",
    "CreateDatabaseView",
    "CreateDomain",
    "CreateEnterpriseGeodatabase",
    "CreateFeatureDataset",
    "CreateFeatureclass",
    "CreateFieldGroup",
    "CreateFileGDB",
    "CreateFishnet",
    "CreateFolder",
    "CreateIntegratedMeshSceneLayerPackage",
    "CreateLasDataset",
    "CreateMapTilePackage",
    "CreateMobileGDB",
    "CreateMobileMapPackage",
    "CreateMobileScenePackage",
    "CreateMosaicDataset",
    "CreateOrthoCorrectedRasterDataset",
    "CreatePansharpenedRasterDataset",
    "CreatePointCloudSceneLayerPackage",
    "CreatePointSceneLayerPackage",
    "CreateRandomPoints",
    "CreateRandomRaster",
    "CreateRasterDataset",
    "CreateRasterType",
    "CreateReferencedMosaicDataset",
    "CreateRelationshipClass",
    "CreateReplica",
    "CreateReplicaFromServer",
    "CreateRole",
    "CreateSQLiteDatabase",
    "CreateSceneLayerPackage",
    "CreateSpatialReference",
    "CreateSpatialType",
    "CreateTable",
    "CreateTopology",
    "CreateTrajectoryDataset",
    "CreateUnRegisteredFeatureclass",
    "CreateUnRegisteredTable",
    "CreateVectorTileIndex",
    "CreateVectorTilePackage",
    "CreateVersion",
    "CreateVersionedView",
    "CreateVoxelSceneLayerContent",
    "DefineMosaicDatasetNoData",
    "DefineOverviews",
    "DefineProjection",
    "Delete",
    "DeleteAttributeRule",
    "DeleteCodedValueFromDomain",
    "DeleteColormap",
    "DeleteDatabaseSequence",
    "DeleteDomain",
    "DeleteFeatures",
    "DeleteField",
    "DeleteFieldGroup",
    "DeleteIdentical",
    "DeleteMosaicDataset",
    "DeleteRasterAttributeTable",
    "DeleteRows",
    "DeleteSchemaGeodatabase",
    "DeleteVersion",
    "DetectFeatureChanges",
    "DiagnoseVersionMetadata",
    "DiagnoseVersionTables",
    "Dice",
    "DisableArchiving",
    "DisableAttachments",
    "DisableAttributeRules",
    "DisableCOGO",
    "DisableEditorTracking",
    "DisableFeatureBinning",
    "DisableReplicaTracking",
    "Dissolve",
    "DomainToTable",
    "DowngradeAttachments",
    "DownloadRasters",
    "EditRasterFunction",
    "Eliminate",
    "EliminatePolygonPart",
    "EnableArchiving",
    "EnableAttachments",
    "EnableAttributeRules",
    "EnableCOGO",
    "EnableEditorTracking",
    "EnableEnterpriseGeodatabase",
    "EnableFeatureBinning",
    "EnableReplicaTracking",
    "EncodeField",
    "EvaluateRules",
    "ExportAcknowledgementMessage",
    "ExportAttributeRules",
    "ExportContingentValues",
    "ExportDataChangeMessage",
    "ExportFrameAndCameraParameters",
    "ExportGeodatabaseConfigurationKeywords",
    "ExportMosaicDatasetGeometry",
    "ExportMosaicDatasetItems",
    "ExportMosaicDatasetPaths",
    "ExportRasterWorldFile",
    "ExportReplicaSchema",
    "ExportReportToPDF",
    "ExportTileCache",
    "ExportTopologyErrors",
    "ExportXMLWorkspaceDocument",
    "ExtractPackage",
    "ExtractSubDataset",
    "FeatureCompare",
    "FeatureEnvelopeToPolygon",
    "FeatureToLine",
    "FeatureToPoint",
    "FeatureToPolygon",
    "FeatureVerticesToPoints",
    "FieldStatisticsToTable",
    "FileCompare",
    "FindIdentical",
    "Flip",
    "GenerateAttachmentMatchTable",
    "GenerateBlockAdjustmentReport",
    "GenerateExcludeArea",
    "GenerateFgdbLicense",
    "GenerateLicensedFgdb",
    "GeneratePointCloud",
    "GeneratePointsAlongLines",
    "GenerateRasterCollection",
    "GenerateRasterFromRasterFunction",
    "GenerateRectanglesAlongLines",
    "GenerateTableFromRasterFunction",
    "GenerateTessellation",
    "GenerateTileCacheTilingScheme",
    "GenerateTransectsAlongLines",
    "GeoTaggedPhotosToPoints",
    "GeodeticDensify",
    "GetCellValue",
    "GetCount",
    "GetRasterProperties",
    "ImportAttributeRules",
    "ImportContingentValues",
    "ImportGeodatabaseConfigurationKeywords",
    "ImportMessage",
    "ImportMosaicDatasetGeometry",
    "ImportReplicaSchema",
    "ImportTileCache",
    "ImportXMLWorkspaceDocument",
    "Integrate",
    "InterpolateFromPointCloud",
    "JoinField",
    "LasDatasetStatistics",
    "LasPointStatsAsRaster",
    "MakeAggregationQueryLayer",
    "MakeBuildingLayer",
    "MakeFeatureLayer",
    "MakeImageServerLayer",
    "MakeLasDatasetLayer",
    "MakeMosaicLayer",
    "MakeQueryLayer",
    "MakeQueryTable",
    "MakeRasterLayer",
    "MakeSceneLayer",
    "MakeTableView",
    "MakeTinLayer",
    "MakeTrajectoryLayer",
    "MakeWCSLayer",
    "MakeXYEventLayer",
    "ManageFeatureBinCache",
    "ManageTileCache",
    "MatchControlPoints",
    "MatchLayerSymbologyToAStyle",
    "MatchPhotosToRowsByTime",
    "Merge",
    "MergeMosaicDatasetItems",
    "MigrateObjectIDTo64Bit",
    "MigrateRelationshipClass",
    "MigrateStorage",
    "MinimumBoundingGeometry",
    "Mirror",
    "Mosaic",
    "MosaicDatasetToMobileMosaicDataset",
    "MosaicToNewRaster",
    "MultipartToSinglepart",
    "PackageLayer",
    "PackageLocator",
    "PackageMap",
    "PackageProject",
    "PackageResult",
    "PivotTable",
    "PointsToLine",
    "PolygonToLine",
    "Project",
    "ProjectRaster",
    "RasterCompare",
    "RasterToDTED",
    "ReExportUnacknowledgedMessages",
    "RebuildIndexes",
    "RecalculateFeatureClassExtent",
    "ReclassifyField",
    "ReconcileVersion",
    "ReconcileVersions",
    "RecoverFileGDB",
    "RefreshExcel",
    "RegisterAsVersioned",
    "RegisterRaster",
    "RegisterWithGeodatabase",
    "Remove3DFormats",
    "RemoveAttachments",
    "RemoveContingentValue",
    "RemoveDomainFromField",
    "RemoveFeatureClassFromTopology",
    "RemoveFieldConflictFilter",
    "RemoveFilesFromLasDataset",
    "RemoveIndex",
    "RemoveJoin",
    "RemoveRastersFromMosaicDataset",
    "RemoveRelate",
    "RemoveRuleFromRelationshipClass",
    "RemoveRuleFromTopology",
    "RemoveSpatialIndex",
    "RemoveSubtype",
    "Rename",
    "ReorderAttributeRule",
    "RepairGeometry",
    "RepairMosaicDatasetPaths",
    "RepairTrajectoryDatasetPaths",
    "RepairVersionMetadata",
    "RepairVersionTables",
    "Resample",
    "Rescale",
    "Rotate",
    "SaveToLayerFile",
    "SaveToolboxToVersion",
    "SelectData",
    "SelectLayerByAttribute",
    "SelectLayerByLocation",
    "SetClusterTolerance",
    "SetDefaultSubtype",
    "SetFeatureClassSplitModel",
    "SetMosaicDatasetProperties",
    "SetRasterProperties",
    "SetRelationshipClassSplitPolicy",
    "SetSubtypeField",
    "SetValueForRangeDomain",
    "SharePackage",
    "Shift",
    "Sort",
    "SortCodedValueDomain",
    "SplitLine",
    "SplitLineAtPoint",
    "SplitMosaicDatasetItems",
    "SplitRaster",
    "StandardizeField",
    "SubdividePolygon",
    "SynchronizeChanges",
    "SynchronizeMosaicDataset",
    "TINCompare",
    "TableCompare",
    "TableToDomain",
    "TableToEllipse",
    "TableToRelationshipClass",
    "TransferFiles",
    "TransformField",
    "TransposeFields",
    "TrimArchiveHistory",
    "TruncateTable",
    "UncompressFileGeodatabaseData",
    "UnregisterAsVersioned",
    "UnregisterReplica",
    "UnsplitLine",
    "UpdateEnterpriseGeodatabaseLicense",
    "UpdateGeodatabaseConnectionPropertiesToBranch",
    "UpdateInteriorOrientation",
    "UpdatePortalDatasetOwner",
    "UpgradeAttachments",
    "UpgradeDataset",
    "UpgradeGDB",
    "UpgradeSceneLayer",
    "ValidateJoin",
    "ValidateSceneLayerPackage",
    "ValidateTopology",
    "Warp",
    "WarpFromFile",
    "WorkspaceToRasterDataset",
    "XYTableToPoint",
    "XYToLine",
]
__alias__ = ...

@gptooldoc("BuildMultidimensionalInfo_management", None)
def BuildMultidimensionalInfo(
    in_mosaic_dataset=...,
    variable_field=...,
    dimension_fields=...,
    variable_desc_units=...,
    delete_multidimensional_info=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildMultidimensionalInfo_management(in_mosaic_dataset, {variable_field}, {dimension_fields;dimension_fields...}, {variable_desc_units;variable_desc_units...}, {delete_multidimensional_info})

       Generates multidimensional metadata in the mosaic dataset, including
       information regarding variables and dimensions.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The input multidimensional mosaic dataset.
     variable_field {String}:
         The field in the mosaic dataset that stores the variable names and is
         used to populate a new field named Variable. If all rasters in the
         mosaic dataset represent the same variable, type the name of the
         variable, for example, Temperature.If the Variable field does not
         already exist, an existing field or
         string value must be specified. If the Variable field exists, the tool
         will update the multidimensional information only.
     dimension_fields {Value Table}:
         The fields in the mosaic dataset that store the dimension information
         and are used to populate a new field named Dimensions.If the
         Dimensions field already exists, the tool will update the
         multidimensional information only.
     variable_desc_units {Value Table}:
         Specify additional information about the Variable field.
     delete_multidimensional_info {Boolean}:
         Specifies whether existing multidimensional information will be
         deleted.

         * Unchecked-If multidimensional information exists in the mosaic
         dataset, it will not be deleted. This is the default.

         * Checked-If multidimensional information exists in the mosaic
         dataset, it will be deleted."""
    ...

@gptooldoc("BuildMultidimensionalTranspose_management", None)
def BuildMultidimensionalTranspose(
    in_multidimensional_raster=..., delete_transpose=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildMultidimensionalTranspose_management(in_multidimensional_raster, {delete_transpose})

       Transposes a multidimensional raster dataset, which divides the
       multidimensional data along each dimension to optimize performance
       when accessing pixel values across all slices.

    INPUTS:
     in_multidimensional_raster (Raster Layer):
         The input CRF multidimensional raster dataset.
     delete_transpose {Boolean}:
         Specifies whether an existing transpose will be deleted.

         * Unchecked-If a transpose exists, it will be overwritten by the newly
         built transpose. This is the default.

         * Checked-If a transpose exists, it will be deleted and no new
         transpose will be built."""
    ...

@gptooldoc("CalculateValue_management", None)
def CalculateValue(
    expression=..., code_block=..., data_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateValue_management(expression, {code_block}, {data_type})

       Returns a value based on a specified Python expression.

    INPUTS:
     expression (SQL Expression):
         The Python expression that will be evaluated.
     code_block {String}:
         A Python code block. The code in the code block must be referenced in
         the Expression parameter.
     data_type {String}:
         Specifies the data type of the return value of the Python expression.
         This parameter helps chain Calculate Value with other tools.

         * 3D Object Feature Layer-3D Object Feature Layer

         * Address Locator-Address Locator

         * Analysis Cell Size-Analysis Cell Size

         * Annotation Layer-Annotation Layer

         * Any Value-Any Value

         * ArcMap Document-ArcMap Document

         * Areal Unit-Areal Unit

         * BIM File Workspace-BIM File Workspace

         * Boolean-Boolean

         * Building Discipline Layer-Building Discipline Layer

         * Building Scene Discipline Layer-Building Scene Discipline Layer

         * Building Layer-Building Layer

         * Building Scene Layer-Building Scene Layer

         * CAD Drawing Dataset-CAD Drawing Dataset

         * Calculator Expression-Calculator Expression

         * Catalog Root-Catalog Root

         * Catalog Layer-Catalog Layer

         * Cell Size-Cell Size

         * Cell Size XY-Cell Size XY

         * Composite Layer-Composite Layer

         * Compression-Compression

         * Coordinate System-Coordinate System

         * Coordinate Systems Folder-Coordinate Systems Folder

         * Coverage-Coverage

         * Coverage Feature Class-Coverage Feature Class

         * Data Element-Data Element

         * Data File-Data File

         * Database Connections-Database Connections

         * Dataset-Dataset

         * Date-Date

         * dBASE Table-dBASE Table

         * Decimate-Decimate

         * Diagram Layer-Diagram Layer

         * Dimension Layer-Dimension Layer

         * Disk Connection-Disk Connection

         * Double-Double

         * Elevation Surface Layer-Elevation Surface Layer

         * Encrypted String-Encrypted String

         * Envelope-Envelope

         * Evaluation Scale-Evaluation Scale

         * Extent-Extent

         * Extract Values-Extract Values

         * Feature Class-Feature Class

         * Feature Dataset-Feature Dataset

         * Feature Layer-Feature Layer

         * Feature Set-Feature Set

         * Field-Field

         * Field Info-Field Info

         * Field Mappings-Field Mappings

         * File-File

         * Folder-Folder

         * Formulated Raster-Formulated Raster

         * Fuzzy Function-Fuzzy Function

         * GeoDataServer-GeoDataServer

         * Geodataset-Geodataset

         * Geometric Network-Geometric Network

         * Geostatistical Layer-Geostatistical Layer

         * Geostatistical Search Neighborhood-Geostatistical Search
         Neighborhood

         * Geostatistical Value Table-Geostatistical Value Table

         * GlobeServer-GlobeServer

         * GPServer-GPServer

         * Graph-Graph

         * Graph Data Table-Graph Data Table

         * Graphics Layer-Graphics Layer

         * Group Layer-Group Layer

         * Horizontal Factor-Horizontal Factor

         * Image Service-Image Service

         * Index-Index

         * INFO Expression-INFO Expression

         * INFO Item-INFO Item

         * INFO Table-INFO Table

         * Internet Tiled Layer-Internet Tiled Layer

         * KML Layer-KML Layer

         * LAS Dataset-LAS Dataset

         * LAS Dataset Layer-LAS Dataset Layer

         * Layer-Layer

         * Layer File-Layer File

         * Layout-Layout

         * Line-Line

         * Linear Unit-Linear Unit

         * Long-Long

         * M Domain-M Domain

         * Map-Map

         * Map Server-Map Server

         * Map Server Layer-Map Server Layer

         * Mosaic Dataset-Mosaic Dataset

         * Mosaic Layer-Mosaic Layer

         * Neighborhood-Neighborhood

         * Network Analyst Class FieldMap-Network Analyst Class FieldMap

         * Network Analyst Hierarchy Settings-Network Analyst Hierarchy
         Settings

         * Network Analyst Layer-Network Analyst Layer

         * Network Data Source-Network Data Source

         * Network Dataset-Network Dataset

         * Network Dataset Layer-Network Dataset Layer

         * Network Travel Mode-Network Travel Mode

         * Parcel Fabric-Parcel Fabric

         * Parcel Fabric for ArcMap-Parcel Fabric for ArcMap

         * Parcel Fabric Layer for ArcMap-Parcel Fabric Layer for ArcMap

         * Parcel Layer-Parcel Layer

         * Point-Point

         * Polygon-Polygon

         * Projection File-Projection File

         * Pyramid-Pyramid

         * Radius-Radius

         * Random Number Generator-Random Number Generator

         * Raster Band-Raster Band

         * Raster Calculator Expression-Raster Calculator Expression

         * Raster Catalog-Raster Catalog

         * Raster Catalog Layer-Raster Catalog Layer

         * Raster Data Layer-Raster Data Layer

         * Raster Dataset-Raster Dataset

         * Raster Layer-Raster Layer

         * Raster Statistics-Raster Statistics

         * Raster Type-Raster Type

         * Record Set-Record Set

         * Relationship Class-Relationship Class

         * Remap-Remap

         * Report-Report

         * Route Measure Event Properties-Route Measure Event Properties

         * Scene Layer-Scene Layer

         * Semivariogram-Semivariogram

         * ServerConnection-ServerConnection

         * Shapefile-Shapefile

         * Spatial Reference-Spatial Reference

         * SQL Expression-SQL Expression

         * String-String

         * String Hidden-String Hidden

         * Table-Table

         * Table View-Table View

         * Terrain Layer-Terrain Layer

         * Text File-Text File

         * Tile Size-Tile Size

         * Time Configuration-Time Configuration

         * Time Unit-Time Unit

         * TIN-TIN

         * TIN Layer-TIN Layer

         * Tool-Tool

         * Toolbox-Toolbox

         * Topo Features-Topo Features

         * Topology-Topology

         * Topology Layer-Topology Layer

         * Trace Network-Trace Network

         * Trace Network Layer-Trace Network Layer

         * Trajectory Layer-Trajectory Layer

         * Transformation Function-Transformation Function

         * Utility Network-Utility Network

         * Utility Network Layer-Utility Network Layer

         * Variant-Variant

         * Vector Tile Layer-Vector Tile Layer

         * Vertical Factor-Vertical Factor

         * Voxel Layer-Voxel Layer

         * VPF Coverage-VPF Coverage

         * VPF Table-VPF Table

         * WCS Coverage-WCS Coverage

         * Weighted Overlay Table-Weighted Overlay Table

         * Weighted Sum-Weighted Sum

         * WMS Map-WMS Map

         * WMTS Layer-WMTS Layer

         * Workspace-Workspace

         * XY Domain-XY Domain

         * Z Domain-Z Domain"""
    ...

@gptooldoc("CreateConnectionString_management", None)
def CreateConnectionString(
    in_data=..., data_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateConnectionString_management(in_data, {data_type})

       Create a connection string

    INPUTS:
     in_data (Data Element / Layer / Table View / Graph / Utility Network):
         Input Data Element
     data_type {String}:
         Data Type"""
    ...

@gptooldoc("MigrateObjectIDTo64Bit_management", None)
def MigrateObjectIDTo64Bit(
    in_datasets=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MigrateObjectIDTo64Bit_management(in_datasets;in_datasets...)

       Migrates the ObjectID to 64-Bit Precision

    INPUTS:
     in_datasets (Table View / Feature Dataset / Layer):
         Input Datasets"""
    ...

@gptooldoc("SelectData_management", None)
def SelectData(
    in_dataelement=..., out_dataelement=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SelectData_management(in_dataelement, {out_dataelement})

       The Select Data tool selects data in a parent data element such as a
       folder, geodatabase, feature dataset, or coverage.

    INPUTS:
     in_dataelement (Data Element / Composite Layer):
         The input data element can be a folder, geodatabase, or feature
         dataset.
     out_dataelement {String}:
         The child data element is contained by the input data element. Once
         the input data element is specified, the child data element control
         contains a drop-down list of the data elements contained in the input
         data element. For example, if the input is a feature dataset, all the
         feature classes within the feature dataset are included in the drop-
         down list. A single element is selected from this list."""
    ...

@gptooldoc("Add3DFormats_management", None)
def Add3DFormats(
    in_features=..., multipatch_materials=..., formats=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Add3DFormats_management(in_features, {multipatch_materials}, {formats;formats...})

       Converts a multipatch to a 3D object feature layer by linking the
       feature class with one or more 3D model formats.

    INPUTS:
     in_features (Table View):
         The input geodatabase multipatch feature that will be converted to a
         3D object feature layer.
     multipatch_materials {Boolean}:
         Specifies whether the multipatch geometry will be visualized using
         material information from the 3D models or the texture and color
         information stored with the multipatch.

         * MULTIPATCH_WITH_MATERIALS-The multipatch geometry will be visualized
         using the textures, colors, effects, and materials associated with the
         3D models. This is the default.

         * MULTIPATCH_WITHOUT_MATERIALS-The multipatch geometry will be
         visualized using the textures and colors defined for the multipatch.
     formats {String}:
         Specifies the 3D formats that will be associated with the multipatch
         features. Each input feature will be duplicated for each selected
         format. The available options depend on the codecs installed on the
         computer.

         * FMT3D_DAE-The COLLADA format will be added.

         * FMT3D_FBX-The Autodesk FilmBox format will be added.

         * FMT3D_GLTF-The JSON Graphics Library Transmission format will be
         added.

         * FMT3D_GLB-The binary Graphics Library Transmission format will be
         added.

         * FMT3D_OBJ-The Wavefront format will be added.

         * FMT3D_DWG-The DWG format will be added.

         * FMT3D_USDC-The Universal Scene Description format will be added.

         * FMT3D_USDZ-The compressed version of the Universal Scene
         Description format will be added."""
    ...

@gptooldoc("Remove3DFormats_management", None)
def Remove3DFormats(
    in_features=..., multipatch_materials=..., formats=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Remove3DFormats_management(in_features, {multipatch_materials}, {formats;formats...})

       Removes the 3D formats referenced by a 3D object feature layer.

    INPUTS:
     in_features (Table View):
         The multipatch feature class that was converted to a 3D object feature
         class.
     multipatch_materials {Boolean}:
         Specifies whether the multipatch geometry will be visualized using
         material information from the 3D models or the texture and color
         information stored with the multipatch.

         * MULTIPATCH_WITH_MATERIALS-The multipatch geometry will be visualized
         using the textures, colors, effects, and materials associated with the
         3D models. This is the default.

         * MULTIPATCH_WITHOUT_MATERIALS-The multipatch geometry will be
         visualized using the textures and colors defined for the multipatch.
     formats {String}:
         Specifies the 3D model formats referenced by the 3D object feature
         layer that will be removed. Only the formats that have been linked to
         the input features can be specified.

         * FMT3D_DAE-The COLLADA format will be removed.

         * FMT3D_FBX-The Autodesk FilmBox format will be removed.

         * FMT3D_GLTF-The JSON Graphics Library Transmission format will be
         removed.

         * FMT3D_GLB-The binary Graphics Library Transmission format will be
         removed.

         * FMT3D_OBJ-The Wavefront format will be removed.

         * FMT3D_DWG-The Autodesk drawing format will be removed."""
    ...

@gptooldoc("DisableArchiving_management", None)
def DisableArchiving(
    in_dataset=..., preserve_history=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableArchiving_management(in_dataset, {preserve_history})

       Disables archiving on a geodatabase feature class, table, or feature
       dataset.

    INPUTS:
     in_dataset (Table / Feature Class / Feature Dataset):
         The geodatabase feature class, table, or feature dataset for which
         archiving will be disabled.
     preserve_history {Boolean}:
         Specifies whether records that are not from the current moment will be
         preserved.If the table or feature class is versioned, the history
         table or
         feature will become enabled.For nonversioned data, a table or feature
         class will be created that
         contains the history information. The name of the new dataset will be
         the same as the input with an appended _h.

         * PRESERVE-Records that are not from the current moment will be
         preserved. This is the default.

         * DELETE-Records that are not from the current moment will not be
         preserved; they will be deleted."""
    ...

@gptooldoc("EnableArchiving_management", None)
def EnableArchiving(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableArchiving_management(in_dataset)

       Enables archiving on a table, feature class, or feature dataset.

    INPUTS:
     in_dataset (Table / Feature Class / Feature Dataset):
         The name of the dataset on which archiving will be enabled."""
    ...

@gptooldoc("TrimArchiveHistory_management", None)
def TrimArchiveHistory(
    in_table=..., trim_mode=..., trim_before_date=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TrimArchiveHistory_management(in_table, trim_mode, {trim_before_date})

       Deletes retired archive records from nonversioned archive-enabled
       datasets.

    INPUTS:
     in_table (Table View):
         The nonversioned archive-enabled table with the archive history to be
         trimmed.
     trim_mode (String):
         Specifies the trim mode that will be used to trim the archive
         history.At the current version of ArcGIS Pro, only the delete trim
         mode is
         available.

         * DELETE-The archive records will be deleted.
     trim_before_date {Date}:
         Archive records older than this date and time will be deleted. The
         date and time must be in UTC. If no date is provided, all archive
         records will be deleted."""
    ...

@gptooldoc("AddAttachments_management", None)
def AddAttachments(
    in_dataset=...,
    in_join_field=...,
    in_match_table=...,
    in_match_join_field=...,
    in_match_path_field=...,
    in_working_folder=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddAttachments_management(in_dataset, in_join_field, in_match_table, in_match_join_field, in_match_path_field, {in_working_folder})

       Adds file attachments to the records of a geodatabase feature class or
       table. The attachments are stored internally in the geodatabase in a
       separate attachment table that maintains linkage to the target
       dataset. Attachments are added to the target dataset using a match
       table that dictates for each input record (or an attribute group of
       records) the path to a file to add as an attachment to that record.

    INPUTS:
     in_dataset (Table View):
         Geodatabase table or feature class to add attachments to. Attachments
         are not added directly to this table, but rather to a related
         attachment table that maintains linkage to the input dataset.The input
         dataset must be stored in a version 10.0 or later
         geodatabase, and the table must have attachments enabled.
     in_join_field (Field):
         Field from the Input Dataset that has values that match the values in
         the Match Join Field. Records that have join field values that match
         between the Input Dataset and the Match Table will have attachments
         added. This field can be an Object ID field or any other identifying
         attribute.
     in_match_table (Table View):
         Table that identifies which input records will have attachments added
         and the paths to those attachments.
     in_match_join_field (Field):
         Field from the match table that indicates which records in the Input
         Dataset will have specified attachments added. This field can have
         values that match Input Dataset Object IDs or some other identifying
         attribute.
     in_match_path_field (Field):
         Field from the match table that contains paths to the attachments to
         add to Input Dataset records.
     in_working_folder {Folder}:
         Folder or workspace where attachment files are centralized. By
         specifying a working folder, the paths in the Match Path Field can be
         the short names of files relative to the working folder.For example,
         if loading attachments with paths like
         C:\\MyPictures\\image1.jpg, C:\\MyPictures\\image2.jpg, set the Working
         Folder to C:\\MyPictures, then paths in the Match Path Field can be the
         short names such as image1.jpg and image2.jpg, instead of the longer
         full paths."""
    ...

@gptooldoc("DisableAttachments_management", None)
def DisableAttachments(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableAttachments_management(in_dataset)

       Disables attachments on a geodatabase feature class or table. The tool
       deletes the attachment relationship class and attachment table.

    INPUTS:
     in_dataset (Table View):
         The geodatabase table or feature class for which attachments will be
         disabled. The input must be in a version 10 or later geodatabase."""
    ...

@gptooldoc("DowngradeAttachments_management", None)
def DowngradeAttachments(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DowngradeAttachments_management(in_dataset)

       Downgrades the attachments functionality of a feature class or table.

    INPUTS:
     in_dataset (Table View):
         The feature class or table that will have its attachments
         functionality downgraded."""
    ...

@gptooldoc("EnableAttachments_management", None)
def EnableAttachments(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableAttachments_management(in_dataset)

       Enables attachments on a geodatabase feature class or table. The tool
       creates the necessary attachment relationship class and attachment
       table that will store attachment files internally.

    INPUTS:
     in_dataset (Table View):
         The geodatabase table or feature class for which attachments will be
         enabled. The input must be in a version 10 or later geodatabase."""
    ...

@gptooldoc("GenerateAttachmentMatchTable_management", None)
def GenerateAttachmentMatchTable(
    in_dataset=...,
    in_folder=...,
    out_match_table=...,
    in_key_field=...,
    in_file_filter=...,
    in_use_relative_paths=...,
    match_pattern=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateAttachmentMatchTable_management(in_dataset, in_folder, out_match_table, in_key_field, {in_file_filter}, {in_use_relative_paths}, {match_pattern})

       Creates a match table to be used with the Add Attachments and Remove
       Attachments tools.

    INPUTS:
     in_dataset (Table View):
         A dataset containing records that will have files attached.
     in_folder (Folder):
         A folder containing files to attach.
     in_key_field (Field):
         The field from which values will be used to match the names of the
         files from the input folder. The matching behavior will compare field
         values with each file name, disregarding the file extension. This
         allows multiple files with various file extensions to match a single
         record in the input dataset.For example, a field value of lot5986 will
         match a file named
         lot5986.jpg if the match_pattern parameter value is EXACT.
     in_file_filter {String}:
         A data filter that will be used to limit the files considered for
         matching.Wild cards (*) can be used for more flexible filtering
         options.
         Multiple filters delimited by semicolons are also supported.For
         example, you have an input directory with a variety of file types.
         To limit the possible matches to only .jpg files, , use a value of
         *.jpg. To limit the possible matches to only .pdf and .doc files, use
         a value of *.pdf; *.doc.To limit possible matches to only file names
         that contain the text
         arc, use a value of *arc*.
     in_use_relative_paths {Boolean}:
         Specifies whether the output match table field FILENAME will contain
         full paths or only the file names.

         * RELATIVE-The field will contain only the file names (relative
         paths). This is the default.

         * ABSOLUTE-The field will contain full paths.
     match_pattern {String}:
         Specifies the type of match pattern that will be used to match file
         names with the specified key_field parameter value.

         * EXACT-File names that are an exact match to the values in the key
         field will be matched. This is the default.

         * PREFIX-File names that have the key field value at the beginning of
         the file name will be matched.

         * SUFFIX-File names that have the key field value at the end of the
         file name will be matched.

         * ANY-File names that have the key field value anywhere in the file
         name will be matched.

    OUTPUTS:
     out_match_table (Table):
         The output match table with the MATCHID and FILENAME fields."""
    ...

@gptooldoc("RemoveAttachments_management", None)
def RemoveAttachments(
    in_dataset=...,
    in_join_field=...,
    in_match_table=...,
    in_match_join_field=...,
    in_match_name_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveAttachments_management(in_dataset, in_join_field, in_match_table, in_match_join_field, {in_match_name_field})

       Removes attachments from geodatabase feature class or table records.

    INPUTS:
     in_dataset (Table View):
         A geodatabase table or feature class from which attachments will be
         removed. Attachments are not removed directly from this table; they
         are removed from the related attachment table that stores the
         attachments. The dataset must have attachments enabled.
     in_join_field (Field):
         A field from the in_dataset parameter value that contains values that
         match the values in the in_match_join_field parameter value. Records
         that have join field values that match the in_dataset parameter value
         and the in_match_table parameter value will have attachments removed.
         This field can be an Object ID field or any other identifying
         attribute.
     in_match_table (Table View):
         A table that identifies which input records will have attachments
         removed.
     in_match_join_field (Field):
         A field from the match table that indicates which records in the
         in_dataset parameter value will have specified attachments removed.
         This field can have values that match the in_dataset Object ID field
         or some other identifying attribute.
     in_match_name_field {Field}:
         A field from the match table that has the names of the attachments
         that will be removed from the in_dataset parameter value's records. If
         no name field is specified, all attachments will be removed from each
         record specified in the in_match_join_field parameter value. If a name
         field is specified but a record has a null or empty value in the name
         field, all attachments will be removed from that record. This field's
         values should be the short names of the attachments to remove, not the
         full paths to the files used to make the original attachments."""
    ...

@gptooldoc("UpgradeAttachments_management", None)
def UpgradeAttachments(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpgradeAttachments_management(in_dataset)

       Upgrades the attachments functionality on the data.

    INPUTS:
     in_dataset (Table View):
         The feature class with attachments enabled."""
    ...

@gptooldoc("AddAttributeRule_management", None)
def AddAttributeRule(
    in_table=...,
    name=...,
    type=...,
    script_expression=...,
    is_editable=...,
    triggering_events=...,
    error_number=...,
    error_message=...,
    description=...,
    subtype=...,
    field=...,
    exclude_from_client_evaluation=...,
    batch=...,
    severity=...,
    tags=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddAttributeRule_management(in_table, name, type, script_expression, {is_editable}, {triggering_events;triggering_events...}, {error_number}, {error_message}, {description}, {subtype}, {field}, {exclude_from_client_evaluation}, {batch}, {severity}, {tags;tags...})

       Adds an attribute rule to a dataset.

    INPUTS:
     in_table (Table View):
         The table or feature class that will have the new rule applied.
     name (String):
         A unique name for the new rule.
     type (String):
         Specifies the type of attribute rule that will be added.

         * CALCULATION-Attribute values will be automatically populated for
         features when another attribute is set on a feature. These rules are
         applied based on the triggering events specified. Long running
         calculations can be set to run in batch mode and will be evaluated at
         a user-defined time.When adding multiple calculation rules, the order
         in which the rules are added is important if there are circular
         dependencies. For example, Rule A calculates Field1 is equal to the
         value of $feature.Field2 + $feature.Field3, and Rule B calculates
         Field4 is equal to $feature.Field1 + $feature.Field5; the results of
         the calculation may be different depending on the order in which the
         rules are added.

         * CONSTRAINT-Permissible attribute configurations will be specified on
         a feature. When the constraint rule is violated, an error is generated
         and the feature is not stored. For example, if the value of Field A
         must be less than the sum of Field B and Field C, an error will be
         generated when that constraint is violated.

         * VALIDATION-Existing features will be identified with a batch
         validation process. Rules are evaluated at a user-defined time. When a
         rule is violated, an error feature is created.
     script_expression (Calculator Expression):
         The Arcade expression that defines the rule.
     is_editable {Boolean}:
         Specifies whether the attribute value can be edited. Attribute rules
         can be configured to either block or allow editors to edit the
         attribute values of the field being calculated. This parameter is only
         applicable for the calculation attribute rule type.

         * EDITABLE-The attribute value can be edited. This is the default.

         * NONEDITABLE-The attribute value cannot be edited.
     triggering_events {String}:
         Specifies the editing events that will trigger the attribute rule to
         take effect. This parameter is valid for calculation and constraint
         rule types only. At least one triggering event must be provided for
         calculation rules in which the batch parameter is set to NOT_BATCH.
         Triggering events are not applicable for calculation rules that have
         the batch parameter set to BATCH.

         * INSERT-The rule will be applied when a new feature is added.

         * UPDATE-The rule will be applied when a feature is updated.

         * DELETE-The rule will be applied when a feature is deleted.
     error_number {String}:
         An error number that will be returned when this rule is violated. This
         value is not required to be unique, so the same custom error number
         may be returned for multiple rules.This parameter is required for the
         constraint and validation rule
         types. It is optional for the calculation rule type.
     error_message {String}:
         An error message that will be returned when this rule is violated. It
         is recommended that you use a descriptive message to help the editor
         understand the violation when it occurs. The message is limited to
         2,000 characters.This parameter is required for the constraint and
         validation rule
         types. It is optional for the calculation rule type.
     description {String}:
         The description of the new attribute rule. The description is limited
         to 256 characters.
     subtype {String}:
         The subtype to which the rule will be applied if the dataset has
         subtypes.
     field {String}:
         The name of an existing field to which the rule will be applied. This
         parameter is only applicable for the calculation attribute rule type.
     exclude_from_client_evaluation {Boolean}:
         Specifies whether the application will evaluate the rule locally
         before applying the edits to the workspace.Not all clients have the
         capability to run all of the available rules
         so authors can exclude certain rules from client evaluation. For
         example, some rules may refer to data that has not been made available
         to all clients (reasons can include the data is offline, size, or
         security), or some rules may depend on the user or context (that is, a
         lightweight field update in a data collection app may not run a rule
         that requires additional user input or knowledge; however, a client
         such as ArcGIS Pro may support it).This parameter is not applicable
         for validation rule or calculation
         rule types if the batch parameter value is set to BATCH.

         * EXCLUDE-The rule will be excluded from client evaluation.

         * INCLUDE-The rule will not be excluded from client evaluation. This
         is the default.
     batch {Boolean}:
         Specifies whether the rule evaluation will be run in batch mode.

         * BATCH-The rule evaluation will be run in batch mode at a later time
         by running validate.

         * NOT_BATCH-The rule evaluation will not be run in batch mode.
         Triggering events will be used to determine when the rule is evaluated
         for insert, update, or delete operations. This is the default.
         Calculation rules can be either BATCH or NOT_BATCH. Validation rules
         are always BATCH for this parameter, and constraint rules are always
         NOT_BATCH.
     severity {Long}:
         The severity of the error.A value within the range of 1 through 5 can
         be chosen to define the
         severity of the rule. A value of 1 is high, being the most severe, and
         a value of 5 is low, being the least severe. For example, you can
         provide a low severity for a specific attribute rule and ignore the
         error during data production workflows, or set a high severity in
         which the error must be fixed for accuracy of data collected.This
         parameter is only applicable to validation rules.
     tags {String}:
         A set of tags that identify the rule (for searching and indexing) as a
         way to map to a functional requirement in a data model. To enter
         multiple tags, use a semicolon delimiter, for example, Tag1;Tag2;Tag3."""
    ...

@gptooldoc("AlterAttributeRule_management", None)
def AlterAttributeRule(
    in_table=...,
    name=...,
    description=...,
    error_number=...,
    error_message=...,
    tags=...,
    triggering_events=...,
    script_expression=...,
    exclude_from_client_evaluation=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterAttributeRule_management(in_table, name, {description}, {error_number}, {error_message}, {tags;tags...}, {triggering_events;triggering_events...}, {script_expression}, {exclude_from_client_evaluation})

       Alters the properties of an attribute rule.

    INPUTS:
     in_table (Table View):
         The table containing the attribute rule that will be altered.
     name (String):
         The name of the attribute rule that will be altered.
     description {String}:
         The description of the attribute rule. To keep the current value of
         the description, leave this parameter empty. To clear the current
         value of the description, use the RESET keyword.

         * RESET-Clear the value of the current rule description.
     error_number {String}:
         The error number of the attribute rule. To keep the current value of
         the error number, leave this parameter empty. To clear the current
         value of the error number for a calculation rule, use the RESET
         keyword. Error number is a required property for constraint and
         validation rules and cannot be cleared.

         * RESET-Clear the value of the current rule error number.
     error_message {String}:
         The error message of the attribute rule. To keep the current value of
         the error message, leave this parameter empty. To clear the current
         value of the error message for a calculation rule, use the RESET
         keyword. Error message is a required property for constraint and
         validation rules and cannot be cleared.

         * RESET-Clear the value of the current rule error message.
     tags {String}:
         The tags for the attribute rule. The new values will replace all
         existing tags; to keep any current tags, include them in this list.
         For multiple tags, use a semicolon delimiter, for example,
         Tag1;Tag2;Tag3. To keep the current tags, leave this parameter empty.
         To clear the current tags, use the RESET keyword.

         * RESET-Clear the tags for the rule.
     triggering_events {String}:
         Specifies the editing events that will trigger the attribute rule to
         take effect. Triggering events are only applicable for constraint
         rules and immediate calculation rules.The new values will replace
         existing triggering events. To keep the current triggering events,
         leave this parameter empty.

         * INSERT-The rule will be applied when a new feature is added.

         * UPDATE-The rule will be applied when a feature is updated.

         * DELETE-The rule will be applied when a feature is deleted.
     script_expression {Calculator Expression}:
         An Arcade expression that defines the rule. To keep the current
         expression, leave this parameter empty. If an expression is provided
         for this parameter, it will replace the existing Arcade expression of
         the rule. If you alter the script expression of a batch calculation or
         validation rule, the rule must be reevaluated.
     exclude_from_client_evaluation {Boolean}:
         Specifies whether the application will evaluate the rule locally
         before applying the edits to the workspace.The default for this
         parameter corresponds to the current value set
         for the rule. That is, if the input rule has the exclude from client
         evaluation parameter set to false, the default for this parameter will
         be INCLUDE so the rule will not be automatically excluded. This
         parameter is not applicable for validation rules or batch calculation
         rules.

         * EXCLUDE-The rule will be excluded from client evaluation.

         * INCLUDE-The rule will not be excluded from client evaluation."""
    ...

@gptooldoc("DeleteAttributeRule_management", None)
def DeleteAttributeRule(
    in_table=..., names=..., type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteAttributeRule_management(in_table, names;names..., {type})

       Deletes one or more attribute rules from a dataset.

    INPUTS:
     in_table (Table View):
         The table or feature class containing the attribute rules that will be
         deleted.
     names (String):
         The names of the rules that will be deleted from the dataset.
     type {String}:
         Specifies the type of attribute rules that will be deleted.

         * CALCULATION-Calculation rules will be deleted.

         * CONSTRAINT-Constraint rules will be deleted.

         * VALIDATION-Validation rules will be deleted."""
    ...

@gptooldoc("DisableAttributeRules_management", None)
def DisableAttributeRules(
    in_table=..., names=..., type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableAttributeRules_management(in_table, names;names..., {type})

       Disables one or more attribute rules for a dataset.

    INPUTS:
     in_table (Table View):
         The table or feature class that contains the attribute rule to be
         disabled.
     names (String):
         The names of the rules to disable for the dataset.
     type {String}:
         Specifies the type of attribute rules to disable. The tool will verify
         that the type of rule specified in this parameter matches the rule
         type specified. If they do not match, the rule will not be disabled.

         * CALCULATION-Disable a calculation rule.

         * CONSTRAINT-Disable a constraint rule.

         * VALIDATION-Disable a validation rule."""
    ...

@gptooldoc("EnableAttributeRules_management", None)
def EnableAttributeRules(
    in_table=..., names=..., type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableAttributeRules_management(in_table, names;names..., {type})

       Enables one or more attribute rules in a dataset

    INPUTS:
     in_table (Table View):
         The table or feature class that contains the attribute rule to be
         enabled.
     names (String):
         The names of the rules to enable for the dataset.
     type {String}:
         Specifies the type of attribute rules to enable. The tool will verify
         that the type of rule specified in this parameter matches the rule
         type specified. If they do not match, the rule will not be enabled.

         * CALCULATION-Enable a calculation rule.

         * CONSTRAINT-Enable a constraint rule.

         * VALIDATION-Enable a validation rule."""
    ...

@gptooldoc("EvaluateRules_management", None)
def EvaluateRules(
    in_workspace=..., evaluation_types=..., extent=..., run_async=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EvaluateRules_management(in_workspace, evaluation_types;evaluation_types..., {extent}, {run_async})

       Evaluates geodatabase rules and functionality.

    INPUTS:
     in_workspace (Workspace):
         A file geodatabase or feature service URL. An example of a feature
         service URL is
         https://myserver/server/rest/services/myservicename/FeatureServer.
     evaluation_types (String):
         Specifies the types of evaluation that will be used.

         * CALCULATION_RULES-Batch calculation attribute rules will be
         evaluated.

         * VALIDATION_RULES-Validation attribute rules will be evaluated.
     extent {Extent}:
         The extent to be evaluated. If there is a selection in the map, only
         selected features within the specified extent will be evaluated.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     run_async {Boolean}:
         Specifies whether the evaluation will run synchronously or
         asynchronously. This parameter is only supported when the input
         workspace is a feature service.

         * ASYNC-The evaluation will run asynchronously. This option dedicates
         server resources to run the evaluation with a longer time-out. Running
         asynchronously is recommended when evaluating large datasets that
         contain many features requiring calculation or validation. This is the
         default.

         * SYNC-The evaluation will run synchronously. This option has a
         shorter time-out and is best used when evaluating an extent with a
         small number of features requiring calculation or validation.
         In earlier releases, this parameter was named async. At ArcGIS Pro
         2.4, the parameter name was changed to run_async to avoid conflicts
         with the reserved Python keyword async."""
    ...

@gptooldoc("ExportAttributeRules_management", None)
def ExportAttributeRules(
    in_table=..., out_csv_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportAttributeRules_management(in_table, out_csv_file)

       Exports attribute rules from a dataset to a comma-separated values
       (.csv) file.

    INPUTS:
     in_table (Table View):
         The table or feature class from which the attribute rules will be
         exported.

    OUTPUTS:
     out_csv_file (File):
         The folder location and name of the .csv file to be created."""
    ...

@gptooldoc("ImportAttributeRules_management", None)
def ImportAttributeRules(
    target_table=..., csv_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportAttributeRules_management(target_table, csv_file)

       Imports attribute rules from a comma-separated values (.csv) file to a
       dataset.

    INPUTS:
     target_table (Table View):
         The table or feature class to which the attribute rules will be
         applied. The dataset must have all the features specified in the rule
         definition.
     csv_file (File):
         The .csv file containing the rules to import."""
    ...

@gptooldoc("ReorderAttributeRule_management", None)
def ReorderAttributeRule(
    in_table=..., name=..., evaluation_order=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReorderAttributeRule_management(in_table, name, evaluation_order)

       Reorders the evaluation order of an attribute rule.

    INPUTS:
     in_table (Table View):
         The table that contains the attribute rule with the evaluation order
         that will be updated.
     name (String):
         The name of the calculation rule that will have its evaluation order
         updated.
     evaluation_order (Long):
         The new evaluation order for the rule. For example, if there are five
         rules and a particular rule is in position 5 (the fifth order
         position, to be evaluated last) but you want it to be evaluated in
         position 2 (to be evaluated second), enter 2 for the value. The
         evaluation order for the rules after position 2 will be reassigned
         (that is, position 2 becomes position 3, position 3 becomes position
         4, and position 4 becomes position 5)."""
    ...

@gptooldoc("AddItemsToCatalogDataset_management", None)
def AddItemsToCatalogDataset(
    target_catalog_dataset=...,
    input_items=...,
    input_item_types=...,
    include_subfolders=...,
    footprint_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddItemsToCatalogDataset_management(target_catalog_dataset, input_items;input_items..., {input_item_types;input_item_types...}, {include_subfolders}, {footprint_type})

       Adds workspace items and layers-such as geodatabase datasets, raster
       layers, feature layers, mosaic layers, and other items-to an existing
       catalog dataset.

    INPUTS:
     target_catalog_dataset (Catalog Layer):
         The catalog dataset to which the items will be added.
     input_items (Workspace / Feature Layer / Image Service / Raster Layer / Mosaic Layer / LAS Dataset Layer / Layer File / CAD Drawing Dataset / ServerConnection / BIM File Workspace):
         The workspace items, layers, and files from which items will be added
         to the catalog dataset. The workspace can be a folder, file
         geodatabase, feature dataset, enterprise database, or a service from a
         server connection.
     input_item_types {String}:
         Specifies the item types that will be added to the catalog dataset
         from any input workspaces. All supported item types will be added by
         default.

         * BIM_FILE_WORKSPACE-BIM file workspaces will be added.

         * BIM_FILE_FLOORPLAN-BIM file floor plans will be added.

         * CAD_DRAWING-CAD drawings will be added.

         * FEATURE_CLASS-Feature classes will be added.

         * FEATURE_SERVICE-Feature services will be added.

         * IMAGE_SERVICE-Image services will be added.

         * LAS_DATASET-LAS datasets will be added.

         * LAS_FILE-LAS files will be added.

         * LAYER_FILE-Layer files will be added.

         * MAP_SERVICE-Map services will be added.

         * MOSAIC_DATASET-Mosaic datasets will be added.

         * RASTER_DATASET-Raster datasets will be added.

         * SCENE_LAYER_PACKAGE-Scene layer packages will be added.
     include_subfolders {Boolean}:
         Specifies whether the contents of folders or workspaces specified in
         the input_items parameter value will be recursively searched and added
         to the catalog dataset. This setting is not applicable to file or
         enterprise geodatabases.

         * INCLUDE_SUBFOLDERS-The contents of folders or workspaces will be
         recursively searched and added to the catalog dataset. This is the
         default.

         * NOT_INCLUDE_SUBFOLDERS-The contents of folders or workspaces will
         not be recursively searched and added to the catalog dataset.
     footprint_type {String}:
         Specifies whether the reference item's footprint will be the full
         extent or a convex hull representing the smallest convex polygon for
         all features.

         * ENVELOPE-The footprint will be a rectangle covering the full extent
         of the reference item. This is the default.

         * CONVEX_HULL-The footprint will be a convex hull enclosing all
         features from the reference item."""
    ...

@gptooldoc("AddPortalItemsToCatalogDataset_management", None)
def AddPortalItemsToCatalogDataset(
    target_catalog_dataset=...,
    input_portal_itemtypes=...,
    content=...,
    portal_folders=...,
    portal_groups=...,
    access_level=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddPortalItemsToCatalogDataset_management(target_catalog_dataset, {input_portal_itemtypes;input_portal_itemtypes...}, {content}, {portal_folders;portal_folders...}, {portal_groups;portal_groups...}, {access_level})

       Adds ArcGIS Online or ArcGIS Enterprise portal service items-such as
       feature, map, image, scene, and tile services-to an existing catalog
       dataset.

    INPUTS:
     target_catalog_dataset (Catalog Layer):
         The catalog dataset to which portal items will be added.
     input_portal_itemtypes {String}:
         Specifies the item types that will be added to the catalog dataset
         from the portal. All supported item types will be added by default.

         * FEATURE_SERVICE-Feature layers will be added. This option does not
         add feature collections.

         * IMAGE_SERVICE-Imagery layers will be added.

         * MAP_SERVICE-Map image and tile layers will be added.

         * SCENE_SERVICE-Scene layers will be added.

         * VECTOR_TILE_SERVICE-Vector tile layers will be added.

         * WFS-Web Feature Service (WFS) layers will be added.

         * WMS-Web Map Service (WMS) layers will be added.

         * WMTS-Web Map Tile Service (WMTS) layers will be added.
     content {String}:
         Specifies the collection in the active portal from which items will be
         added to the catalog dataset.

         * MY_CONTENT-Items from your My Content collection will be added.
         This is the default.

         * MY_GROUPS-Items from groups to which you belong will be added.

         * MY_ORGANIZATION-Items from your ArcGIS organization will be added.
     portal_folders {String}:
         The portal folders from which items will be added to the catalog
         dataset.
     portal_groups {String}:
         The portal groups from which items will be added to the catalog
         dataset.
     access_level {String}:
         Specifies the sharing level that portal items must have to be added to
         the catalog dataset.

         * PUBLIC-Items that are shared with the public will be added to the
         catalog dataset. This is the default.

         * ORG-Items that are shared with the organization, as well as items
         you own will be added to the catalog dataset. Items that are shared
         with the organization and one or more groups will also be added.

         * SHARED-Items shared with one or more groups, item owners, and those
         who have access to the item through group membership will be added to
         the catalog dataset.

         * PRIVATE-Items owned by you will be added to the catalog dataset.
         Only you or administrators who have access to your content can add
         these items."""
    ...

@gptooldoc("CreateCatalogDataset_management", None)
def CreateCatalogDataset(
    out_path=...,
    out_name=...,
    spatial_reference=...,
    template=...,
    has_z=...,
    out_alias=...,
    config_keyword=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateCatalogDataset_management(out_path, out_name, spatial_reference, {template;template...}, {has_z}, {out_alias}, {config_keyword})

       Creates a catalog dataset to which collections of layers, rasters,
       datasets, and other items can be added.

    INPUTS:
     out_path (Workspace / Feature Dataset):
         The enterprise or file geodatabase in which the output catalog dataset
         will be created.
     out_name (String):
         The name of the catalog dataset that will be created.
     spatial_reference (Spatial Reference):
         The spatial reference of the catalog dataset.
     template {Table View}:
         The feature class or table that will be used as a template to define
         the attribute fields of the new catalog dataset.
     has_z {String}:
         Specifies whether the catalog dataset will contain elevation values
         (z-values).

         * DISABLED-The output catalog dataset will not contain z-values. This
         is the default.

         * ENABLED-The output catalog dataset will contain z-values.

         * SAME_AS_TEMPLATE-The output catalog dataset will contain z-values if
         the dataset specified in the template parameter contains z-values.
     out_alias {String}:
         The alias name of the catalog dataset.
     config_keyword {String}:
         The configuration keyword determines the storage parameters of the
         database table. The configuration keyword applies to enterprise data
         only."""
    ...

@gptooldoc("AddContingentValue_management", None)
def AddContingentValue(
    target_table=..., field_group_name=..., values=..., subtype=..., retire_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddContingentValue_management(target_table, field_group_name, values;values..., {subtype}, {retire_value})

       Adds a contingent value to a field group on a feature class or table.

    INPUTS:
     target_table (Table View):
         The input geodatabase feature class or table to which the contingent
         value will be added.
     field_group_name (String):
         The field group to which the contingent value will be added.
     values (Value Table):
         The field name, field value type, and associated field values
         to be used for the new contingent attribute value.

         * Field Name-The name of the field that participates in the field
         group

         * Field Value Type-The type of contingent value. The ANY and
         NULL types will ignore any value specified in the Field Value
         parameter.

         * ANY-The value can be any field value.

         * NULL-The value is null.

         * CODED_VALUE-The value is from a coded value domain.

         * RANGE-The value is a min/max subset of a range domain.

         * Field Value-The specific field value. If the Field Value Type is
         CODED_VALUE, specify the code description. If the Field Value Type is
         RANGE, specify the minimum and maximum values in the format min;max
         (for example, 10;100).
     subtype {String}:
         The input table subtype to which the contingent value will be added.
     retire_value {Boolean}:
         Specifies whether the contingent value will be retired. The contingent
         value is considered retired when it is no longer created but can still
         be used in an existing field. When a contingent value is retired, it
         will still be shown in the list of valid values for a field, such as
         in the Attribute pane, but it will be disabled and you won't be able
         to select it as a field value. An example is using asbestos as a
         building material. New construction cannot use asbestos as a building
         material, but existing structures may still have this attribute.

         * RETIRE-The contingent value will be retired.

         * DO_NOT_RETIRE-The contingent value will not be retired. This is the
         default."""
    ...

@gptooldoc("AlterFieldGroup_management", None)
def AlterFieldGroup(
    target_table=..., name=..., new_name=..., fields=..., is_restrictive=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterFieldGroup_management(target_table, name, {new_name}, {fields;fields...}, {is_restrictive})

       Alters the properties of a field group.

    INPUTS:
     target_table (Table View):
         The table containing the field group to be altered.
     name (String):
         The name of the field group to be altered.
     new_name {String}:
         The new, unique name for the field group.
     fields {String}:
         The fields that participate in the field group. To modify the fields,
         enter new field names. Provided values will replace, not append, the
         current list of fields that participates in the field group. If no
         values are provided, the fields will not be altered.
     is_restrictive {Boolean}:
         Specifies whether the field group is restrictive. This parameter
         allows you to control the editing experience when using contingent
         values.

         * RESTRICT-The field group is restrictive. Values entered on a field
         in the field group are restricted to those specified as contingent
         values. This is the default.

         * DO_NOT_RESTRICT-The field group is not restrictive. Values can be
         committed to a field in a field group even if they are not specified
         as contingent values."""
    ...

@gptooldoc("CreateFieldGroup_management", None)
def CreateFieldGroup(
    target_table=..., name=..., fields=..., is_restrictive=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFieldGroup_management(target_table, name, fields;fields..., {is_restrictive})

       Create a field group for a feature class or table. Field groups are
       used when creating contingent values.

    INPUTS:
     target_table (Table View):
         The input geodatabase table or feature class in which the field group
         will be created.
     name (String):
         The name of the field group that will be created. This name must be
         unique to the feature class or table that will contain the field
         group.
     fields (String):
         The names of the fields in the field group.
     is_restrictive {Boolean}:
         Specifies if the field group is restrictive. This parameter allows you
         to control the editing experience when using contingent values.

         * RESTRICT-The field group is restrictive. Values entered on a field
         in the field group are restricted to those specified as contingent
         values. This is the default.

         * DO_NOT_RESTRICT-The field group is not restrictive. Values can be
         committed to a field in a field group even if they are not specified
         as contingent values."""
    ...

@gptooldoc("DeleteFieldGroup_management", None)
def DeleteFieldGroup(
    target_table=..., name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteFieldGroup_management(target_table, name)

       Deletes a field group from a table or feature class.

    INPUTS:
     target_table (Table View):
         The input geodatabase feature class or table that will have the field
         group deleted.
     name (String):
         The name of the field group that will be deleted."""
    ...

@gptooldoc("ExportContingentValues_management", None)
def ExportContingentValues(
    target_table=..., field_groups_file=..., contingent_values_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportContingentValues_management(target_table, field_groups_file, contingent_values_file)

       Exports field groups and contingent values to a .csv file.

    INPUTS:
     target_table (Table View):
         The input geodatabase table or feature class from which the field
         groups and contingent values will be exported.

    OUTPUTS:
     field_groups_file (File):
         The location and name of the output .csv file that will be created
         with specific column names containing information about the field
         groups of the target table.
     contingent_values_file (File):
         The location and name of the output .csv file that will be created
         with specific column names containing information about the contingent
         values of the target table."""
    ...

@gptooldoc("ImportContingentValues_management", None)
def ImportContingentValues(
    target_table=..., field_group_file=..., contingent_value_file=..., import_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportContingentValues_management(target_table, field_group_file, contingent_value_file, {import_type})

       Imports multiple contingent values and field groups from a comma-
       separated values file (.csv) into a dataset.

    INPUTS:
     target_table (Table View):
         The input geodatabase table or feature class to which the field groups
         and contingent values will be imported.
     field_group_file (File):
         A .csv file with specific column names that contains information about
         the field groups.
     contingent_value_file (File):
         A .csv file with specific column names that contains information about
         the contingent values.
     import_type {Boolean}:
         Specifies whether existing values will be replaced or merged upon
         import.

         * REPLACE-Existing values for the target table will be replaced with
         the values in the input .csv files.

         * UNION-Existing values will be merged with the values in the input
         .csv files. Any duplicates will be excluded. This is the default."""
    ...

@gptooldoc("RemoveContingentValue_management", None)
def RemoveContingentValue(
    target_table=..., id=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveContingentValue_management(target_table, id)

       Removes a contingent value from a field group.

    INPUTS:
     target_table (Table View):
         The input geodatabase feature class or table containing the contingent
         value that will be removed.
     id (String):
         The unique contingent value ID.To view the contingent value ID in the
         Contingent Values view, click
         the Toggle Value IDs button on the ribbon. In Python, this value can
         be accessed using the arcpy.da.ListContingentValues function."""
    ...

@gptooldoc("DetectFeatureChanges_management", None)
def DetectFeatureChanges(
    update_features=...,
    base_features=...,
    out_feature_class=...,
    search_distance=...,
    match_fields=...,
    out_match_table=...,
    change_tolerance=...,
    compare_fields=...,
    compare_line_direction=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DetectFeatureChanges_management(update_features, base_features, out_feature_class, search_distance, {match_fields;match_fields...}, {out_match_table}, {change_tolerance}, {compare_fields;compare_fields...}, {compare_line_direction})

       Finds where the update line features spatially match the base line
       features and detects spatial changes, attribute changes, or both, as
       well as no change. It then creates an output feature class containing
       matched update features with information about their changes,
       unmatched update features, and unmatched base features.

    INPUTS:
     update_features (Feature Layer):
         The line features that will be compared to the base features.
     base_features (Feature Layer):
         The line features that will be compared to the update features for
         change detection.
     search_distance (Linear Unit):
         The distance used to search for match candidates. A distance must be
         specified and it must be greater than zero. You can choose a preferred
         unit; the default is the feature unit.
     match_fields {Value Table}:
         The match fields from the update and base features. If specified, each
         pair of fields are compared for match candidates to help determine the
         right match.
     change_tolerance {Linear Unit}:
         The distance used to determine if there is a spatial change. All
         matched update features and base features are compared to this
         tolerance. If any portions of the update or the base features fall
         outside the zone around the matched feature, it is considered a
         spatial change. The value must be greater than the XY Tolerance of the
         input data so this process can be performed and the output will
         include the LEN_PCT and LEN_ABS fields. The default is 0, meaning this
         process is not performed. Any value between 0 and the data's XY
         Tolerance (inclusively) will make the process irrelevant and will be
         replaced by 0. You can choose a unit; the default is the feature unit.
     compare_fields {Value Table}:
         The fields that will determine if there is an attribute change between
         the matched update and base features.
     compare_line_direction {Boolean}:
         Specifies whether line directions will be compared for matched
         features.

         * NO_COMPARE_DIRECTION-Line directions will not be compared for
         matched features. This is the default.

         * COMPARE_DIRECTION-Line directions will be compared for matched
         features.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output line feature class containing the change information. The
         output contains all participating update features (matched and
         unmatched) and any unmatched base features.
     out_match_table {Table}:
         The output table containing complete feature matching information."""
    ...

@gptooldoc("FeatureCompare_management", None)
def FeatureCompare(
    in_base_features=...,
    in_test_features=...,
    sort_field=...,
    compare_type=...,
    ignore_options=...,
    xy_tolerance=...,
    m_tolerance=...,
    z_tolerance=...,
    attribute_tolerances=...,
    omit_field=...,
    continue_compare=...,
    out_compare_file=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureCompare_management(in_base_features, in_test_features, sort_field;sort_field..., {compare_type}, {ignore_options;ignore_options...}, {xy_tolerance}, {m_tolerance}, {z_tolerance}, {attribute_tolerances;attribute_tolerances...}, {omit_field;omit_field...}, {continue_compare}, {out_compare_file})

       Compares two feature classes or layers and returns the comparison
       results.

    INPUTS:
     in_base_features (Feature Layer):
         The Input Base Features are compared with the Input Test Features.
         Input Base Features refers to data that you have declared valid. This
         base data has the correct geometry definitions, field definitions, and
         spatial reference.
     in_test_features (Feature Layer):
         The Input Test Features are compared against the Input Base Features.
         Input Test Features refers to data that you have made changes to by
         editing or compiling new features.
     sort_field (Value Table):
         The field or fields used to sort records in the Input Base Features
         and the Input Test Features. The records are sorted in ascending
         order. Sorting by a common field in both the Input Base Features and
         the Input Test Features ensures that you are comparing the same row
         from each input dataset.
     compare_type {String}:
         The comparison type. The default is All, which will compare all
         properties of the features being compared.

         * ALL-All properties of the feature classes will be compared. This is
         the default.

         * GEOMETRY_ONLY-Only the geometries of the feature classes will be
         compared.

         * ATTRIBUTES_ONLY-Only the attributes and their values will be
         compared.

         * SCHEMA_ONLY-Only the schema of the feature classes will be compared.

         * SPATIAL_REFERENCE_ONLY-Only the spatial references of the two
         feature classes will be compared.
     ignore_options {String}:
         These properties will not be compared.

         * IGNORE_M-Do not compare measure properties.

         * IGNORE_Z-Do not compare elevation properties.

         * IGNORE_POINTID-Do not compare point ID properties.

         * IGNORE_EXTENSION_PROPERTIES-Do not compare extension properties.

         * IGNORE_SUBTYPES-Do not compare subtypes.

         * IGNORE_RELATIONSHIPCLASSES-Do not compare relationship classes.

         * IGNORE_REPRESENTATIONCLASSES-Do not compare representation classes.

         * IGNORE_FIELDALIAS-Do not compare field aliases.
     xy_tolerance {Linear Unit}:
         The distance that determines the range in which features are
         considered equal. To minimize error, the value you choose for the
         compare tolerance should be as small as possible. By default, the
         compare tolerance is the XY tolerance of the input base features.
     m_tolerance {Double}:
         The measure tolerance is the minimum distance between measures before
         they are considered equal.
     z_tolerance {Double}:
         The Z Tolerance is the minimum distance between z coordinates before
         they are considered equal.
     attribute_tolerances {Value Table}:
         The numeric value that determines the range in which attribute values
         are considered equal. This only applies to numeric field types.
     omit_field {String}:
         The field or fields that will be omitted during comparison. The field
         definitions and the tabular values for these fields will be ignored.
     continue_compare {Boolean}:
         Indicates whether to compare all properties after encountering the
         first mismatch.

         * NO_CONTINUE_COMPARE-Stops after encountering the first mismatch.
         This is the default.

         * CONTINUE_COMPARE-Compares other properties after encountering the
         first mismatch.

    OUTPUTS:
     out_compare_file {File}:
         This file will contain all similarities and differences between the
         in_base_features and the in_test_features. This file is a comma-
         delimited text file that can be viewed and used as a table in ArcGIS."""
    ...

@gptooldoc("FileCompare_management", None)
def FileCompare(
    in_base_file=...,
    in_test_file=...,
    file_type=...,
    continue_compare=...,
    out_compare_file=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FileCompare_management(in_base_file, in_test_file, {file_type}, {continue_compare}, {out_compare_file})

       Compares two files and returns the comparison results..

    INPUTS:
     in_base_file (File):
         The Input Base File is compared with the Input Test File. The Input
         Base File refers to a file that you have declared valid. This base
         file has the correct content and information.
     in_test_file (File):
         The Input Test File is compared against the Input Base File. The Input
         Test File refers to a file that you have made changes to by editing or
         compiling new information.
     file_type {String}:
         The type of files being compared.

         * ASCII-Compare using ASCII characters. This is the default.

         * BINARY-Perform a binary compare.
     continue_compare {Boolean}:
         Indicates whether to compare all properties after encountering the
         first mismatch.

         * NO_CONTINUE_COMPARE-Stops after encountering the first mismatch.
         This is the default.

         * CONTINUE_COMPARE-Compares other properties after encountering the
         first mismatch.

    OUTPUTS:
     out_compare_file {File}:
         This file will contain all similarities and differences between the
         Input Base File and the Input Test File. This file is a comma-
         delimited text file which can be viewed and used as a table in ArcGIS."""
    ...

@gptooldoc("RasterCompare_management", None)
def RasterCompare(
    in_base_raster=...,
    in_test_raster=...,
    compare_type=...,
    ignore_option=...,
    continue_compare=...,
    out_compare_file=...,
    parameter_tolerances=...,
    attribute_tolerances=...,
    omit_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RasterCompare_management(in_base_raster, in_test_raster, {compare_type}, {ignore_option;ignore_option...}, {continue_compare}, {out_compare_file}, {parameter_tolerances;parameter_tolerances...}, {attribute_tolerances;attribute_tolerances...}, {omit_field;omit_field...})

       Compares the properties of two raster datasets or two mosaic datasets.

    INPUTS:
     in_base_raster (Raster Layer / Mosaic Layer):
         The first raster or mosaic dataset to compare.
     in_test_raster (Raster Layer / Mosaic Layer):
         The second raster or mosaic dataset to compare with the first.
     compare_type {String}:
         Specifies the type of rasters that will be compared.

         * RASTER_DATASET-Two raster datasets will be compared.

         * GDB_RASTER_DATASET-Two raster datasets in a geodatabase will be
         compared.

         * MOSAIC_DATASET-Two mosaic datasets will be compared.
     ignore_option {String}:
         Specifies the properties that will be ignored in the comparison.

         * BandCount-The number of bands will be ignored.

         * Extent-The extent will be ignored.

         * Columns And Rows-The number of columns and rows will be ignored.

         * Pixel Type-The pixel type will be ignored.

         * NoData-The NoData value will be ignored.

         * Spatial Reference-The spatial reference system will be ignored.

         * Pixel Value-The pixel values will be ignored.

         * Colormap-Existing color maps will be ignored.

         * Raster Attribute Table-Existing attribute tables will be ignored.

         * Statistics-Statistics will be ignored.

         * Metadata-Metadata will be ignored.

         * Pyramids Exist-Existing pyramids will be ignored.

         * Compression Type-The compression type will be ignored.

         * Data Source Type-The data source type will be ignored.
     continue_compare {Boolean}:
         Specifies whether the comparison will stop if a mismatch is
         encountered.

         * NO_CONTINUE_COMPARE-The comparison will stop if a mismatch is
         encountered. This is the default.

         * CONTINUE_COMPARE-The comparison will continue if a mismatch is
         encountered.
     parameter_tolerances {Value Table}:
         The tolerances that determine the range in which values are considered
         equal. The same tolerance can be applied to all parameters, or
         different tolerances can be applied to individual parameters.The
         tolerance type can be either a value or a fraction. If the
         tolerance type is a fraction, the tolerance for each
         pixel will be different since each pixel has a different value. For
         example, if a tolerance fraction is set to 0.5, the tolerance will be
         calculated as follows:

         * If a pixel has a value of 0.2, the tolerance will be 0.1, since 0.5
         * 0.2 = 0.1.

         * If a pixel has a value of 3, the tolerance will be 1.5, since 0.5 *
         3 = 1.5.
     attribute_tolerances {Value Table}:
         The fields that will be compared to determine if they are within a
         tolerance. The tolerance value is a value in the units of the
         attribute.
     omit_field {String}:
         The field or fields that will be omitted during comparison.

    OUTPUTS:
     out_compare_file {File}:
         A text file containing the comparison results."""
    ...

@gptooldoc("TINCompare_management", None)
def TINCompare(
    in_base_tin=...,
    in_test_tin=...,
    compare_type=...,
    continue_compare=...,
    out_compare_file=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TINCompare_management(in_base_tin, in_test_tin, {compare_type}, {continue_compare}, {out_compare_file})

       Compares two TINs and returns the comparison results.

    INPUTS:
     in_base_tin (TIN Layer):
         The Input Base Tin is compared with the Input Test Tin. Input Base Tin
         refers to data that you have declared valid. This base data has the
         correct geometry, tag values (if any), and spatial reference.
     in_test_tin (TIN Layer):
         The Input Test Tin is compared against the Input Base Tin.
     compare_type {String}:
         The comparison type.

         * ALL-This is the default.

         * PROPERTIES_ONLY-Refers to both geometry and TIN tag values, if any,
         that are assigned to nodes and triangles.

         * SPATIAL_REFERENCE_ONLY-Coordinate system information.
     continue_compare {Boolean}:
         Indicates whether to compare all properties after encountering the
         first mismatch.

         * NO_CONTINUE_COMPARE-Stop after encountering the first mismatch. This
         is the default.

         * CONTINUE_COMPARE-Compare other properties after encountering the
         first mismatch.

    OUTPUTS:
     out_compare_file {File}:
         The name and path of the text file which will contain the comparison
         results."""
    ...

@gptooldoc("TableCompare_management", None)
def TableCompare(
    in_base_table=...,
    in_test_table=...,
    sort_field=...,
    compare_type=...,
    ignore_options=...,
    attribute_tolerances=...,
    omit_field=...,
    continue_compare=...,
    out_compare_file=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TableCompare_management(in_base_table, in_test_table, sort_field;sort_field..., {compare_type}, {ignore_options;ignore_options...}, {attribute_tolerances;attribute_tolerances...}, {omit_field;omit_field...}, {continue_compare}, {out_compare_file})

       Compares two tables or table views and returns the comparison results.

    INPUTS:
     in_base_table (Table View / Raster Layer):
         The Input Base Table is compared with the Input Test Table. The Input
         Base Table refers to tabular data that you have declared valid. The
         base data has the correct field definitions and attribute values.
     in_test_table (Table View / Raster Layer):
         The Input Test Table is compared against the Input Base Table. The
         Input Test Table refers to data that you have made changes to by
         editing or compiling new fields, new records, or new attribute values.
     sort_field (Value Table):
         The field or fields used to sort records in the Input Base Table and
         the Input Test Table. The records are sorted in ascending order.
         Sorting by a common field in both the Input Base Table and the Input
         Test Table ensures that you are comparing the same row from each input
         dataset.
     compare_type {String}:
         The comparison type. ALL is the default. The default will compare all
         properties of the tables being compared.

         * ALL-Compare all properties. This is the default.

         * ATTRIBUTES_ONLY-Only compare the attributes and their values.

         * SCHEMA_ONLY-Only compare the schema.
     ignore_options {String}:
         These properties will not be compared.

         * IGNORE_EXTENSION_PROPERTIES-Do not compare extension properties.

         * IGNORE_SUBTYPES-Do not compare subtypes.

         * IGNORE_RELATIONSHIPCLASSES-Do not compare relationship classes.

         * IGNORE_FIELDALIAS-Do not compare field aliases.
     attribute_tolerances {Value Table}:
         The numeric value that determines the range in which attribute values
         are considered equal. This only applies to numeric field types.
     omit_field {String}:
         The field or fields that will be omitted during comparison. The field
         definitions and the tabular values for these fields will be ignored.
     continue_compare {Boolean}:
         Indicates whether to compare all properties after encountering the
         first mismatch.

         * NO_CONTINUE_COMPARE-Stop after encountering the first mismatch. This
         is the default.

         * CONTINUE_COMPARE-Compare other properties after encountering the
         first mismatch.

    OUTPUTS:
     out_compare_file {File}:
         This file will contain all similarities and differences between the
         in_base_table and the in_test_table. This file is a comma-delimited
         text file that can be viewed and used as a table in ArcGIS."""
    ...

@gptooldoc("CompareReplicaSchema_management", None)
def CompareReplicaSchema(
    in_geodatabase=..., in_source_file=..., output_replica_schema_changes_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CompareReplicaSchema_management(in_geodatabase, in_source_file, output_replica_schema_changes_file)

       Generates an .xml file that describes schema differences between a
       replica geodatabase and the relative replica geodatabase.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase to which the replica schema will be compared.
         The geodatabase can be a local geodatabase or a geodata service.
     in_source_file (File):
         The file that contains the relative replica schema that will be used
         for the comparison.

    OUTPUTS:
     output_replica_schema_changes_file (File):
         The file that will contain a description of the schema differences."""
    ...

@gptooldoc("CreateReplica_management", None)
def CreateReplica(
    in_data=...,
    in_type=...,
    out_geodatabase=...,
    out_name=...,
    access_type=...,
    initial_data_sender=...,
    expand_feature_classes_and_tables=...,
    reuse_schema=...,
    get_related_data=...,
    geometry_features=...,
    archiving=...,
    register_existing_data=...,
    out_type=...,
    out_xml=...,
    all_records_for_tables=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateReplica_management(in_data;in_data..., in_type, {out_geodatabase}, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, {archiving}, {register_existing_data}, {out_type}, {out_xml}, {all_records_for_tables})

       Creates a replica in a geodatabase from a specified list of feature
       classes, layers, datasets, and tables in an enterprise geodatabase.

    INPUTS:
     in_data (Table View / Dataset):
         The data to be replicated. This list consists of layers and tables
         referencing versioned, editable data from an enterprise geodatabase.
     in_type (String):
         Specifies the type of replica that will be created.

         * TWO_WAY_REPLICA-Changes will be sent between child and parent
         replicas in both directions.

         * ONE_WAY_REPLICA-Changes will be sent from the parent replica to the
         child replica only.

         * CHECK_OUT-Data will be replicated, edited, and checked back in one
         time.

         * ONE_WAY_CHILD_TO_PARENT_REPLICA-Changes will be sent from the child
         replica to the parent replica only.
     out_geodatabase {Workspace / GeoDataServer}:
         The local geodatabase that will host the child replica. Geodata
         services are used to represent remote geodatabases. The geodatabase
         can be an enterprise or file geodatabase. For two-way replicas, the
         child geodatabase must be an enterprise geodatabase. For one-way and
         checkout replicas, the geodatabase can be a file or enterprise
         geodatabase. File geodatabases must exist before running this
         tool.This parameter is required if the out_type parameter is set to
         GEODATABASE.
     out_name (String):
         The name that identifies the replica.
     access_type {String}:
         Specifies the type of replica access.

         * FULL-Complex types such as topologies, are supported and the data
         must be versioned. This is the default.

         * SIMPLE-The data on the child is not versioned and must be simple.
         This allows the replica to be interoperable. Nonsimple features in the
         parent (for example, features in topologies) will be converted to
         simple features (for example, point, line, and polygon feature
         classes).
     initial_data_sender {String}:
         Specifies which replica will send changes when in disconnected mode.
         If you are working in a connected mode, this parameter is
         inconsequential. This ensures that the relative replica will not send
         updates until the changes are first received from the initial data
         sender.

         * CHILD_DATA_SENDER-The child replica will be the initial data sender.
         This is the default.

         * PARENT_DATA_SENDER-The parent replica will be the initial data
         sender.
     expand_feature_classes_and_tables {String}:
         Specifies whether expanded feature classes and tables-such as those in
         networks, topologies, or relationship classes-will be added.

         * USE_DEFAULTS-The expanded feature classes and tables related to the
         feature classes and tables in the replica will be added. The default
         for feature classes is to replicate all features intersecting the
         spatial filter. If no spatial filter has been provided, all features
         will be included. The default for tables is to replicate the schema
         only. This is the default.

         * ADD_WITH_SCHEMA_ONLY-Only the schema for the expanded feature
         classes and tables will be added.

         * ALL_ROWS-All rows for expanded feature classes and tables will be
         added.

         * DO_NOT_ADD-No expanded feature classes or tables will be added.
     reuse_schema {String}:
         Specifies whether a geodatabase that contains the schema of the data
         to be replicated will be reused. This reduces the amount of time
         required to replicate the data. This parameter is only available for
         checkout replicas.

         * DO_NOT_REUSE-Schema will not be reused. This is the default.

         * REUSE-Schema will be used.
     get_related_data {String}:
         Specifies whether rows related to rows existing in the replica will be
         replicated. For example, a feature (f1) is inside the replication
         filter and a related feature (f2) from another class is outside the
         filter. Feature f2 will be included in the replica if you choose to
         get related data.

         * DO_NOT_GET_RELATED-Related data will not be replicated.

         * GET_RELATED-Related data will be replicated. This is the default.
     geometry_features {Feature Layer}:
         The features that will be used to define the area to replicate.
     archiving {Boolean}:
         Specifies whether the archive class will be used to track changes
         instead of the versioning delta tables. This is only available for
         one-way replicas.

         * ARCHIVING-Archiving will be used to track changes.

         * DO_NOT_USE_ARCHIVING-Archiving will not be used to track changes.
         This is the default.
     register_existing_data {Boolean}:
         Specifies whether existing data in the child geodatabase will be used
         to define the replica datasets. The datasets in the child geodatabase
         must have the same names as the datasets in the parent geodatabase.

         * REGISTER_EXISTING_DATA-Existing data in the child geodatabase will
         be used to register the replica.

         * DO_NOT_USE_REGISTER_EXISTING_DATA-Data in the parent geodatabase
         will be copied to the child geodatabase. This is the default.
     out_type {String}:
         Specifies the output type of the data that will be replicated.

         * GEODATABASE-The data will be replicated to a geodatabase. This is
         the default.

         * XML_FILE-The data will be replicated to an XML workspace document.
     all_records_for_tables {Boolean}:
         Specifies whether all records or only the schema will be copied to the
         child geodatabase for tables that do not have filters applied (such as
         selections or definition queries).Tables with applied filters will be
         honored.

         * ALL_RECORDS_FOR_TABLES-For tables with no applied filters, all
         records will be copied to the child geodatabase. This option will
         override the expand_feature_classes_and_tables parameter value.

         * SCHEMA_ONLY_FOR_TABLES-For tables with no applied filters, only the
         schema will be copied to the child geodatabase. Tables with applied
         filters will be honored. This is the default.

    OUTPUTS:
     out_xml {File}:
         The name and location of the .xml file that will be created.This
         parameter is required if the out_type parameter is set to
         XML_FILE."""
    ...

@gptooldoc("CreateReplicaFromServer_management", None)
def CreateReplicaFromServer(
    in_geodataservice=...,
    datasets=...,
    in_type=...,
    out_geodatabase=...,
    out_name=...,
    access_type=...,
    initial_data_sender=...,
    expand_feature_classes_and_tables=...,
    reuse_schema=...,
    get_related_data=...,
    geometry_features=...,
    archiving=...,
    all_records_for_tables=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateReplicaFromServer_management(in_geodataservice, datasets;datasets..., in_type, out_geodatabase, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, archiving, {all_records_for_tables})

       Creates a replica using a specified list of feature classes, layers,
       feature datasets, and tables from a remote geodatabase using a geodata
       service published on ArcGIS Server.

    INPUTS:
     in_geodataservice (GeoDataServer):
         The geodata service representing the geodatabase from which the
         replica will be created. The geodatabase referenced by the geodata
         service must be an enterprise geodatabase.
     datasets (String):
         The list of the feature datasets, stand-alone feature classes, tables,
         and stand-alone attributed relationship classes from the geodata
         service to be replicated.
     in_type (String):
         Specifies the type of replica that will be created.

         * TWO_WAY_REPLICA-Changes will be sent between child and parent
         replicas in both directions.

         * ONE_WAY_REPLICA-Changes will be sent from the parent replica to the
         child replica only.

         * CHECK_OUT-Data will be replicated, edited, and checked back in one
         time.

         * ONE_WAY_CHILD_TO_PARENT_REPLICA-Changes will be sent from the child
         replica to the parent replica only.
     out_geodatabase (Workspace / GeoDataServer):
         The local geodatabase that will host the child replica. Geodata
         services are used to represent remote geodatabases. The geodatabase
         can be an enterprise or file geodatabase. For two-way replicas, the
         child geodatabase must be an enterprise geodatabase. For one-way and
         check-out replicas, the geodatabase can be a file or enterprise
         geodatabase. File geodatabases must exist before running this tool.
     out_name (String):
         The name that identifies the replica.
     access_type {String}:
         Specifies the type of replica access.

         * FULL-Complex types such as topologies, are supported and the data
         must be versioned. This is the default.

         * SIMPLE-The data on the child is not versioned and must be simple.
         This allows the replica to be interoperable. Nonsimple features in the
         parent (for example, features in topologies) will be converted to
         simple features (for example, point, line, and polygon feature
         classes).
     initial_data_sender {String}:
         Specifies which replica will send changes when in disconnected mode.
         If you are working in a connected mode, this parameter is
         inconsequential. This ensures that the relative replica will not send
         updates until the changes are first received from the initial data
         sender.

         * CHILD_DATA_SENDER-The child replica will be the initial data sender.
         This is the default.

         * PARENT_DATA_SENDER-The parent replica will be the initial data
         sender.
     expand_feature_classes_and_tables {String}:
         Specifies whether expanded feature classes and tables-such as those in
         networks, topologies, or relationship classes-will be added.

         * USE_DEFAULTS-The expanded feature classes and tables related to the
         feature classes and tables in the replica will be added. The default
         for feature classes is to replicate all features intersecting the
         spatial filter. If no spatial filter has been provided, all features
         will be included. The default for tables is to replicate the schema
         only. This is the default.

         * ADD_WITH_SCHEMA_ONLY-Only the schema for the expanded feature
         classes and tables will be added.

         * ALL_ROWS-All rows for expanded feature classes and tables will be
         added.

         * DO_NOT_ADD-No expanded feature classes or tables will be added.
     reuse_schema {String}:
         Specifies whether a geodatabase that contains the schema of the data
         to be replicated will be reused. This reduces the amount of time
         required to replicate the data. This parameter is only available for
         checkout replicas.

         * DO_NOT_REUSE-Schema will not be reused. This is the default.

         * REUSE-Schema will be used.
     get_related_data {String}:
         Specifies whether rows related to rows existing in the replica will be
         replicated. For example, a feature (f1) is inside the replication
         filter and a related feature (f2) from another class is outside the
         filter. Feature f2 will be included in the replica if you choose to
         get related data.

         * DO_NOT_GET_RELATED-Related data will not be replicated.

         * GET_RELATED-Related data will be replicated. This is the default.
     geometry_features {Feature Layer}:
         The features that will be used to define the area to replicate.
     archiving (Boolean):
         Specifies whether the archive class will be used to track changes
         instead of the versioning delta tables. This is only available for
         one-way replicas.

         * ARCHIVING-Archiving will be used to track changes.

         * DO_NOT_USE_ARCHIVING-Archiving will not be used to track changes.
         This is the default.
     all_records_for_tables {Boolean}:
         Specifies whether all records or only the schema will be copied to the
         child geodatabase for tables that do not have filters applied (such as
         selections or definition queries).Tables with applied filters will be
         honored.

         * ALL_RECORDS_FOR_TABLES-For tables with no applied filters, all
         records will be copied to the child geodatabase. This option will
         override the expand_feature_classes_and_tables parameter value.

         * SCHEMA_ONLY_FOR_TABLES-For tables with no applied filters, only the
         schema will be copied to the child geodatabase. Tables with applied
         filters will be honored. This is the default."""
    ...

@gptooldoc("DisableReplicaTracking_management", None)
def DisableReplicaTracking(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableReplicaTracking_management(in_dataset)

       Disables replica tracking on data.

    INPUTS:
     in_dataset (Table / Feature Class / Feature Dataset):
         The enterprise geodatabase table, feature class, feature dataset,
         attributed relationship class, or many-to-many relationship class on
         which to disable replica tracking."""
    ...

@gptooldoc("EnableReplicaTracking_management", None)
def EnableReplicaTracking(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableReplicaTracking_management(in_dataset)

       Enables replica tracking on data, allowing you to work with offline
       maps and in distributed collaborations. Replica tracking applies to
       services that are configured with the sync capability with the option
       of creating a version for each downloaded map.

    INPUTS:
     in_dataset (Table / Feature Class / Feature Dataset):
         The enterprise geodatabase table, feature class, feature dataset,
         attributed relationship class, or many-to-many relationship class on
         which to enable replica tracking."""
    ...

@gptooldoc("ExportAcknowledgementMessage_management", None)
def ExportAcknowledgementMessage(
    in_geodatabase=..., out_acknowledgement_file=..., in_replica=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportAcknowledgementMessage_management(in_geodatabase, out_acknowledgement_file, in_replica)

       Creates an output acknowledgement file to acknowledge the reception of
       previously received data change messages.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         Specifies the replica geodatabase from which to export the
         acknowledgement message. The geodatabase may be local or remote.
     in_replica (String):
         The replica from which the acknowledgement message will be exported.

    OUTPUTS:
     out_acknowledgement_file (File):
         Specifies the delta file to export to."""
    ...

@gptooldoc("ExportDataChangeMessage_management", None)
def ExportDataChangeMessage(
    in_geodatabase=...,
    out_data_changes_file=...,
    in_replica=...,
    switch_to_receiver=...,
    include_unacknowledged_changes=...,
    include_new_changes=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportDataChangeMessage_management(in_geodatabase, out_data_changes_file, in_replica, switch_to_receiver, include_unacknowledged_changes, include_new_changes)

       Creates an output delta file containing updates from an input replica.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase from which the data change message will be
         exported. The geodatabase can be local or remote.
     in_replica (String):
         The replica containing the updates to be exported.
     switch_to_receiver (Boolean):
         Specifies whether the replica will be changed from a sender to a
         receiver. The receiver may not send replica updates until updates from
         the relative replica sender arrive.

         * DO_NOT_SWITCH-The replica role will not be changed. This is the
         default.

         * SWITCH-The replica role will be changed from a sender to receiver.
     include_unacknowledged_changes (Boolean):
         Specifies whether data changes that were previously exported for which
         no acknowledgment message was received will be included.

         * NO_UNACKNOWLEDGED-Data changes that were previously sent will not be
         included.

         * UNACKNOWLEDGED-All data changes that were previously exported for
         which no acknowledgment message was received will be included. This is
         the default.
     include_new_changes (Boolean):
         Specifies whether all data changes made since the last exported data
         change message will be included.

         * NO_NEW_CHANGES-Data changes made since the last exported data change
         message will not be included.

         * NEW_CHANGES-All data changes made since the last exported data
         change message will be included. This is the default.

    OUTPUTS:
     out_data_changes_file (File):
         The output delta file."""
    ...

@gptooldoc("ExportReplicaSchema_management", None)
def ExportReplicaSchema(
    in_geodatabase=..., output_replica_schema_file=..., in_replica=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportReplicaSchema_management(in_geodatabase, output_replica_schema_file, in_replica)

       Creates a replica schema file with the schema of an input one- or two-
       way replica.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase from which the replica schema will be
         exported. The geodatabase can be a local or remote geodatabase.
     in_replica (String):
         The replica from which the schema will be exported.

    OUTPUTS:
     output_replica_schema_file (File):
         The file to which the replica schema will be exported."""
    ...

@gptooldoc("ImportMessage_management", None)
def ImportMessage(
    in_geodatabase=...,
    source_delta_file=...,
    output_acknowledgement_file=...,
    conflict_policy=...,
    conflict_definition=...,
    reconcile_with_parent_version=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportMessage_management(in_geodatabase, source_delta_file, {output_acknowledgement_file}, {conflict_policy}, {conflict_definition}, {reconcile_with_parent_version})

       Imports changes from a delta file into a replica geodatabase or
       imports an acknowledgment message into a replica geodatabase.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase that will receive the imported message. The
         geodatabase can be local or remote.
     source_delta_file (Workspace / File):
         The file from which the message will be imported.
     conflict_policy {String}:
         Specifies how conflicts will be resolved when they are encountered
         while importing a data change message.

         * MANUAL-Conflicts must be manually resolved in the versioning
         reconcile environment.

         * IN_FAVOR_OF_DATABASE-Conflicts will be automatically resolved in
         favor of the database receiving the changes.

         * IN_FAVOR_OF_IMPORTED_CHANGES-Conflicts will be automatically
         resolved in favor of the imported changes.
     conflict_definition {String}:
         Specifies whether the conditions required for a conflict to occur will
         be detected by object (row) or by attribute (column).

         * BY_OBJECT-Conflicts will be detected by row.

         * BY_ATTRIBUTE-Conflicts will be detected by column.
     reconcile_with_parent_version {Boolean}:
         Specifies whether data changes will be automatically reconciled once
         they are sent to the parent replica if no conflicts are present. This
         parameter is only enabled for check-out/check-in replicas.

         * DO_NOT_RECONCILE-Changes will not be reconciled with the parent
         version. This is the default.

         * RECONCILE-Changes will be reconciled with the parent version.

    OUTPUTS:
     output_acknowledgement_file {File}:
         The file that will contain the acknowledgement message. When importing
         data changes, you can also export a message to acknowledge the import
         of a data change message. This parameter is only supported for a data
         change message."""
    ...

@gptooldoc("ImportReplicaSchema_management", None)
def ImportReplicaSchema(
    in_geodatabase=..., in_source=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportReplicaSchema_management(in_geodatabase, in_source)

       Applies replica schema differences using an input replica geodatabase
       and an XML schema file.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase to which the replica schema will be imported.
         The geodatabase can be a local geodatabase or a geodata service.
     in_source (File):
         The file that contains the replica schema differences that will be
         imported."""
    ...

@gptooldoc("ReExportUnacknowledgedMessages_management", None)
def ReExportUnacknowledgedMessages(
    in_geodatabase=..., output_delta_file=..., in_replica=..., in_export_option=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReExportUnacknowledgedMessages_management(in_geodatabase, output_delta_file, in_replica, in_export_option)

       Creates an output delta file containing unacknowledged replica updates
       from a one-way or two-way replica geodatabase.

    INPUTS:
     in_geodatabase (Workspace / GeoDataServer):
         The replica geodatabase from which the unacknowledged messages will be
         reexported. The geodatabase can be a local geodatabase or a geodata
         service.
     in_replica (String):
         The replica from which the unacknowledged messages will be reexported.
     in_export_option (String):
         Specifies the changes that will be reexported.

         * ALL_UNACKNOWLEDGED-All changes with unacknowledged messages will be
         reexported.

         * MOST_RECENT-Only those changes made since the last set of exported
         changes was sent will be reexported.

    OUTPUTS:
     output_delta_file (File):
         The delta file to which data changes will be reexported."""
    ...

@gptooldoc("SynchronizeChanges_management", None)
def SynchronizeChanges(
    geodatabase_1=...,
    in_replica=...,
    geodatabase_2=...,
    in_direction=...,
    conflict_policy=...,
    conflict_definition=...,
    reconcile=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SynchronizeChanges_management(geodatabase_1, in_replica, geodatabase_2, in_direction, conflict_policy, conflict_definition, reconcile)

       Synchronizes updates between two replica geodatabases in a specified
       direction.

    INPUTS:
     geodatabase_1 (Workspace / GeoDataServer):
         The geodatabase hosting the replica to synchronize. The geodatabase
         can be local or remote.
     in_replica (String):
         A valid replica with a parent contained in one input geodatabase and a
         child in the other input geodatabase.
     geodatabase_2 (Workspace / GeoDataServer):
         The geodatabase hosting the relative replica. The geodatabase can be
         local or remote.
     in_direction (String):
         Specifies the direction in which the changes will be synchronized:
         from geodatabase 1 to geodatabase 2, from geodatabase 2 to geodatabase
         1, or in both directions. For check-out/check-in replicas or one-way
         replicas, there is only one appropriate direction. If the replica is
         two-way, all of the choices are available.

         * BOTH_DIRECTIONS-Changes will be synchronized in both directions.
         This is the default.

         * FROM_GEODATABASE2_TO_1-Changes will be synchronized from geodatabase
         2 to geodatabase 1.

         * FROM_GEODATABASE1_TO_2-Changes will be synchronized from geodatabase
         1 to geodatabase 2.
     conflict_policy (String):
         Specifies how conflicts will be resolved when they are encountered.

         * MANUAL-Conflicts will be resolved manually in the versioning
         reconcile environment.

         * IN_FAVOR_OF_GDB1-Conflicts will be resolved in favor of geodatabase
         1. This is the default.

         * IN_FAVOR_OF_GDB2-Conflicts will be resolved in favor of geodatabase
         2.
     conflict_definition (String):
         Specifies how conflicts will be defined.

         * BY_OBJECT-Changes to the same row or feature in the parent and child
         versions will conflict during reconcile. This is the default.

         * BY_ATTRIBUTE-Only changes to the same attribute (column) of the
         same row or feature in the parent and child versions will be flagged
         as a conflict during reconcile. Changes to different attributes will
         not be considered a conflict during reconcile.
     reconcile (Boolean):
         Specifies whether to automatically reconcile once data changes are
         sent to the parent replica if there are no conflicts present. This
         option is only available for check-out/check-in replicas.

         * DO_NOT_RECONCILE-Do not reconcile with the parent version. This is
         the default.

         * RECONCILE-Reconcile with the parent version."""
    ...

@gptooldoc("UnregisterReplica_management", None)
def UnregisterReplica(
    in_geodatabase=..., in_replica=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UnregisterReplica_management(in_geodatabase, in_replica)

       Unregisters a replica from an enterprise geodatabase.

    INPUTS:
     in_geodatabase (Workspace):
         The enterprise geodatabase that contains the replica to unregister.
     in_replica (String):
         The name or id of the replica that will be unregistered. If providing
         the replica name, it must be fully qualified, for example,
         myuser.myreplica."""
    ...

@gptooldoc("AddCodedValueToDomain_management", None)
def AddCodedValueToDomain(
    in_workspace=..., domain_name=..., code=..., code_description=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddCodedValueToDomain_management(in_workspace, domain_name, code, code_description)

       Adds a value to a domain's coded value list.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase containing the domain to be updated.
     domain_name (String):
         The name of the attribute domain that will have a value added to its
         coded value list.
     code (String):
         The value to be added to the specified domain's coded value list.
     code_description (String):
         A description of what the coded value represents."""
    ...

@gptooldoc("AlterDomain_management", None)
def AlterDomain(
    in_workspace=...,
    domain_name=...,
    new_domain_name=...,
    new_domain_description=...,
    split_policy=...,
    merge_policy=...,
    new_domain_owner=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterDomain_management(in_workspace, domain_name, {new_domain_name}, {new_domain_description}, {split_policy}, {merge_policy}, {new_domain_owner})

       Alters the properties of an existing attribute domain in a workspace.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase that contains the domain that will be altered.
     domain_name (String):
         The name of the domain the will be altered.
     new_domain_name {String}:
         The new name of the domain.
     new_domain_description {String}:
         The new description of the domain.
     split_policy {String}:
         Specifies the split policy that will used for the domain. The behavior
         of an attribute's values when a feature is split is controlled by its
         split policy.

         * DEFAULT-The attributes of the two resulting features will be the
         default value of the attribute of the given feature class or subtype.

         * DUPLICATE-The attribute of the two resulting features will be a copy
         of the original object's attribute value.

         * GEOMETRY_RATIO-The attributes of resulting features will be a ratio
         of the original feature's value. The ratio is based on the proportion
         into which the original geometry is divided. If the geometry is
         divided equally, the attribute of each new feature will be one-half
         the value of the original object's attribute. This option only applies
         to range domains.
     merge_policy {String}:
         Specifies the merge policy that will be used for the domain. When two
         features are merged into a single feature, merge policies will control
         attribute values in the new feature. This parameter is only applicable
         to range domains, as coded value domains can only use the default
         merge policy.

         * DEFAULT-The attribute of the resulting feature will be the default
         value of the attribute of the given feature class or subtype. This
         option only applies to nonnumeric fields and coded value domains.

         * SUM_VALUES-The attribute of the resulting feature will be on the sum
         of the values from the original feature's attribute. This option only
         applies to range domains.

         * AREA_WEIGHTED-The attribute of the resulting feature will be the
         weighted average of the attribute values of the original features. The
         average is based on the original feature's geometry. This option only
         applies to range domains.
     new_domain_owner {String}:
         The name of the database user that the domain ownership will be
         transferred to. Ensure that the new domain owner exists in the
         database; the tool does not check the validity of the owner name
         specified. This parameter is not applicable for domains created in a
         file geodatabase."""
    ...

@gptooldoc("AssignDomainToField_management", None)
def AssignDomainToField(
    in_table=..., field_name=..., domain_name=..., subtype_code=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AssignDomainToField_management(in_table, field_name, domain_name, {subtype_code;subtype_code...})

       Sets the domain for a particular field and, optionally, for a subtype.
       If no subtype is specified, the domain is only assigned to the
       specified field.

    INPUTS:
     in_table (Table View):
         The name of the table or feature class containing the field that will
         be assigned a domain.
     field_name (Field):
         The name of the field to be assigned a domain.
     domain_name (String):
         The name of a geodatabase domain to assign to the field name.
         Available domains will automatically be loaded.
     subtype_code {String}:
         The subtype code to be assigned a domain."""
    ...

@gptooldoc("CreateDomain_management", None)
def CreateDomain(
    in_workspace=...,
    domain_name=...,
    domain_description=...,
    field_type=...,
    domain_type=...,
    split_policy=...,
    merge_policy=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDomain_management(in_workspace, domain_name, {domain_description}, {field_type}, {domain_type}, {split_policy}, {merge_policy})

       Creates an attribute domain in the specified workspace.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase that will contain the new domain.
     domain_name (String):
         The name of the domain that will be created.
     domain_description {String}:
         The description of the domain that will be created.
     field_type {String}:
         Specifies the type of attribute domain that will be created. Attribute
         domains are rules that describe the accepted values of a field type.
         Specify a field type that matches the data type of the field to which
         the attribute domain will be assigned.

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * DATE-The field type will be date. Date fields support date and time
         values.
     domain_type {String}:
         Specifies the domain type that will be created.

         * CODED-A coded type domain will be created that contains a valid set
         of values for an attribute. This is the default. For example, a coded
         value domain can specify valid pipe material values such as CL-cast
         iron pipe, DL-ductile iron pipe, or ACP-asbestos concrete pipe.

         * RANGE-A range type domain will be created that contains a valid
         range of values for a numeric attribute. For example, if distribution
         water mains have a pressure between 50 and 75 psi, a range domain
         specifies these minimum and maximum values.
     split_policy {String}:
         Specifies the split policy that will be used for the created domain.
         The behavior of an attribute's values when a feature that is split is
         controlled by its split policy.

         * DEFAULT-The attributes of the two resulting features will use the
         default value of the attribute of the given feature class or subtype.

         * DUPLICATE-The attribute of the two resulting features will use a
         copy of the original object's attribute value.

         * GEOMETRY_RATIO-The attributes of resulting features will be a ratio
         of the original feature's value. The ratio is based on the proportion
         into which the original geometry is divided. If the geometry is
         divided equally, each new feature's attribute gets one-half the value
         of the original object's attribute. The geometry ratio policy only
         applies to range domains.
     merge_policy {String}:
         Specifies the merge policy that will be used for the created domain.
         When two features are merged into a single feature, merge policies
         control attribute values in the new feature.

         * DEFAULT-The attribute of the resulting feature will use the default
         value of the attribute of the given feature class or subtype. This is
         the only merge policy that applies to nonnumeric fields and coded
         value domains.

         * SUM_VALUES-The attribute of the resulting feature will use the sum
         of the values from the original feature's attribute. The sum values
         policy only applies to range domains.

         * AREA_WEIGHTED-The attribute of the resulting feature will be the
         weighted average of the attribute values of the original features.
         This average is based on the original feature's geometry. The area
         weighted policy only applies to range domains."""
    ...

@gptooldoc("DeleteCodedValueFromDomain_management", None)
def DeleteCodedValueFromDomain(
    in_workspace=..., domain_name=..., code=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteCodedValueFromDomain_management(in_workspace, domain_name, code;code...)

       Removes a value from a coded value domain.

    INPUTS:
     in_workspace (Workspace):
         The workspace containing the domain to be updated.
     domain_name (String):
         The name of the coded value domain to be updated.
     code (String):
         The value(s) to be deleted from the specified domain."""
    ...

@gptooldoc("DeleteDomain_management", None)
def DeleteDomain(
    in_workspace=..., domain_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteDomain_management(in_workspace, domain_name)

       Deletes a domain from a workspace.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase that contains the domain to be deleted.
     domain_name (String):
         The name of the domain to be deleted."""
    ...

@gptooldoc("DomainToTable_management", None)
def DomainToTable(
    in_workspace=...,
    domain_name=...,
    out_table=...,
    code_field=...,
    description_field=...,
    configuration_keyword=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DomainToTable_management(in_workspace, domain_name, out_table, code_field, description_field, {configuration_keyword})

       Creates a table from an attribute domain.

    INPUTS:
     in_workspace (Workspace):
         The workspace containing the attribute domain to be converted to a
         table.
     domain_name (String):
         The name of the existing attribute domain.
     code_field (String):
         The name of the field in the created table that will store code
         values.
     description_field (String):
         The name of the field in the created table that will store code value
         descriptions.
     configuration_keyword {String}:
         For geodatabase tables, the custom storage keywords for creating the
         table.

    OUTPUTS:
     out_table (Table):
         The table to be created."""
    ...

@gptooldoc("RemoveDomainFromField_management", None)
def RemoveDomainFromField(
    in_table=..., field_name=..., subtype_code=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveDomainFromField_management(in_table, field_name, {subtype_code;subtype_code...})

       Removes an attribute domain association from a feature class or table
       field.

    INPUTS:
     in_table (Table View):
         The input table containing the attribute domain that will be removed.
     field_name (Field):
         The field that will no longer be associated with an attribute domain.
     subtype_code {String}:
         The subtype code(s) that will no longer be associated with an
         attribute domain."""
    ...

@gptooldoc("SetValueForRangeDomain_management", None)
def SetValueForRangeDomain(
    in_workspace=..., domain_name=..., min_value=..., max_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetValueForRangeDomain_management(in_workspace, domain_name, min_value, max_value)

       Sets the minimum and maximum values for an existing Range domain.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase containing the domain to be updated.
     domain_name (String):
         The name of the range domain to be updated.
     min_value (String):
         The minimum value of the range domain.
     max_value (String):
         The maximum value of the range domain."""
    ...

@gptooldoc("SortCodedValueDomain_management", None)
def SortCodedValueDomain(
    in_workspace=..., domain_name=..., sort_by=..., sort_order=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SortCodedValueDomain_management(in_workspace, domain_name, sort_by, sort_order)

       Sorts the code or description of a coded value domain in either
       ascending or descending order.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase containing the domain to be sorted.
     domain_name (String):
         The name of the coded value domain to be sorted.
     sort_by (String):
         Specifies whether the code or description will be used to sort the
         domain.

         * CODE-Records are sorted based on the code value for the domain.

         * DESCRIPTION-Records are sorted based on the description value for
         the domain.
     sort_order (String):
         Specifies the direction the records will be sorted.

         * ASCENDING-Records are sorted from low value to high value.

         * DESCENDING-Records are sorted from high value to low value."""
    ...

@gptooldoc("TableToDomain_management", None)
def TableToDomain(
    in_table=...,
    code_field=...,
    description_field=...,
    in_workspace=...,
    domain_name=...,
    domain_description=...,
    update_option=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TableToDomain_management(in_table, code_field, description_field, in_workspace, domain_name, {domain_description}, {update_option})

       Creates or updates a coded value domain with values from a table.

    INPUTS:
     in_table (Table View):
         The database table from which to derive domain values.
     code_field (Field):
         The field in the database table from which to derive domain code
         values.
     description_field (Field):
         The field in the database table from which to derive domain
         description values.
     in_workspace (Workspace):
         The workspace that contains the domain to be created or updated.
     domain_name (String):
         The name of the domain to be created or updated.
     domain_description {String}:
         The description of the domain to be created or updated. Domain
         descriptions of existing domains are not updated.
     update_option {String}:
         If the domain already exists, specifies how the domain will be
         updated.

         * APPEND-Appends to the domain values from the database table. This is
         the default.

         * REPLACE-Replaces the values in the domain with values from the
         database table."""
    ...

@gptooldoc("DisableFeatureBinning_management", None)
def DisableFeatureBinning(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableFeatureBinning_management(in_features)

       Disables database computed feature binning on a feature class.

    INPUTS:
     in_features (Feature Layer):
         The feature class for which database computed feature binning will be
         disabled."""
    ...

@gptooldoc("EnableFeatureBinning_management", None)
def EnableFeatureBinning(
    in_features=...,
    bin_type=...,
    bin_coord_sys=...,
    summary_stats=...,
    generate_static_cache=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableFeatureBinning_management(in_features, {bin_type}, {bin_coord_sys;bin_coord_sys...}, {summary_stats;summary_stats...}, {generate_static_cache})

       Enables database computation for feature binning on a feature class.

    INPUTS:
     in_features (Feature Layer):
         The feature class for which database computed feature binning will be
         enabled. Only point and multipoint feature classes stored in an
         enterprise geodatabase, database, or cloud data warehouse are
         supported. The data cannot be versioned or archive enabled.
     bin_type {String}:
         Specifies the type of binning that will be enabled. If you are using
         SAP HANA data, only the SQUARE, FLAT_HEXAGON, and POINTY_HEXAGON
         options are supported. If you are using Snowflake or Redshift data,
         only the GEOHASH option is supported.

         * FLAT_HEXAGON-The flat hexagon binning scheme, also known as flat
         geohex or flat hexbinning, will be enabled. The tiles are a
         tessellation of hexagons in which the orientation of the hexagons has
         a flat edge of the hexagon on top. This is the default for Microsoft
         SQL Server, Oracle, PostgreSQL, and BigQuery data.

         * POINTY_HEXAGON-The pointy hexagon binning scheme, also known as
         pointy geohex or pointy hexbinning, will be enabled. The tiles are a
         tessellation of hexagons in which the orientation of the hexagons has
         a point of the hexagon on top.

         * SQUARE-The square binning scheme, also known as geosquare or
         squarebinning, will be enabled. The tiles are a tessellation of
         squares This is the default for Db2 and SAP HANA data.

         * GEOHASH-The geohash binning scheme, in which the tiles are a
         tessellation of rectangles, will be enabled. Because geohash bins
         always use the WGS84 geographic coordinate system (GCS WGS84, EPSG
         WKID 4326), you cannot specify a bin coordinate system for geohash
         bins. This is the default and only option for data in Snowflake or
         Redshift.
     bin_coord_sys {Coordinate System}:
         The coordinate systems that will be used to visualize the aggregated
         output feature layer. You can specify up to two coordinate systems to
         visualize the output layer. By default, the coordinate system of the
         input feature class is used. Custom coordinate systems are not
         supported.This parameter does not apply to BigQuery, Redshift, or
         Snowflake. For
         those platforms, the coordinate system of the input feature class is
         used.
     summary_stats {Value Table}:
         Specifies the statistics that will be summarized and stored in
         the bin cache. Statistics are used to symbolize bins and provide
         aggregate information for all the points in a bin. One summary
         statistic, the total feature count (shape_count), is always available.
         You can define up to five additional summary statistics.

         * Field-The field on which the summary statistics will be calculated.
         Supported field types are short integer, long integer, float, and
         double.

         * Statistic Type-The type of statistic that will be
         calculated for the specified field. Statistics are calculated for all
         features in the bin. Available statistics types are as follows:

         * Mean (AVG)-Calculates the average for the specified field

         * Minimum (MIN)-Finds the smallest value for all records of the
         specified field

         * Maximum (MAX)-Finds the largest value for all records of the
         specified field

         * Standard deviation (STDDEV)-Calculates the standard deviation value
         for the field

         * Sum (SUM)-Adds the total value for the specified field
     generate_static_cache {Boolean}:
         Specifies whether a static cache of the aggregated results will be
         generated or visualizations will be aggregated on the fly. The cache
         is not necessarily created for all levels of detail.

         * STATIC_CACHE-A static cache of the aggregated results will
         be generated. It is recommended that you use this option for better
         performance. However, changes to the underlying data will not be
         updated in the cache unless the Manage Feature Bin Cache tool is run.

         * A static cache is generated by default for data in IBM Db2,
         Microsoft SQL Server, Oracle, and PostgreSQL.

         * To generate a static cache for feature classes in PostgreSQL that
         use PostGIS spatial types, GDAL libraries must be installed in the
         database.

         * A static cache is always generated for data in BigQuery, Redshift,
         and Snowflake.

         * DYNAMIC-A static cache of the aggregated results will not be
         generated, and visualizations will be aggregated on the fly. This is
         the only option for SAP HANA data."""
    ...

@gptooldoc("ManageFeatureBinCache_management", None)
def ManageFeatureBinCache(
    in_features=...,
    bin_type=...,
    max_lod=...,
    add_cache_statistics=...,
    delete_cache_statistics=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ManageFeatureBinCache_management(in_features, {bin_type}, {max_lod}, {add_cache_statistics;add_cache_statistics...}, {delete_cache_statistics;delete_cache_statistics...})

       Manages the feature binning cache for data that has database computed
       feature binning enabled.

    INPUTS:
     in_features (Feature Layer):
         The binned feature class that will have its static cache updated.
     bin_type {String}:
         Specifies the type of feature binning visualization that will be
         enabled.

         * FLAT_HEXAGON-The flat hexagon binning scheme, also known as flat
         geohex or flat hexbinning, will be enabled. The tiles are a
         tessellation of hexagons in which the orientation of the hexagons has
         a flat edge of the hexagon on top. This is the default for Microsoft
         SQL Server, Oracle, and PostgreSQL data.

         * POINTY_HEXAGON-The pointy hexagon binning scheme, also known as
         pointy geohex or pointy hexbinning, will be enabled. The tiles are a
         tessellation of hexagons in which the orientation of the hexagons has
         a point of the hexagon on top.

         * SQUARE-The square binning scheme in which the tiles are a
         tessellation of squares, also known as geosquare or squarebinning,
         will be enabled. This is the default for Db2 data.

         * GEOHASH-The geohash binning scheme in which the tiles are a
         tessellation of rectangles will be enabled. Because geohash bins
         always use the WGS84 geographic coordinate system (GCS WGS84, EPSG
         WKID 4326), you cannot specify a bin coordinate system for geohash
         bins.
     max_lod {String}:
         Specifies the maximum level of detail that will be used for the
         cache.Tiling schemes are a continuum of scale ranges. Depending on the
         map,
         you may want to forego caching of some of the extremely large or small
         scales in the tiling scheme. This tool examines the scale dependencies
         in the map and attempts to provide a maximum range of scale for
         caching. Choose a level of detail that most closely matches the
         intended use of the map in which the data will be shown.

         * WORLD-A world scale will be used as the maximum level of detail.

         * CONTINENTS-Multiple continents scale will be used as the maximum
         level of detail.

         * CONTINENT-A single continent scale will be used as the maximum level
         of detail.

         * COUNTRIES-Multiple countries scale will be used as the maximum level
         of detail.

         * COUNTRY-A single country scale will be used as the maximum level of
         detail.

         * STATES-Multiple states scale will be used as the maximum level of
         detail.

         * STATE-A single state scale will be used as the maximum level of
         detail.

         * COUNTIES-Multiple counties scale will be used as the maximum level
         of detail.

         * COUNTY-A single county scale will be used as the maximum level of
         detail.

         * CITIES-Multiple cities scale will be used as the maximum level of
         detail.

         * CITY-A single city scale will be used as the maximum level of
         detail.
     add_cache_statistics {Value Table}:
         Specifies the statistics that will be summarized and stored in
         the bin cache. Statistics are used to symbolize bins and provide
         aggregate information for all the points in a bin. One summary
         statistic, shape_count (which is the total feature count), is always
         available.

         * Field-The field on which the summary statistics will be calculated.
         Supported field types are short, long, float, and double.

         * Statistic Type-The type of statistic that will be
         calculated for the specified field. Statistics are calculated for all
         features in the bin. Available statistics types are as follows:

         * Mean (AVG)-Calculates the average for the specified field.

         * Minimum (MIN)-Finds the smallest value for all records of the
         specified field.

         * Maximum (MAX)-Finds the largest value for all records of the
         specified field.

         * Standard deviation (STDDEV)-Calculates the standard deviation value
         for the field.

         * Sum (SUM)-Adds the total value for the specified field.
     delete_cache_statistics {String}:
         The summary statistic that will be deleted from the cache. You cannot
         delete the default COUNT summary statistic."""
    ...

@gptooldoc("AppendAnnotation_management", None)
def AppendAnnotation(
    input_features=...,
    output_featureclass=...,
    reference_scale=...,
    create_single_class=...,
    require_symbol_from_table=...,
    create_annotation_when_feature_added=...,
    update_annotation_when_feature_modified=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AppendAnnotation_management(input_features;input_features..., output_featureclass, reference_scale, {create_single_class}, {require_symbol_from_table}, {create_annotation_when_feature_added}, {update_annotation_when_feature_modified})

       Creates a geodatabase annotation feature class or appends to an
       existing annotation feature class by combining annotation from
       multiple input geodatabase annotation feature classes into a single
       feature class with annotation classes.

    INPUTS:
     input_features (Feature Layer):
         The input annotation features that will form an annotation class in
         the output feature class.
     reference_scale (Double):
         The reference scale set in the output feature class. Input features
         created at a different reference scale will be transformed to match
         this output reference scale.
     create_single_class {Boolean}:
         Specifies how annotation features will be added to the output feature
         class.

         * ONE_CLASS_ONLY-All annotation features will be aggregated into one
         annotation class in the output feature class.

         * CREATE_CLASSES-Separate annotation classes will be created for each
         input annotation class in the output feature class unless the classes
         are named the same and have the same properties. In this case, they
         will be merged. This is the default.
     require_symbol_from_table {Boolean}:
         Specifies how symbols can be selected for newly created annotation
         features.

         * REQUIRE_SYMBOL-Restricts the creation of annotation features to the
         list of symbols in the symbol collection of the output feature class.

         * NO_SYMBOL_REQUIRED-Allows annotation features to be created with any
         symbology. This is the default.
     create_annotation_when_feature_added {Boolean}:
         This parameter is only available with ArcGIS Desktop Standard and
         ArcGIS Desktop Advanced licenses.Specifies whether feature-linked
         annotation will be created when a
         feature is added.

         * AUTO_CREATE-Feature-linked annotation will be created using the
         label engine when a linked feature is created. The is the default.

         * NO_AUTO_CREATE-Feature-linked annotation will not be created when a
         feature is created.
     update_annotation_when_feature_modified {Boolean}:
         This parameter is only available with ArcGIS Desktop Standard and
         ArcGIS Desktop Advanced licenses.Specifies whether feature-linked
         annotation is updated when a linked
         feature changes.

         * AUTO_UPDATE-Feature-linked annotation will be updated using the
         label engine when a linked feature changes. This is the default.

         * NO_AUTO_UPDATE-Feature-linked annotation will not be updated when a
         linked feature changes.

    OUTPUTS:
     output_featureclass (Feature Class):
         A new annotation feature class that will contain an annotation class
         for each input annotation feature class."""
    ...

@gptooldoc("CalculateDefaultClusterTolerance_management", None)
def CalculateDefaultClusterTolerance(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateDefaultClusterTolerance_management(in_features)

       Calculates the default xy tolerance.

    INPUTS:
     in_features (Feature Layer):
         Input Features"""
    ...

@gptooldoc("CalculateDefaultGridIndex_management", None)
def CalculateDefaultGridIndex(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateDefaultGridIndex_management(in_features)

       Calculate the default spatial grid index.

    INPUTS:
     in_features (Feature Layer):
         Input Features"""
    ...

@gptooldoc("CreateFeatureclass_management", None)
def CreateFeatureclass(
    out_path=...,
    out_name=...,
    geometry_type=...,
    template=...,
    has_m=...,
    has_z=...,
    spatial_reference=...,
    config_keyword=...,
    spatial_grid_1=...,
    spatial_grid_2=...,
    spatial_grid_3=...,
    out_alias=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFeatureclass_management(out_path, out_name, {geometry_type}, {template;template...}, {has_m}, {has_z}, {spatial_reference}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3}, {out_alias})

       Creates an empty feature class in a geodatabase or a shapefile in a
       folder.

    INPUTS:
     out_path (Workspace / Feature Dataset):
         The enterprise or file geodatabase or the folder in which the output
         feature class will be created. This workspace must already exist.
     out_name (String):
         The name of the feature class to be created.
     geometry_type {String}:
         Specifies the geometry type of the output feature class.

         * POINT-The geometry type will be point.

         * MULTIPOINT-The geometry type will be multipoint.

         * POLYGON-The geometry type will be polygon.

         * POLYLINE-The geometry type will be polyline.

         * MULTIPATCH-The geometry type will be multipatch.
     template {Table View}:
         The feature class or table used as a template to define the attribute
         fields of the new feature class.
     has_m {String}:
         Specifies whether the feature class will have linear measurement
         values (m-values).

         * DISABLED-The output feature class will not have m-values. This is
         the default.

         * ENABLED-The output feature class will have m-values.

         * SAME_AS_TEMPLATE-The output feature class will have m-values if the
         dataset specified in the Template Feature Class parameter (template
         parameter in Python) has m-values.
     has_z {String}:
         Specifies whether the feature class will have elevation values
         (z-values).

         * DISABLED-The output feature class will not have z-values. This is
         the default.

         * ENABLED-The output feature class will have z-values.

         * SAME_AS_TEMPLATE-The output feature class will have z-values if the
         dataset specified in the Template Feature Class parameter (template
         parameter in Python) has z-values.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature dataset. You can
         specify the spatial reference in the following ways:

         * Enter the path to a .prj file, such as C:/workspace/watershed.prj.

         * Reference a feature class or feature dataset whose spatial reference
         you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland.

         * Define a spatial reference object prior to using this tool, such as
         sr = arcpy.SpatialReference("C:/data/Africa/Carthage.prj"), which you
         then use as the spatial reference parameter.
         If a spatial reference is not provided, the output will have an
         undefined spatial reference.The spatial reference of the Template
         Feature Class has no effect on
         the output spatial reference. If you want your output to be in the
         coordinate system of the Template Feature Class, set the Coordinate
         System parameter to the spatial reference of the Template Feature
         Class.
     config_keyword {String}:
         The configuration keyword applies to enterprise geodatabase data only.
         It determines the storage parameters of the database table.
     spatial_grid_1 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.
     spatial_grid_2 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.
     spatial_grid_3 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.
     out_alias {String}:
         The alternate name for the output feature class that will be created."""
    ...

@gptooldoc("CreateUnRegisteredFeatureclass_management", None)
def CreateUnRegisteredFeatureclass(
    out_path=...,
    out_name=...,
    geometry_type=...,
    template=...,
    has_m=...,
    has_z=...,
    spatial_reference=...,
    config_keyword=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateUnRegisteredFeatureclass_management(out_path, out_name, {geometry_type}, {template;template...}, {has_m}, {has_z}, {spatial_reference}, {config_keyword})

       Creates an empty feature class in a database or enterprise
       geodatabase. The feature class is not registered with the geodatabase.

    INPUTS:
     out_path (Workspace / Feature Dataset):
         The enterprise geodatabase or database in which the output feature
         class will be created.
     out_name (String):
         The name of the feature class to be created.
     geometry_type {String}:
         Specifies the geometry type of the feature class. This parameter is
         only relevant for those geometry types that store dimensionality
         metadata, such as ST_Geometry in PostgreSQL, PostGIS Geometry, and
         Oracle SDO_Geometry.

         * POINT-The geometry type will be point.

         * MULTIPOINT-The geometry type will be multipoint.

         * POLYLINE-The geometry type will be polyline.

         * POLYGON-The geometry type will be polygon. This is the default.
     template {Feature Layer}:
         An existing feature class or list of feature classes with fields and
         attribute schema used to defined the fields in the output feature
         class.
     has_m {String}:
         Specifies whether the feature class will have linear measurement
         values (m-values).

         * DISABLED-The output feature class will not have m-values. This is
         the default.

         * ENABLED-The output feature class will have m-values.

         * SAME_AS_TEMPLATE-The output feature class will have m-values if the
         dataset specified in the Template Feature Class parameter (template
         parameter in Python) has m-values.
     has_z {String}:
         Specifies whether the feature class will have elevation values
         (z-values).

         * DISABLED-The output feature class will not have z-values. This is
         the default.

         * ENABLED-The output feature class will have z-values.

         * SAME_AS_TEMPLATE-The output feature class will have z-values if the
         dataset specified in the Template Feature Class parameter (template
         parameter in Python) has z-values.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature dataset. You can
         specify the spatial reference in the following ways:

         * Enter the path to a .prj file, such as C:/workspace/watershed.prj.

         * Reference a feature class or feature dataset whose spatial reference
         you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland.

         * Define a spatial reference object prior to using this tool, such as
         sr = arcpy.SpatialReference("C:/data/Africa/Carthage.prj"), which you
         then use as the spatial reference parameter.
     config_keyword {String}:
         Specifies the default storage parameters (configurations) for
         geodatabases in a relational database management system (RDBMS). This
         setting is applicable only when using enterprise geodatabase
         tables.Configuration keywords are set by the database administrator."""
    ...

@gptooldoc("Integrate_management", None)
def Integrate(
    in_features=..., cluster_tolerance=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Integrate_management(in_features;in_features..., {cluster_tolerance})

       Analyzes the coordinate locations of feature vertices among features
       in one or more feature classes. Those that fall within a specified
       distance of one another are assumed to represent the same location and
       are assigned a common coordinate value (in other words, they are
       colocated). The tool also adds vertices where feature vertices are
       within the x,y tolerance of an edge and where line segments intersect.

    INPUTS:
     in_features (Value Table):
         The feature classes to be integrated. When the distance between
         features is small in comparison to the tolerance, the vertices or
         points will be clustered (moved to be coincident). The feature class
         or layer with the lower rank will snap to the feature from the feature
         class or layer with the higher rank (with 1 being a higher rank than
         2). Features in the feature class with a rank of 1 may move when a
         large x,y tolerance is used. For more information, see Priority ranks
         and geoprocessing tools.
     cluster_tolerance {Linear Unit}:
         The distance that determines the range in which feature vertices are
         made coincident. To minimize undesired movement of vertices, the x,y
         tolerance should be fairly small. If no value is specified, the xy
         tolerance from the first dataset in the list of inputs will be
         used.Changing this parameter's value may cause failure or unexpected
         results. It is recommended that you do not modify this parameter. It
         has been removed from view on the tool dialog box. By default, the
         input feature class's spatial reference x,y tolerance property is
         used."""
    ...

@gptooldoc("RecalculateFeatureClassExtent_management", None)
def RecalculateFeatureClassExtent(
    in_features=..., store_extent=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RecalculateFeatureClassExtent_management(in_features, {store_extent})

       Recalculates the xy, z, and m extent properties of a feature class
       based on the features in the feature class.

    INPUTS:
     in_features (Feature Layer):
         The shapefile or geodatabase feature class that will be updated.
     store_extent {Boolean}:
         Specifies whether the extent will be stored for feature classes that
         are not registered.If the input feature class is updated frequently,
         you may choose not
         to store the recalculated extent value. If you choose to store the
         extent, the extent will not be recalculated each time the feature
         class is added to the map.

         * STORE_EXTENT-The extent will be stored for the input feature class.

         * DO_NOT_STORE_EXTENT-The extent will not be stored for the input
         feature class. This is the default."""
    ...

@gptooldoc("SetFeatureClassSplitModel_management", None)
def SetFeatureClassSplitModel(
    in_feature_class=..., split_model=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetFeatureClassSplitModel_management(in_feature_class, {split_model})

       Defines the behavior of a split operation on a feature class.

    INPUTS:
     in_feature_class (Polygon|Polyline):
         The feature class on which the split model will be set.
     split_model {String}:
         Specifies the split model to apply to the input feature class.

         * DELETE_INSERT_INSERT-The original feature will be deleted, and both
         parts of the split feature will be inserted as new features with two
         new rows in the table.

         * UPDATE_INSERT-The original feature will be updated, becoming the
         largest feature, and the smaller feature will be inserted as a new row
         in the table. This is the default."""
    ...

@gptooldoc("AddGeometryAttributes_management", None)
def AddGeometryAttributes(
    Input_Features=...,
    Geometry_Properties=...,
    Length_Unit=...,
    Area_Unit=...,
    Coordinate_System=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddGeometryAttributes_management(Input_Features, Geometry_Properties;Geometry_Properties..., {Length_Unit}, {Area_Unit}, {Coordinate_System})

       Adds new attribute fields to the input features representing the
       spatial or geometric characteristics and location of each feature,
       such as length or area and x-, y-, z-, and m-coordinates.

    INPUTS:
     Input_Features (Feature Layer):
         The input features to which new attribute fields will be added to
         store properties such as length, area, or x-, y-, z-, and
         m-coordinates.
     Geometry_Properties (String):
         Specifies the geometry or shape properties that will be calculated
         into new attribute fields.

         * AREA-An attribute will be added to store the area of each polygon
         feature.

         * AREA_GEODESIC-An attribute will be added to store the shape-
         preserving geodesic area of each polygon feature.

         * CENTROID-Attributes will be added to store the centroid coordinates
         of each feature.

         * CENTROID_INSIDE-Attributes will be added to store the coordinates of
         a central point inside or on each feature.

         * EXTENT-Attributes will be added to store the extent coordinates of
         each feature.

         * LENGTH-An attribute will be added to store the length of each line
         feature.

         * LENGTH_GEODESIC-An attribute will be added to store the shape-
         preserving geodesic length of each line feature.

         * LENGTH_3D-An attribute will be added to store the 3D length of each
         line feature.

         * LINE_BEARING-An attribute will be added to store the start-to-end
         bearing of each line feature. Values range from 0 to 360, with 0
         meaning north, 90 east, 180 south, and 270 west.

         * LINE_START_MID_END-Attributes will be added to store the coordinates
         of the start, mid, and end points of each feature.

         * PART_COUNT-An attribute will be added to store the number of parts
         composing each feature.

         * PERIMETER_LENGTH-An attribute will be added to store the length of
         the perimeter or border of each polygon feature.

         * PERIMETER_LENGTH_GEODESIC-An attribute will be added to store the
         shape-preserving geodesic length of the perimeter or border of each
         polygon feature.

         * POINT_COUNT-An attribute will be added to store the number of points
         or vertices composing each feature.

         * POINT_X_Y_Z_M-Attributes will be added to store the x-, y-, z-, and
         m-coordinates of each point feature.
     Length_Unit {String}:
         Specifies the unit in which the length will be calculated.

         * FEET_US-Length in feet (United States)

         * METERS-Length in meters

         * KILOMETERS-Length in kilometers

         * MILES_US-Length in miles (United States)

         * NAUTICAL_MILES-Length in nautical miles (United States)

         * YARDS-Length in yards (United States)
     Area_Unit {String}:
         Specifies the unit in which the area will be calculated.

         * ACRES-Area in acres

         * HECTARES-Area in hectares

         * SQUARE_MILES_US-Area in square miles (United States)

         * SQUARE_KILOMETERS-Area in square kilometers

         * SQUARE_METERS-Area in square meters

         * SQUARE_FEET_US-Area in square feet (United States)

         * SQUARE_YARDS-Area in square yards (United States)

         * SQUARE_NAUTICAL_MILES-Area in square nautical miles (United States)
     Coordinate_System {Coordinate System}:
         The coordinate system in which the coordinates, length, and area will
         be calculated. The coordinate system of the input features is used by
         default."""
    ...

@gptooldoc("AddXY_management", None)
def AddXY(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddXY_management(in_features)

       Adds the fields POINT_X and POINT_Y to the point input features and
       calculates their values. It also appends the POINT_Z and POINT_M
       fields if the input features are Z- and M-enabled.

    INPUTS:
     in_features (Feature Layer):
         The point features whose x,y coordinates will be appended as POINT_X
         and POINT_Y fields."""
    ...

@gptooldoc("Adjust3DZ_management", None)
def Adjust3DZ(
    in_features=..., reverse_sign=..., adjust_value=..., from_units=..., to_units=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Adjust3DZ_management(in_features, {reverse_sign}, {adjust_value}, {from_units}, {to_units})

       Modifies z-values of 3D features.

    INPUTS:
     in_features (Feature Layer):
         The 3D features with the z-values that will be modified.
     reverse_sign {String}:
         Specifies whether features will be inverted along the z-axis.

         * REVERSE-The sign of z-values will be inverted causing the feature to
         flip upside down.

         * NO_REVERSE-The sign of z-values will not be inverted; it will be
         maintained. This is the default.
     adjust_value {Double / Field}:
         A numeric value or field from the input features that will be used to
         adjust the z of each vertex in the input features. A positive value
         will shift the feature higher, while a negative number will shift it
         lower along the z-axis.
     from_units {String}:
         Specifies the existing units of the z-values. This parameter is used
         in conjunction with the to_units parameter.

         * MILLIMETERS-The units will be millimeters.

         * CENTIMETERS-The units will be centimeters.

         * METERS-The units will be meters.

         * INCHES-The units will be inches.

         * FEET-The units will be feet.

         * YARDS-The units will be yards.

         * FATHOMS-The units will be fathoms.
     to_units {String}:
         Specifies the units that existing z-values will be converted to.

         * MILLIMETERS-The units will be millimeters.

         * CENTIMETERS-The units will be centimeters.

         * METERS-The units will be meters.

         * INCHES-The units will be inches.

         * FEET-The units will be feet.

         * YARDS-The units will be yards.

         * FATHOMS-The units will be fathoms."""
    ...

@gptooldoc("BearingDistanceToLine_management", None)
def BearingDistanceToLine(
    in_table=...,
    out_featureclass=...,
    x_field=...,
    y_field=...,
    distance_field=...,
    distance_units=...,
    bearing_field=...,
    bearing_units=...,
    line_type=...,
    id_field=...,
    spatial_reference=...,
    attributes=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BearingDistanceToLine_management(in_table, out_featureclass, x_field, y_field, distance_field, distance_units, bearing_field, bearing_units, {line_type}, {id_field}, {spatial_reference}, {attributes})

       Creates a feature class containing geodetic or planar line features
       from the values in an x-coordinate field, y-coordinate field, bearing
       field, and distance field of a table.

    INPUTS:
     in_table (Table View):
         The input table. It can be a text file, CSV file, Excel file, dBASE
         table, or geodatabase table.
     x_field (Field):
         A numerical field in the input table containing the x-coordinates (or
         longitudes) of the starting points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     y_field (Field):
         A numerical field in the input table containing the y-coordinates (or
         latitudes) of the starting points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     distance_field (Field):
         A numerical field in the input table containing the distances from the
         starting points for creating the output lines.
     distance_units (String):
         Specifies the units that will be used for the distance_field
         parameter.

         * METERS-The units will be meters.

         * KILOMETERS-The units will be kilometers.

         * MILES-The units will be miles.

         * NAUTICAL_MILES-The units will be nautical miles.

         * FEET-The units will be feet.

         * US_SURVEY_FEET-The units will be U.S. survey feet.
     bearing_field (Field):
         A numerical field in the input table containing bearing angle values
         for the output line rotation. The angles are measured clockwise from
         north.
     bearing_units (String):
         Specifies the units of the bearing_field parameter values.

         * DEGREES-The units will be decimal degrees. This is the default.

         * MILS-The units will be mils.

         * RADS-The units will be radians.

         * GRADS-The units will be gradians.
     line_type {String}:
         Specifies the type of line that will be constructed.

         * GEODESIC-A type of geodetic line that most accurately represents
         the shortest distance between any two points on the surface of the
         earth will be constructed. This is the default.

         * GREAT_CIRCLE-A type of geodetic line that represents the path
         between any two points along the intersection of the surface of the
         earth and a plane that passes through the center of the earth will be
         constructed. If the Spatial Reference parameter value is a spheroid-
         based coordinate system, the line is a great elliptic. If the Spatial
         Reference parameter value is a sphere-based coordinate system, the
         line is uniquely called a great circle-a circle of the largest radius
         on the spherical surface.

         * RHUMB_LINE-A type of geodetic line, also known as a loxodrome line,
         that represents a path between any two points on the surface of a
         spheroid defined by a constant azimuth from a pole will be
         constructed. A rhumb line is shown as a straight line in the Mercator
         projection.

         * NORMAL_SECTION-A type of geodetic line that represents a path
         between any two points on the surface of a spheroid defined by the
         intersection of the spheroid surface and a plane that passes through
         the two points and is normal (perpendicular) to the spheroid surface
         at the starting point of the two points will be constructed. The
         normal section line from point A to point B is different from the line
         from point B to point A.

         * PLANAR-A straight line in the projected plane will be used. A planar
         line usually does not accurately represent the shortest distance on
         the surface of the earth as a geodesic line does. This option is not
         available for geographic coordinate systems.
     id_field {Field}:
         A field in the input table. This field and the values are included in
         the output and can be used to join the output features with the
         records in the input table.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature class. A spatial
         reference can be specified as any of the following:

         * The path to a .prj file, such as C:/workspace/watershed.prj

         * The path to a feature class or feature dataset whose spatial
         reference you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland

         * A SpatialReference object, such as
         arcpy.SpatialReference("C:/data/Africa/Carthage.prj")
     attributes {Boolean}:
         Specifies whether the remaining input fields will be added to the
         output feature class.

         * NO_ATTRIBUTES-The remaining input fields will not be added to the
         output feature class. This is the default.

         * ATTRIBUTES-The remaining input fields will be added to the output
         feature class. A new field, ORIG_FID, will also be added to the output
         feature class to store the input feature ID values.

    OUTPUTS:
     out_featureclass (Feature Class):
         The output feature class containing geodetic or planar lines."""
    ...

@gptooldoc("CalculateGeometryAttributes_management", None)
def CalculateGeometryAttributes(
    in_features=...,
    geometry_property=...,
    length_unit=...,
    area_unit=...,
    coordinate_system=...,
    coordinate_format=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateGeometryAttributes_management(in_features, geometry_property;geometry_property..., {length_unit}, {area_unit}, {coordinate_system}, {coordinate_format})

       Adds information to a feature's attribute fields representing the
       spatial or geometric characteristics and location of each feature,
       such as length or area and x-, y-, z-coordinates, and m-values.

    INPUTS:
     in_features (Feature Layer):
         The features with a field that will be updated with geometry
         calculations.
     geometry_property (Value Table):
         The fields in which the specified geometry properties will be
         calculated.You can select an existing field or provide a new field
         name. If a new
         field name is provided, the field type is determined by the type of
         values that are written to the field. Count attributes are written to
         long integer fields; area, length, and x-, y-, z-coordinate, and
         m-value attributes are written to double fields; and coordinate
         notations such as Degrees Minutes Seconds or MGRS are written to text
         fields.Unless otherwise noted, area and length properties are planar
         measurements using 2D Cartesian mathematics.

         * AREA-An attribute will be added to store the area of each polygon
         feature.

         * AREA_GEODESIC-An attribute will be added to store the shape-
         preserving geodesic area of each polygon feature.

         * CENTROID_X-An attribute will be added to store the centroid
         x-coordinate of each feature.

         * CENTROID_Y-An attribute will be added to store the centroid
         y-coordinate of each feature.

         * CENTROID_Z-An attribute will be added to store the centroid
         z-coordinate of each feature.

         * CENTROID_M-An attribute will be added to store the centroid m-value
         of each feature.

         * INSIDE_X-An attribute will be added to store the x-coordinate of a
         central point inside or on each feature. This point is the same as the
         centroid if the centroid is inside the feature; otherwise, it is an
         inner label point.

         * INSIDE_Y-An attribute will be added to store the y-coordinate of a
         central point inside or on each feature. This point is the same as the
         centroid if the centroid is inside the feature; otherwise, it is an
         inner label point.

         * INSIDE_Z-An attribute will be added to store the z-coordinate of a
         central point inside or on each feature. This point is the same as the
         centroid if the centroid is inside the feature; otherwise, it is an
         inner label point.

         * INSIDE_M-An attribute will be added to store the m-value of a
         central point inside or on each feature. This point is the same as the
         centroid if the centroid is inside the feature; otherwise, it is an
         inner label point.

         * CURVE_COUNT-An attribute will be added to store the number of curves
         in each feature. Curves include elliptical arcs, circular arcs, and
         Bezier curves.

         * HOLE_COUNT-An attribute will be added to store the number of
         interior holes within each polygon feature.

         * EXTENT_MIN_X-An attribute will be added to store the minimum
         x-coordinate of each feature's extent.

         * EXTENT_MIN_Y-An attribute will be added to store the minimum
         y-coordinate of each feature's extent.

         * EXTENT_MIN_Z-An attribute will be added to store the minimum
         z-coordinate of each feature's extent.

         * EXTENT_MAX_X-An attribute will be added to store the maximum
         x-coordinate of each feature's extent.

         * EXTENT_MAX_Y-An attribute will be added to store the maximum
         y-coordinate of each feature's extent.

         * EXTENT_MAX_Z-An attribute will be added to store the maximum
         z-coordinate of each feature's extent.

         * LENGTH-An attribute will be added to store the length of each line
         feature.

         * LENGTH_GEODESIC-An attribute will be added to store the shape-
         preserving geodesic length of each line feature.

         * LENGTH_3D-An attribute will be added to store the 3D length of each
         line feature.

         * LINE_BEARING-An attribute will be added to store the start-to-end
         bearing of each line feature. Values range from 0 to 360, with 0
         meaning north, 90 east, 180 south, 270 west, and so on.

         * LINE_START_X-An attribute will be added to store the x-coordinate of
         the start point of each line feature.

         * LINE_START_Y-An attribute will be added to store the y-coordinate of
         the start point of each line feature.

         * LINE_START_Z-An attribute will be added to store the z-coordinate of
         the start point of each line feature.

         * LINE_START_M-An attribute will be added to store the m-value of the
         start point of each line feature.

         * LINE_END_X-An attribute will be added to store the x-coordinate of
         the end point of each line feature.

         * LINE_END_Y-An attribute will be added to store the y-coordinate of
         the end point of each line feature.

         * LINE_END_Z-An attribute will be added to store the z-coordinate of
         the end point of each line feature.

         * LINE_END_M-An attribute will be added to store the m-value of the
         end point of each line feature.

         * PART_COUNT-An attribute will be added to store the number of parts
         composing each feature.

         * POINT_COUNT-An attribute will be added to store the number of points
         or vertices composing each feature.

         * PERIMETER_LENGTH-An attribute will be added to store the length of
         the perimeter or border of each polygon feature.

         * PERIMETER_LENGTH_GEODESIC-An attribute will be added to store the
         shape-preserving geodesic length of the perimeter or border of each
         polygon feature.

         * POINT_X-An attribute will be added to store the x-coordinate of each
         point feature.

         * POINT_Y-An attribute will be added to store the y-coordinate of each
         point feature.

         * POINT_Z-An attribute will be added to store the z-coordinate of each
         point feature.

         * POINT_M-An attribute will be added to store the m-value of each
         point feature.

         * POINT_COORD_NOTATION-An attribute will be added to store the x- and
         y-coordinate of each point feature formatted as a specified coordinate
         notation.
     length_unit {String}:
         Specifies the unit that will be used to calculate length.

         * KILOMETERS-The length unit will be kilometers.

         * METERS-The length unit will be meters.

         * MILES_INT-The length unit will be statute miles.

         * NAUTICAL_MILES_INT-The length unit will be international nautical
         miles.

         * YARDS_INT-The length unit will be international yards.

         * FEET_INT-The length unit will be international feet.

         * MILES_US-The length unit will be US survey miles.

         * NAUTICAL_MILES-The length unit will be US survey nautical miles.

         * YARDS-The length unit will be US survey yards.

         * FEET_US-The length unit will be US survey feet.
     area_unit {String}:
         Specifies the unit that will be used to calculate area.

         * SQUARE_KILOMETERS-The area unit will be square kilometers.

         * HECTARES-The area unit will be hectares.

         * SQUARE_METERS-The area unit will be square meters.

         * SQUARE_MILES_INT-The area unit will be square statute miles.

         * SQUARE_NAUTICAL_MILES-The area unit will be square international
         nautical miles.

         * ACRES-The area unit will be international acres.

         * SQUARE_YARDS-The area unit will be square international yards.

         * SQUARE_FEET_INT-The area unit will be square international feet.

         * SQUARE_MILES_US-The area unit will be square US survey miles.

         * SQUARE_NAUTICAL_MILES_US-The area unit will be square US survey
         nautical miles.

         * ACRES_US-The area unit will be US survey acres.

         * SQUARE_YARDS_US-The area unit will be square US survey yards.

         * SQUARE_FEET_US-The area unit will be square US survey feet.
     coordinate_system {Coordinate System}:
         The coordinate system in which the coordinates, length, and area will
         be calculated. The coordinate system of the input features is used by
         default.
     coordinate_format {String}:
         Specifies the coordinate format in which the x- and y-coordinates will
         be calculated. The coordinate format matching the input features'
         spatial reference units is used by default.Several coordinate formats,
         including Degrees Minutes Seconds, Degrees
         Decimal Minutes, and others, require the calculation to be performed
         in a text field.

         * SAME_AS_INPUT-The input features' spatial reference units will be
         used for coordinate formatting. This is the default.

         * DD-The coordinate format will be Decimal Degrees.

         * DMS_DIR_LAST-The coordinate format will be Degrees Minutes Seconds
         with cardinal direction component at the end (DDD° MM' SSS.ss"
         <N|S|E|W>).

         * DMS_DIR_FIRST-The coordinate format will be Degrees Minutes Seconds
         with cardinal direction component at the beginning (<N|S|E|W> DDD° MM'
         SSS.ss").

         * DMS_POS_NEG-The coordinate format will be Degrees Minutes Seconds
         with positive or negative direction component at the beginning (<+|->
         DDD° MM' SSS.ss").

         * DMS_PACKED-The coordinate format will be Degrees Minutes Seconds
         packed into a single value with positive or negative direction
         component at the beginning (<+|-> DDD.MMSSSss).

         * DDM_DIR_LAST-The coordinate format will be Degrees Decimal Minutes
         with cardinal direction component at the end (DDD° MM.mmm' <N|S|E|W>).

         * DDM_DIR_FIRST-The coordinate format will be Degrees Decimal Minutes
         with cardinal direction component at the beginning (<N|S|E|W> DDD°
         MM.mmm').

         * DDM_POS_NEG-The coordinate format will be Degrees Decimal Minutes
         with positive or negative direction component at the beginning (<+|->
         DDD° MM.mmm').

         * GARS-The coordinate format will be Global Area Reference System. The
         Global Area Reference System is based on latitude and longitude,
         dividing and subdividing the world into cells.

         * GEOREF-The coordinate format will be World Geographic Reference
         System. The World Geographic Reference System is based on the
         geographic system of latitude and longitude, but using a simpler and
         more flexible notation.

         * MGRS-The coordinate format will be Military Grid Reference System.

         * USNG-The coordinate format will be United States National Grid.

         * UTM-The coordinate format will be Universal Transverse Mercator.

         * UTMNS-The coordinate format will be Universal Transverse Mercator
         with no spaces."""
    ...

@gptooldoc("CheckGeometry_management", None)
def CheckGeometry(
    in_features=..., out_table=..., validation_method=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CheckGeometry_management(in_features;in_features..., {out_table}, {validation_method})

       Generates a report of geometry problems in a feature class.

    INPUTS:
     in_features (Feature Layer):
         The feature class or layer to be processed.A Desktop Basic license
         only allows shapefiles and feature classes
         stored in a file geodatabase, GeoPackage, or SpatiaLite database as
         valid input feature formats. A Desktop Standard or Desktop Advanced
         license also allows feature classes stored in an enterprise database
         or enterprise geodatabase to be used as valid input feature formats.
     validation_method {String}:
         Specifies the geometry validation method that will be used to identify
         geometry problems.

         * ESRI-The Esri geometry validation method will be used. This is the
         default.

         * OGC-The OGC geometry validation method will be used.

    OUTPUTS:
     out_table {Table}:
         The report (as a table) of the problems discovered."""
    ...

@gptooldoc("CopyFeatures_management", None)
def CopyFeatures(
    in_features=...,
    out_feature_class=...,
    config_keyword=...,
    spatial_grid_1=...,
    spatial_grid_2=...,
    spatial_grid_3=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CopyFeatures_management(in_features, out_feature_class, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})

       Copies features from the input feature class or layer to a new feature
       class.

    INPUTS:
     in_features (Feature Layer):
         The features to be copied.
     config_keyword {String}:
         Geodatabase configuration keyword to be applied if the output is a
         geodatabase.
     spatial_grid_1 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.
     spatial_grid_2 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.
     spatial_grid_3 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         is ignored.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class which will be created and to which the features will
         be copied."""
    ...

@gptooldoc("DeleteFeatures_management", None)
def DeleteFeatures(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteFeatures_management(in_features)

       Deletes all or the selected subset of features from the input.

    INPUTS:
     in_features (Feature Layer):
         The feature class, shapefile, or layer containing features to be
         deleted."""
    ...

@gptooldoc("Dice_management", None)
def Dice(
    in_features=..., out_feature_class=..., vertex_limit=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Dice_management(in_features, out_feature_class, vertex_limit)

       Subdivides a feature into smaller features based on a specified vertex
       limit. This tool is intended as a way to subdivide extremely large
       features that cause issues with drawing, analysis, editing, and/or
       performance but are difficult to split up with standard editing and
       geoprocessing tools. This tool should not be used in any cases other
       than those where tools are failing to complete successfully due to the
       size of features.

    INPUTS:
     in_features (Feature Layer):
         The input feature class or feature layer. The geometry type must be
         multipoint, line, or polygon.
     vertex_limit (Long):
         Features with geometries that exceed this vertex limit will be
         subdivided before being written to the output feature class.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class of diced features."""
    ...

@gptooldoc("FeatureEnvelopeToPolygon_management", None)
def FeatureEnvelopeToPolygon(
    in_features=..., out_feature_class=..., single_envelope=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureEnvelopeToPolygon_management(in_features, out_feature_class, {single_envelope})

       Creates a feature class containing polygons, each of which represents
       the envelope of an input feature.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be multipoint, line, polygon, or
         annotation.
     single_envelope {Boolean}:
         Specifies whether to use one envelope for each entire multipart
         feature or one envelope per part of a multipart feature. This
         parameter will affect the results of multipart input features only.

         * SINGLEPART-Uses one envelope containing an entire multipart feature;
         therefore, the resulting polygon will be singlepart. This is the
         default.

         * MULTIPART-Uses one envelope for each part of a multipart feature;
         the resulting polygon of the multipart feature will remain multipart.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output polygon feature class."""
    ...

@gptooldoc("FeatureToLine_management", None)
def FeatureToLine(
    in_features=..., out_feature_class=..., cluster_tolerance=..., attributes=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureToLine_management(in_features;in_features..., out_feature_class, {cluster_tolerance}, {attributes})

       Creates a feature class containing lines generated by converting
       polygon boundaries to lines, or splitting line, polygon, or both
       features at their intersections.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be line or polygon, or both.
     cluster_tolerance {Linear Unit}:
         The minimum distance separating all feature coordinates, and the
         distance a coordinate can move in X, Y, or both during spatial
         computation. The default XY tolerance is set to 0.001 meter or its
         equivalent in feature units.Changing this parameter's value may cause
         failure or unexpected
         results. It is recommended that you do not modify this parameter. It
         has been removed from view on the tool dialog box. By default, the
         input feature class's spatial reference x,y tolerance property is
         used.
     attributes {Boolean}:
         Specifies whether to preserve or omit the input attributes in the
         output feature class.

         * ATTRIBUTES-Preserves the input attributes in the output features.
         This is the default.

         * NO_ATTRIBUTES-Omits the input attributes in the output features.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output line feature class."""
    ...

@gptooldoc("FeatureToPoint_management", None)
def FeatureToPoint(
    in_features=..., out_feature_class=..., point_location=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureToPoint_management(in_features, out_feature_class, {point_location})

       Creates a feature class containing points generated from the
       representative locations of input features.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be multipoint, line, polygon, or
         annotation.
     point_location {Boolean}:
         Specifies whether to use representative centers of input features or
         locations contained by input features as the output point locations.

         * CENTROID-Uses the representative center of an input feature as its
         output point location. This point location may not always be contained
         by the input feature. This is the default.

         * INSIDE-Uses a location contained by an input feature as its output
         point location.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output point feature class."""
    ...

@gptooldoc("FeatureToPolygon_management", None)
def FeatureToPolygon(
    in_features=...,
    out_feature_class=...,
    cluster_tolerance=...,
    attributes=...,
    label_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureToPolygon_management(in_features;in_features..., out_feature_class, {cluster_tolerance}, {attributes}, {label_features})

       Creates a feature class containing polygons generated from areas
       enclosed by input line or polygon features.

    INPUTS:
     in_features (Feature Layer):
         The input features, which can be line, polygon, or both.
     cluster_tolerance {Linear Unit}:
         The minimum distance separating all feature coordinates, and the
         distance a coordinate can move in X, Y, or both during spatial
         computation. The default XY tolerance is set to 0.001 meter or its
         equivalent in feature units.Changing this parameter's value may cause
         failure or unexpected
         results. It is recommended that you do not modify this parameter. It
         has been removed from view on the tool dialog box. By default, the
         input feature class's spatial reference x,y tolerance property is
         used.
     attributes {Boolean}:
         This parameter is no longer supported.
     label_features {Feature Layer}:
         The optional input point features that contain the attributes to be
         transferred to the output polygon features.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output polygon feature class."""
    ...

@gptooldoc("FeatureVerticesToPoints_management", None)
def FeatureVerticesToPoints(
    in_features=..., out_feature_class=..., point_location=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FeatureVerticesToPoints_management(in_features, out_feature_class, {point_location})

       Creates a feature class containing points generated from specified
       vertices or locations of the input features.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be line or polygon.
     point_location {String}:
         Specifies where an output point will be created.

         * ALL-A point will be created at each input feature vertex. This is
         the default.

         * MID-A point will be created at the midpoint, not necessarily a
         vertex, of each input line or polygon boundary.

         * START-A point will be created at the start point (first vertex) of
         each input feature.

         * END-A point will be created at the end point (last vertex) of each
         input feature.

         * BOTH_ENDS-Two points will be created, one at the start point and
         another at the endpoint of each input feature.

         * DANGLE-A dangle point will be created for any start or end point of
         an input line, if that point is not connected to another line at any
         location along that line. This option does not apply to polygon input.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output point feature class."""
    ...

@gptooldoc("GeodeticDensify_management", None)
def GeodeticDensify(
    in_features=..., out_feature_class=..., geodetic_type=..., distance=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeodeticDensify_management(in_features, out_feature_class, geodetic_type, {distance})

       Creates new features by replacing input feature's segments with
       densified approximations of geodesic segments. Four type of geodesic
       segments can be constructed: Geodesic, Great Elliptic, Loxodrome, and
       Normal Section.

    INPUTS:
     in_features (Feature Layer):
         The input line or polygon features.
     geodetic_type (String):
         Specifies the type of geodetic segment to construct. Geodetic
         calculations are performed on the ellipsoid associated with the input
         data's coordinate system.

         * GEODESIC-The shortest distance between two points on the surface of
         the spheroid (ellipsoid).

         * LOXODROME-The line of equal azimuth (from a pole) connecting the two
         points.

         * GREAT_ELLIPTIC-The line made by the intersection of a plane that
         contains the center of the spheroid and the two points.

         * NORMAL_SECTION-The line made by the intersection of a plane that
         contains the center of the spheroid and is perpendicular to the
         surface at the first point.
     distance {Linear Unit}:
         The distance between vertices along the output geodesic segment. The
         default value is 50 kilometers.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class containing the densified geodesic features."""
    ...

@gptooldoc("MinimumBoundingGeometry_management", None)
def MinimumBoundingGeometry(
    in_features=...,
    out_feature_class=...,
    geometry_type=...,
    group_option=...,
    group_field=...,
    mbg_fields_option=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MinimumBoundingGeometry_management(in_features, out_feature_class, {geometry_type}, {group_option}, {group_field;group_field...}, {mbg_fields_option})

       Creates a feature class containing polygons which represent a
       specified minimum bounding geometry enclosing each input feature or
       each group of input features.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be point, multipoint, line, polygon, or
         multipatch.
     geometry_type {String}:
         Specifies what type of minimum bounding geometry the output polygons
         will represent.

         * RECTANGLE_BY_AREA-The rectangle of the smallest area enclosing an
         input feature. This is the default.

         * RECTANGLE_BY_WIDTH-The rectangle of the smallest width enclosing an
         input feature.

         * CONVEX_HULL-The smallest convex polygon enclosing an input feature.

         * CIRCLE-The smallest circle enclosing an input feature envelope.

         * ENVELOPE-The envelope of an input feature.
     group_option {String}:
         Specifies how the input features will be grouped; each group will be
         enclosed with one output polygon.

         * NONE-Input features will not be grouped. This is the default. This
         option is not available for point input.

         * ALL-All input features will be treated as one group.

         * LIST-Input features will be grouped based on their common values in
         the specified field or fields in the group field parameter.
     group_field {Field}:
         The field or fields in the input features that will be used to group
         features, when LIST is specified as group_option. At least one group
         field is required for LIST option. All features that have the same
         value in the specified field or fields will be treated as a group.
     mbg_fields_option {Boolean}:
         Specifies whether to add the geometric attributes in the output
         feature class or omit them in the output feature class.

         * NO_MBG_FIELDS-Omits any input attributes in the output feature
         class. This is the default.

         * MBG_FIELDS-Adds the geometric attributes in the output feature
         class.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output polygon feature class."""
    ...

@gptooldoc("MultipartToSinglepart_management", None)
def MultipartToSinglepart(
    in_features=..., out_feature_class=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MultipartToSinglepart_management(in_features, out_feature_class)

       Creates a feature class containing singlepart features generated by
       separating multipart input features.

    INPUTS:
     in_features (Feature Layer):
         The input features that can be any feature type.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class containing features that vary with input
         feature type."""
    ...

@gptooldoc("PointsToLine_management", None)
def PointsToLine(
    Input_Features=...,
    Output_Feature_Class=...,
    Line_Field=...,
    Sort_Field=...,
    Close_Line=...,
    Line_Construction_Method=...,
    Attribute_Source=...,
    Transfer_Fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PointsToLine_management(Input_Features, Output_Feature_Class, {Line_Field}, {Sort_Field}, {Close_Line}, {Line_Construction_Method}, {Attribute_Source}, {Transfer_Fields;Transfer_Fields...})

       Creates line features from points.

    INPUTS:
     Input_Features (Feature Layer):
         The point features that will be used to construct lines.
     Line_Field {Field}:
         The field that will be used to identify unique attribute values so
         line features can be constructed using points of the same values.If no
         field is specified, lines will be constructed without using
         unique attribute values. This is the default.
     Sort_Field {Field}:
         The field that will be used to sort the order of the points.If no
         field is specified, points used to create output line features
         will be sorted in the order they are found. This is the default.
     Close_Line {Boolean}:
         Specifies whether the output line features will be closed.

         * CLOSE-For a continuous line, an extra segment connecting the last
         point with the first point will be included to form a closed line. For
         two-point lines, an extra line feature connecting the last point with
         the first point will be included to form a closed shape.

         * NO_CLOSE-No extra segment or line will be created to ensure a closed
         line or closed shape. This is the default.
     Line_Construction_Method {String}:
         Specifies the method that will be used to construct the line features.

         * CONTINUOUS-Line features will be created by connecting points
         continuously. This is the default.

         * TWO_POINT-Line features will be created by connecting two
         consecutive points.
     Attribute_Source {String}:
         Specifies how the specified attributes will be transferred.

         * NONE-No attributes will be transferred. This is the default.

         * BOTH_ENDS-The attributes from the start and end points of the line
         will be transferred.

         * START-The attributes from the start point of the line will be
         transferred.

         * END-The attributes from the end point of the line will be
         transferred.
     Transfer_Fields {Field}:
         The fields containing values that will be transferred from the source
         points to the output lines. If no fields are selected, no attributes
         will be transferred.If the Attribute_Source parameter value is
         specified as NONE, this
         parameter will be disabled.

    OUTPUTS:
     Output_Feature_Class (Feature Class):
         The line feature class that will be created from the input points."""
    ...

@gptooldoc("PolygonToLine_management", None)
def PolygonToLine(
    in_features=..., out_feature_class=..., neighbor_option=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PolygonToLine_management(in_features, out_feature_class, {neighbor_option})

       Creates a feature class containing lines that are converted from
       polygon boundaries with or without considering neighboring polygons.

    INPUTS:
     in_features (Feature Layer):
         The input features that must be polygon.
     neighbor_option {Boolean}:
         Specifies whether or not to identify and store polygon neighboring
         information.

         * IDENTIFY_NEIGHBORS-Polygon neighboring relationship will be
         identified and stored in the output. If different segments of a
         polygon share boundary with different polygons, the boundary will be
         split such that each uniquely shared segment will become a line with
         its two neighboring polygon FIDs stored in the output. This is the
         default.

         * IGNORE_NEIGHBORS-Polygon neighboring relationship will be ignored;
         every polygon boundary will become a line feature with its original
         polygon feature ID stored in the output.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output line feature class."""
    ...

@gptooldoc("RepairGeometry_management", None)
def RepairGeometry(
    in_features=..., delete_null=..., validation_method=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RepairGeometry_management(in_features, {delete_null}, {validation_method})

       Inspects features for geometry problems and repairs them. If a problem
       is found, a repair will be performed, and a one-line description will
       identify the feature, as well as the geometry problem that was
       repaired.

    INPUTS:
     in_features (Feature Layer):
         The feature class or layer to be processed.A Desktop Basic license
         only allows shapefiles and feature classes
         stored in a file geodatabase, GeoPackage, or SpatiaLite database as
         valid input feature formats. A Desktop Standard or Desktop Advanced
         license also allows feature classes stored in an enterprise database
         or enterprise geodatabase to be used as valid input feature formats.
     delete_null {Boolean}:
         Specifies whether features with null geometries will be deleted.

         * DELETE_NULL-Features with null geometry will be deleted from the
         input. This is the default.

         * KEEP_NULL-Features with null geometry will not be deleted from the
         input.
         Only KEEP_NULL is valid for inputs from an enterprise database,
         enterprise geodatabase, GeoPackage, or SpatiaLite database.
     validation_method {String}:
         Specifies the geometry validation method that will be used to identify
         geometry problems.

         * ESRI-The Esri geometry validation method will be used. This is the
         default.

         * OGC-The OGC geometry validation method will be used."""
    ...

@gptooldoc("SplitLine_management", None)
def SplitLine(
    in_features=..., out_feature_class=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SplitLine_management(in_features, out_feature_class)

       Creates a polyline feature class by splitting input lines or polygons
       at their vertices.

    INPUTS:
     in_features (Feature Layer):
         The input line or polygon features.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output line feature class."""
    ...

@gptooldoc("SplitLineAtPoint_management", None)
def SplitLineAtPoint(
    in_features=..., point_features=..., out_feature_class=..., search_radius=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SplitLineAtPoint_management(in_features, point_features, out_feature_class, {search_radius})

       Splits line features based on intersection or proximity to point
       features.

    INPUTS:
     in_features (Feature Layer):
         The input line features that will be split.
     point_features (Feature Layer):
         The input point features whose locations will be used to split the
         input lines.
     search_radius {Linear Unit}:
         The distance that will be used to split lines by their proximity to
         point features. Points within the search distance to an input line
         will be used to split those lines at the nearest location to the point
         along the line segment.If this parameter is not specified, the single
         nearest point will be
         used to split the line feature. If a radius is specified, all points
         within the radius will be used to split the line.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class that will contain the split lines."""
    ...

@gptooldoc("SubdividePolygon_management", None)
def SubdividePolygon(
    in_polygons=...,
    out_feature_class=...,
    method=...,
    num_areas=...,
    target_area=...,
    target_width=...,
    split_angle=...,
    subdivision_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SubdividePolygon_management(in_polygons, out_feature_class, method, {num_areas}, {target_area}, {target_width}, {split_angle}, {subdivision_type})

       Divides polygon features into a number of equal areas or parts.

    INPUTS:
     in_polygons (Feature Layer):
         The polygon features to be subdivided.
     method (String):
         Specifies the method that will be used to divide the polygons.

         * NUMBER_OF_EQUAL_PARTS-Polygons will be divided evenly into a number
         of parts. This is the default.

         * EQUAL_AREAS-Polygons will be divided into a specified number of
         parts of a certain area, and a remainder part.
     num_areas {Long}:
         The number of areas into which the polygon will be divided if the
         NUMBER_OF_EQUAL_PARTS subdivision method is specified.
     target_area {Areal Unit}:
         The area of the equal parts if the EQUAL_AREAS subdivision method is
         specified. If the target_area is larger than the area of the input
         polygon, the polygon will not be subdivided.
     target_width {Linear Unit}:
         This parameter is not yet supported.
     split_angle {Double}:
         The angle that will be used to draw the lines that divide the polygon.
         The default is 0.
     subdivision_type {String}:
         Specifies how the polygons will be divided.

         * STRIPS-Polygons will be divided into strips. This is the default.

         * STACKED_BLOCKS-Polygons will be divided into stacked blocks.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class of subdivided polygons."""
    ...

@gptooldoc("TableToEllipse_management", None)
def TableToEllipse(
    in_table=...,
    out_featureclass=...,
    x_field=...,
    y_field=...,
    major_field=...,
    minor_field=...,
    distance_units=...,
    azimuth_field=...,
    azimuth_units=...,
    id_field=...,
    spatial_reference=...,
    attributes=...,
    geometry_type=...,
    method=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TableToEllipse_management(in_table, out_featureclass, x_field, y_field, major_field, minor_field, distance_units, {azimuth_field}, {azimuth_units}, {id_field}, {spatial_reference}, {attributes}, {geometry_type}, {method})

       Creates a feature class containing geodetic or planar ellipses from
       the values in an x-coordinate field, y-coordinate field, major axis
       and minor axis fields, and azimuth field of a table.

    INPUTS:
     in_table (Table View):
         The input table. It can be a text file, CSV file, Excel file, dBASE
         table, or geodatabase table.
     x_field (Field):
         A numerical field in the input table containing the x-coordinates (or
         longitudes) of the center points of ellipses to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     y_field (Field):
         A numerical field in the input table containing the y-coordinates (or
         latitudes) of the center points of ellipses to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     major_field (Field):
         A numerical field in the input table containing major axis lengths of
         the ellipses.
     minor_field (Field):
         A numerical field in the input table containing minor axis lengths of
         the ellipses.
     distance_units (String):
         Specifies the units that will be used for the major_field and
         minor_field parameters.

         * METERS-The units will be meters.

         * KILOMETERS-The units will be kilometers.

         * MILES-The units will be miles.

         * NAUTICAL_MILES-The units will be nautical miles.

         * FEET-The units will be feet.

         * US_SURVEY_FEET-The units will be U.S. survey feet.
     azimuth_field {Field}:
         A numerical field in the input table containing azimuth angle values
         for the major axis rotations of the output ellipses. The values are
         measured clockwise from north.
     azimuth_units {String}:
         Specifies the units that will be used for the azimuth_field parameter.

         * DEGREES-The units will be decimal degrees. This is the default.

         * MILS-The units will be mils.

         * RADS-The units will be radians.

         * GRADS-The units will be gradians.
     id_field {Field}:
         A field in the input table. This field and the values are included in
         the output and can be used to join the output features with the
         records in the input table.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature class. A spatial
         reference can be specified as any of the following:

         * The path to a .prj file, such as C:/workspace/watershed.prj

         * The path to a feature class or feature dataset whose spatial
         reference you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland

         * A SpatialReference object, such as
         arcpy.SpatialReference("C:/data/Africa/Carthage.prj")
     attributes {Boolean}:
         Specifies whether the remaining input fields will be added to the
         output feature class.

         * NO_ATTRIBUTES-The remaining input fields will not be added to the
         output feature class. This is the default.

         * ATTRIBUTES-The remaining input fields will be added to the output
         feature class. A new field, ORIG_FID, will also be added to the output
         feature class to store the input feature ID values.
     geometry_type {String}:
         Specifies the geometry type for the output feature class.

         * LINE-An output polyline feature class will be created. This is the
         default.

         * POLYGON-An output polygon feature class will be created.

         * LINE-Line

         * POLYGON-Polygon
     method {String}:
         Specifies whether the ellipse will be generated based on geodesic or
         planar measurements.

         * GEODESIC-A geodesic ellipse will be generated. The ellipse will
         accurately represent the shape on the surface of the earth. This is
         the default.

         * PLANAR-A planar ellipse will be generated on the projected plane. It
         usually does not accurately represent the shape on the surface of the
         earth as a geodesic ellipse does. This option is not available for
         geographic coordinate systems.

         * GEODESIC-Geodesic

         * PLANAR-Planar

    OUTPUTS:
     out_featureclass (Feature Class):
         The output feature class containing geodetic or planar ellipse."""
    ...

@gptooldoc("UnsplitLine_management", None)
def UnsplitLine(
    in_features=...,
    out_feature_class=...,
    dissolve_field=...,
    statistics_fields=...,
    concatenation_separator=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UnsplitLine_management(in_features, out_feature_class, {dissolve_field;dissolve_field...}, {statistics_fields;statistics_fields...}, {concatenation_separator})

       Aggregates line features that have coincident endpoints and,
       optionally, common attribute values.

    INPUTS:
     in_features (Feature Layer):
         The line features to be aggregated.
     dissolve_field {Field}:
         The field or fields on which features will be aggregated.
     statistics_fields {Value Table}:
         Specifies the numeric field or fields containing the attribute values
         that will be used to calculate the specified statistic. Multiple
         statistic and field combinations can be specified. Null values are
         excluded from all calculations.Text attribute fields can be summarized
         using first and last
         statistics. Numeric attribute fields can be summarized using any
         statistic.Available statistics types are as follows:

         * SUM-The values for the specified field will be added together.

         * MEAN-The average for the specified field will be calculated.

         * MIN-The smallest value for all records of the specified field will
         be found.

         * MAX-The largest value for all records of the specified field will be
         found.

         * RANGE-The range of values (maximum minus minimum) for the specified
         field will be calculated.

         * STD-The standard deviation of values in the specified field will be
         calculated.

         * COUNT-The number of values included in the calculations will be
         found. Each value will be counted except null values. To determine the
         number of null values in a field, create a count on the field in
         question, create a count on a different field that does not contain
         null values (for example, the OID if present), and subtract the two
         values.

         * FIRST-The specified field value of the first record in the input
         will be used.

         * LAST-The specified field value of the last record in the input will
         be used.

         * MEDIAN-The median for all records of the specified field will be
         calculated.

         * VARIANCE-The variance for all records of the specified field will be
         calculated.

         * UNIQUE-The number of unique values of the specified field will be
         counted.

         * CONCATENATE-The values for the specified field will be concatenated.
         The values can be separated using the concatenation_separator
         parameter.
     concatenation_separator {String}:
         A character or characters that will be used to concatenate values when
         the CONCATENATION option is used for the statistics_fields parameter.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class to be created that will contain the aggregated
         features."""
    ...

@gptooldoc("XYTableToPoint_management", None)
def XYTableToPoint(
    in_table=...,
    out_feature_class=...,
    x_field=...,
    y_field=...,
    z_field=...,
    coordinate_system=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """XYTableToPoint_management(in_table, out_feature_class, x_field, y_field, {z_field}, {coordinate_system})

       Creates a point feature class based on x-, y-, and z-coordinates from
       a table.

    INPUTS:
     in_table (Table View):
         The table containing the x- and y-coordinates that define the
         locations of the point features that will be created.
     x_field (Field):
         The field in the input table that contains the x-coordinates (or
         longitude).
     y_field (Field):
         The field in the input table that contains the y-coordinates (or
         latitude).
     z_field {Field}:
         The field in the input table that contains the z-coordinates.
     coordinate_system {Spatial Reference}:
         The coordinate system of the x- and y-coordinates. This will be the
         coordinate system of the output feature class.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class containing the output point features."""
    ...

@gptooldoc("XYToLine_management", None)
def XYToLine(
    in_table=...,
    out_featureclass=...,
    startx_field=...,
    starty_field=...,
    endx_field=...,
    endy_field=...,
    line_type=...,
    id_field=...,
    spatial_reference=...,
    attributes=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """XYToLine_management(in_table, out_featureclass, startx_field, starty_field, endx_field, endy_field, {line_type}, {id_field}, {spatial_reference}, {attributes})

       Creates a feature class containing geodetic or planar line features
       from the values in a start x-coordinate field, start y-coordinate
       field, end x-coordinate field, and end y-coordinate field of a table.

    INPUTS:
     in_table (Table View):
         The input table. It can be a text file, CSV file, Excel file, dBASE
         table, or geodatabase table.
     startx_field (Field):
         A numerical field in the input table containing the x-coordinates (or
         longitudes) of the starting points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     starty_field (Field):
         A numerical field in the input table containing the y-coordinates (or
         latitudes) of the starting points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     endx_field (Field):
         A numerical field in the input table containing the x-coordinates (or
         longitudes) of the ending points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     endy_field (Field):
         A numerical field in the input table containing the y-coordinates (or
         latitudes) of the ending points of lines to be positioned in the
         output coordinate system specified by the spatial_reference parameter.
     line_type {String}:
         Specifies the type of line that will be constructed.

         * GEODESIC-A type of geodetic line that most accurately represents
         the shortest distance between any two points on the surface of the
         earth will be constructed. This is the default.

         * GREAT_CIRCLE-A type of geodetic line that represents the path
         between any two points along the intersection of the surface of the
         earth and a plane that passes through the center of the earth will be
         constructed. If the Spatial Reference parameter value is a spheroid-
         based coordinate system, the line is a great elliptic. If the Spatial
         Reference parameter value is a sphere-based coordinate system, the
         line is uniquely called a great circle-a circle of the largest radius
         on the spherical surface.

         * RHUMB_LINE-A type of geodetic line, also known as a loxodrome line,
         that represents a path between any two points on the surface of a
         spheroid defined by a constant azimuth from a pole will be
         constructed. A rhumb line is shown as a straight line in the Mercator
         projection.

         * NORMAL_SECTION-A type of geodetic line that represents a path
         between any two points on the surface of a spheroid defined by the
         intersection of the spheroid surface and a plane that passes through
         the two points and is normal (perpendicular) to the spheroid surface
         at the starting point of the two points will be constructed. The
         normal section line from point A to point B is different from the line
         from point B to point A.

         * PLANAR-A straight line in the projected plane will be used. A planar
         line usually does not accurately represent the shortest distance on
         the surface of the earth as a geodesic line does. This option is not
         available for geographic coordinate systems.
     id_field {Field}:
         A field in the input table. This field and the values are included in
         the output and can be used to join the output features with the
         records in the input table.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature class. A spatial
         reference can be specified as any of the following:

         * The path to a .prj file, such as C:/workspace/watershed.prj

         * The path to a feature class or feature dataset whose spatial
         reference you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland

         * A SpatialReference object, such as
         arcpy.SpatialReference("C:/data/Africa/Carthage.prj")
     attributes {Boolean}:
         Specifies whether the remaining input fields will be added to the
         output feature class.

         * NO_ATTRIBUTES-The remaining input fields will not be added to the
         output feature class. This is the default.

         * ATTRIBUTES-The remaining input fields will be added to the output
         feature class. A new field, ORIG_FID, will also be added to the output
         feature class to store the input feature ID values.

    OUTPUTS:
     out_featureclass (Feature Class):
         The output feature class containing geodetic or planar lines."""
    ...

@gptooldoc("AddField_management", None)
def AddField(
    in_table=...,
    field_name=...,
    field_type=...,
    field_precision=...,
    field_scale=...,
    field_length=...,
    field_alias=...,
    field_is_nullable=...,
    field_is_required=...,
    field_domain=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddField_management(in_table, field_name, field_type, {field_precision}, {field_scale}, {field_length}, {field_alias}, {field_is_nullable}, {field_is_required}, {field_domain})

       Adds a new field to a table or the table of a feature class or feature
       layer, as well as to rasters with attribute tables.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table to which the specified field will be added. The field
         will be added to the existing input table and will not create a new
         output table.Fields can be added to feature classes in geodatabases,
         shapefiles,
         coverages, stand-alone tables, raster catalogs, rasters with attribute
         tables, and layers.
     field_name (String):
         The name of the field that will be added to the input table.
     field_type (String):
         Specifies the field type of the new field.

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * DATE-The field type will be date. Date fields support date and time
         values.

         * BLOB-The field type will be BLOB. BLOB fields support data stored as
         a long sequence of binary numbers. You need a custom loader or viewer
         or a third-party application to load items into a BLOB field or view
         the contents of a BLOB field.

         * RASTER-The field type will be raster. Raster fields can store raster
         data in or alongside the geodatabase. All ArcGIS software-supported
         raster dataset formats can be stored, but it is recommended that only
         small images be used.

         * GUID-The field type will be GUID. GUID fields store registry-style
         strings consisting of 36 characters enclosed in curly brackets.
         Although the Field object's type property values are not an exact
         match for the keywords used by the Add Field tool's field_type
         parameter, all of the Field object's type values can be used as input
         to this parameter. The different field types are mapped as follows:
         Integer to LONG, String to TEXT, and SmallInteger to SHORT.
     field_precision {Long}:
         The number of digits that can be stored in the field. All digits are
         counted regardless of which side of the decimal they are on.This
         parameter is only applicable to fields of type float, double,
         short, or long.If the input table is a file geodatabase, the field
         precision value
         will be ignored.
     field_scale {Long}:
         The number of decimal places stored in a field.This parameter is only
         applicable to fields of type float or double.If the input table is a
         file geodatabase, the field scale value will
         be ignored.
     field_length {Long}:
         The length of the field. This sets the maximum number of allowable
         characters for each record of the field.This parameter is only
         applicable to fields of type text.
     field_alias {String}:
         The alternate name for the field. This name is used to describe
         cryptic field names. This parameter only applies to geodatabases.
     field_is_nullable {Boolean}:
         Specifies whether the field can contain null values. Null values are
         different from zero or empty fields and are only supported for fields
         in a geodatabase.

         * NULLABLE-The field can contain null values. This is the default.

         * NON_NULLABLE-The field cannot contain null values.
     field_is_required {Boolean}:
         Specifies whether the field being created is a required field for the
         table. Required fields are only supported in a geodatabase.

         * NON_REQUIRED-The field is not a required field. This is the default.

         * REQUIRED-The field is a required field. Required fields are
         permanent and cannot be deleted.
     field_domain {String}:
         Constrains the values allowed in any particular attribute for a table,
         feature class, or subtype in a geodatabase. You must specify the name
         of an existing domain for it to be applied to the field."""
    ...

@gptooldoc("AddFields_management", None)
def AddFields(
    in_table=..., field_description=..., template=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddFields_management(in_table, {field_description;field_description...}, {template;template...})

       Adds new fields to a table, feature class, or raster.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table where the fields will be added. The fields will be
         added to the existing input table and will not create a new output
         table.Fields can be added to feature classes in geodatabases,
         shapefiles,
         coverages, stand-alone tables, raster catalogs, rasters with attribute
         tables, and layers.
     field_description {Value Table}:
         The fields and their properties that will be added to the input table.

         * Field Name-The name of the field that will be added to the input
         table.

         * Field Type-The type of the new field.

         * Field Alias-The alternate name given to the field name. This is used
         to give more descriptive names to cryptic field names. This value only
         applies to geodatabases.

         * Field Length-The length of the field being added. This sets the
         maximum number of allowable characters for each record of the field.
         This option is only applicable to fields of type text; the default
         length is 255.

         * Default Value-The default value of the field.

         * Field Domain-The geodatabase domain that will be assigned to the
         field.
         Available field types are as follows:

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * DATE-The field type will be date. Date fields support date and time
         values.

         * BLOB-The field type will be BLOB. BLOB fields support data stored as
         a long sequence of binary numbers. You need a custom loader or viewer
         or a third-party application to load items into a BLOB field or view
         the contents of a BLOB field.

         * RASTER-The field type will be raster. Raster fields can store raster
         data in or alongside the geodatabase. All ArcGIS software-supported
         raster dataset formats can be stored, but it is recommended that only
         small images be used.

         * GUID-The field type will be GUID. GUID fields store registry-style
         strings consisting of 36 characters enclosed in curly brackets.
         In the field_description parameter with optional parameters, use None
         as an empty place holder.
     template {Table View}:
         The feature classes or tables that will be used as a template to
         define the attribute fields to add.Fields from the inputs specified
         with this parameter will be added to
         the in_table value in addition to any fields specified by the
         field_description parameter."""
    ...

@gptooldoc("AddGPSMetadataFields_management", None)
def AddGPSMetadataFields(
    in_point_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddGPSMetadataFields_management(in_point_features)

       Adds GNSS fields to a feature class in a geodatabase.

    INPUTS:
     in_point_features (Feature Layer):
         The input feature class to be updated. The input can be a point,
         polyline, or polygon feature class."""
    ...

@gptooldoc("AddGlobalIDs_management", None)
def AddGlobalIDs(
    in_datasets=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddGlobalIDs_management(in_datasets;in_datasets...)

       Adds global IDs to a list of geodatabase feature classes, tables, and
       feature datasets.

    INPUTS:
     in_datasets (Layer / Table View / Dataset):
         A list of geodatabase classes, tables, and feature datasets to which
         global IDs will be added."""
    ...

@gptooldoc("AddIncrementingIDField_management", None)
def AddIncrementingIDField(
    in_table=..., field_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddIncrementingIDField_management(in_table, {field_name})

       Adds a database-maintained, incrementing ID field to an existing table
       or feature class in a Dameng, IBM Db2, Microsoft Azure SQL Database,
       Microsoft SQL Server, Oracle, or PostgreSQL database. A database-
       maintained ID field is required for all feature classes or tables you
       plan to edit through a feature service.

    INPUTS:
     in_table (Table View):
         The location and name of the table or feature class to which an ID
         field will be added.
     field_name {String}:
         The name to be used for the ID field. If no input is provided, the
         default ObjectID will be used."""
    ...

@gptooldoc("AlterField_management", None)
def AlterField(
    in_table=...,
    field=...,
    new_field_name=...,
    new_field_alias=...,
    field_type=...,
    field_length=...,
    field_is_nullable=...,
    clear_field_alias=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterField_management(in_table, field, {new_field_name}, {new_field_alias}, {field_type}, {field_length}, {field_is_nullable}, {clear_field_alias})

       Renames fields and field aliases or alters field properties.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input geodatabase table or feature class that contains the field
         to alter.
     field (Field):
         The name of the field to alter. If the field is a required field, only
         the field alias can be altered.
     new_field_name {String}:
         The new name for the field.
     new_field_alias {String}:
         The new field alias for the field.
     field_type {String}:
         Specifies the new field type for the field. This parameter is only
         applicable if the input table is empty (does not contain records).

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * DATE-The field type will be date. Date fields support date and time
         values.

         * BLOB-The field type will be BLOB. BLOB fields support data stored as
         a long sequence of binary numbers. You need a custom loader or viewer
         or a third-party application to load items into a BLOB field or view
         the contents of a BLOB field.

         * RASTER-The field type will be raster. Raster fields can store raster
         data in or alongside the geodatabase. All ArcGIS software-supported
         raster dataset formats can be stored, but it is recommended that only
         small images be used.

         * GUID-The field type will be GUID. GUID fields store registry-style
         strings consisting of 36 characters enclosed in curly brackets.
     field_length {Long}:
         The new length of the field. This sets the maximum number of allowable
         characters for each record of the field. This parameter is only
         applicable to fields of type TEXT or BLOB. If the table is empty, the
         field length can be increased or decreased. If the table is not empty,
         the length can only be increased from the current value.
     field_is_nullable {Boolean}:
         Specifies whether the field can contain null values. Null values are
         only supported for fields in a geodatabase. This parameter is only
         applicable if the input table is empty (does not contain records).

         * NON_NULLABLE-The field can contain null values.

         * NULLABLE-The field cannot contain null values. This is the default.
     clear_field_alias {Boolean}:
         Specifies whether the alias for the input field will be cleared. The
         new_field_alias parameter must be empty to clear the alias of the
         field.

         * CLEAR_ALIAS-The field alias will be cleared (set to null).

         * DO_NOT_CLEAR-The field alias will not be cleared. This is the
         default."""
    ...

@gptooldoc("AssignDefaultToField_management", None)
def AssignDefaultToField(
    in_table=..., field_name=..., default_value=..., subtype_code=..., clear_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AssignDefaultToField_management(in_table, field_name, {default_value}, {subtype_code;subtype_code...}, {clear_value})

       Creates a default value for a specified field. When a new row is added
       to the table or feature class, the specified field will be set to this
       default value.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table or feature class that will have a default value added
         to one of its fields.
     field_name (Field):
         The field to which the default value will be added each time a new row
         is added to the table or feature class.
     default_value {String}:
         The default value to be added to each new table or feature class. The
         value entered must match the data type of the field.
     subtype_code {String}:
         The subtypes that can participate in the default value.
     clear_value {Boolean}:
         Specifies whether the default value for either the field or the
         subtype will be cleared. To clear the default value, the default_value
         parameter must be passed in as an empty string. To clear the default
         value for the subtype, you must also specify the subtype to be
         cleared.

         * CLEAR_VALUE-The default value will be cleared (set to null).

         * DO_NOT_CLEAR-The default value will not be cleared. This is the
         default."""
    ...

@gptooldoc("BatchUpdateFields_management", None)
def BatchUpdateFields(
    in_table=...,
    out_table=...,
    field_definition_table=...,
    script_file=...,
    output_field_name=...,
    source_field_name=...,
    output_field_type=...,
    output_field_decimals_or_length=...,
    output_field_alias=...,
    output_field_script=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BatchUpdateFields_management(in_table, out_table, field_definition_table, {script_file}, {output_field_name}, {source_field_name}, {output_field_type}, {output_field_decimals_or_length}, {output_field_alias}, {output_field_script})

       Transforms fields in a table or feature class based on schema defined
       in the definition table and creates a new table or feature class.

    INPUTS:
     in_table (Table View):
         The input table or feature class.
     field_definition_table (Table View):
         A table containing the field definitions and calculations that will be
         used to create the output.
     script_file {File}:
         A Python file that stores multiple line Python functions to perform
         calculations for the out_table parameter fields.
     output_field_name {Field}:
         The field name from the definition table that contains the target
         field names for the output table.
     source_field_name {Field}:
         The field name from the definition table that contains the source
         field names from the input table.
     output_field_type {Field}:
         The field name from the definition table that defines the data types
         for the output table.
     output_field_decimals_or_length {Field}:
         The field name from the definition table that defines the number of
         decimals or the length of the field for the output fields.
     output_field_alias {Field}:
         The field name from the definition table that defines the alias names
         for the fields of the output table.
     output_field_script {Field}:
         The field name from the definition table that defines the calculations
         for the output fields.

    OUTPUTS:
     out_table (Feature Class / Table):
         The output table or feature class containing the updated fields."""
    ...

@gptooldoc("CalculateEndTime_management", None)
def CalculateEndTime(
    in_table=..., start_field=..., end_field=..., fields=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateEndTime_management(in_table, start_field, end_field, {fields;fields...})

       Calculates the end time of features based on the time values stored in
       another field.

    INPUTS:
     in_table (Table View):
         The feature class or table for which an End Time Field is calculated
         based on the Start Time Field specified.
     start_field (Field):
         The field containing values that will be used to calculate values for
         the End Time Field. The Start Time Field and the End Time Field must
         be of the same type. For example, if the Start Time Field is of type
         LONG, the End Time Field should be of type LONG as well.
     end_field (Field):
         The field that will be populated with values based on the Start Time
         Field specified. The Start Time Field and the End Time Field must be
         of the same format.
     fields {Field}:
         The name of the field or fields that can be used to uniquely identify
         spatial entities. These fields are used to first sort based on entity
         type if there is more than one entity. For instance, for a feature
         class representing population values per state over time, the state
         name could be the unique value field (the entity). If population
         figures are per county, you would need to set the county name and
         state name as the unique value fields, since some county names are the
         same for different states. If there is only one entity, this parameter
         can be ignored."""
    ...

@gptooldoc("CalculateField_management", None)
def CalculateField(
    in_table=...,
    field=...,
    expression=...,
    expression_type=...,
    code_block=...,
    field_type=...,
    enforce_domains=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateField_management(in_table, field, expression, {expression_type}, {code_block}, {field_type}, {enforce_domains})

       Calculates the values of a field for a feature class, feature layer,
       or raster.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the field that will be updated with the new
         calculation.
     field (Field):
         The field that will be updated with the new calculation.If a field
         with the specified name does not exist in the input table,
         it will be added.
     expression (SQL Expression):
         The simple calculation expression used to create a value that will
         populate the selected rows.
     expression_type {String}:
         Specifies the type of expression that will be used.

         * PYTHON3-The Python expression type will be used. This is the
         default.

         * ARCADE-The Arcade expression type will be used.

         * SQL-The SQL expression type will be used.
         To learn more about Python expressions, see Calculate Field Python
         examples.To learn more about Arcade expressions, see the ArcGIS Arcade
         guide.To learn more about SQL expressions, see Calculate field
         values.SQL expressions support faster calculations for feature
         services and
         enterprise geodatabases. Instead of performing calculations one
         feature or row at a time, a single request is sent to the server or
         database, resulting in significantly faster calculations.Only feature
         services and enterprise geodatabases support SQL
         expressions. For other formats, use Python or Arcade expressions.
     code_block {String}:
         A block of code that will be entered for complex expressions.
     field_type {String}:
         Specifies the field type of the new field. This parameter is only used
         when the field name does not exist in the input table.If the field is
         of type text, the new field will have a length of 512.
         For shapefiles and dBASE files, the field will have a length of 254.
         The length of the new field can be adjusted using the Alter Field
         tool.

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * DATE-The field type will be date. Date fields support date and time
         values.

         * BLOB-The field type will be BLOB. BLOB fields support data stored as
         a long sequence of binary numbers. You need a custom loader or viewer
         or a third-party application to load items into a BLOB field or view
         the contents of a BLOB field.

         * RASTER-The field type will be raster. Raster fields can store raster
         data in or alongside the geodatabase. All ArcGIS software-supported
         raster dataset formats can be stored, but it is recommended that only
         small images be used.

         * GUID-The field type will be GUID. GUID fields store registry-style
         strings consisting of 36 characters enclosed in curly brackets.
     enforce_domains {Boolean}:
         Specifies whether field domain rules will be enforced.

         * ENFORCE_DOMAINS-Field domain rules will be enforced.

         * NO_ENFORCE_DOMAINS-Field domain rules will not be enforced. This is
         the default."""
    ...

@gptooldoc("CalculateFields_management", None)
def CalculateFields(
    in_table=..., expression_type=..., fields=..., code_block=..., enforce_domains=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateFields_management(in_table, expression_type, fields;fields..., {code_block}, {enforce_domains})

       Calculates the values of two or more fields for a feature class,
       feature layer, or raster.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the fields that will be updated with the new
         calculations.
     expression_type (String):
         Specifies the type of expression that will be used.

         * PYTHON3-The Python expression type will be used. This is the
         default.

         * ARCADE-The Arcade expression type will be used.

         * SQL-The SQL expression type will be used.
         To learn more about Python expressions, see Calculate Field Python
         examples.To learn more about Arcade expressions, see the ArcGIS Arcade
         guide.To learn more about SQL expressions, see Calculate field values.
     fields (Value Table):
         The fields that will be calculated, their expressions, and a where
         clause.The optional SQL expression will be used to select a subset of
         records. Only records that match this where clause will be calculated.
         If the where clause is left blank, all records will be calculated. For
         more information about SQL syntax, see SQL reference for query
         expressions used in ArcGIS.
     code_block {String}:
         A block of code that will be used for complex expressions.A function
         cannot be used to return multiple values.
     enforce_domains {Boolean}:
         Specifies whether field domain rules will be enforced.

         * ENFORCE_DOMAINS-Field domain rules will be enforced.

         * NO_ENFORCE_DOMAINS-Field domain rules will not be enforced. This is
         the default."""
    ...

@gptooldoc("ConvertTimeField_management", None)
def ConvertTimeField(
    in_table=...,
    input_time_field=...,
    input_time_format=...,
    output_time_field=...,
    output_time_type=...,
    output_time_format=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConvertTimeField_management(in_table, input_time_field, {input_time_format}, output_time_field, {output_time_type}, {output_time_format})

       Transfers date and time values stored in a field to another field. The
       tool can be used to convert between different field types (text,
       numeric, or date fields) or to convert the values to a different
       format such as dd/MM/yy HH:mm:ss to yyyy-MM-dd.

    INPUTS:
     in_table (Table View):
         The layer or table that contains the field containing the time values
         that will be converted.
     input_time_field (Field):
         The field containing the time values. The field can be of type short,
         long, float, double, text, or date.
     input_time_format {String}:
         The format of the time values in the input_time_field parameter value.
         The parameter is not supported when the input time field is of type
         date.The format strings are case sensitive.

         * If the data type of the time field is date, no time format is
         required.

         * If the data type of the time field is numeric (short, long, float,
         or double), a list of standard numeric time formats is provided in the
         drop-down list.

         * If the data type of the time field is string, a list of
         standard string time formats is provided in the drop-down list. For
         string fields, you can also specify a custom time format. For example,
         the time values may have been stored in a string field in one of the
         standard formats such as yyyy/MM/dd HH:mm:ss or in a custom format
         such as dd/MM/yyyy HH:mm:ss. For the custom format, you can also
         specify the a.m. or p.m. designator. Some commonly used formats are
         listed below:

         * yyyy-Year represented by four digits

         * MM-Month as digits with leading zero for single-digit months

         * MMM-Month as a three-letter abbreviation

         * dd-Day of month as digits with leading zero for single-digit days

         * ddd-Day of week as a three-letter abbreviation

         * hh-Hours with leading zero for single-digit hours; 12-hour clock

         * HH-Hours with leading zero for single-digit hours; 24-hour clock

         * mm-Minutes with leading zero for single-digit minutes

         * ss-Seconds with leading zero for single-digit seconds

         * t-One character time marker string, such as A or P

         * tt-Multicharacter time marker string, such as AM or PM

         * unix_us-Unix time in microseconds

         * unix_ms-Unix time in milliseconds

         * unix_s-Unix time in seconds

         * unix_hex-Unix time in hexadecimal
     output_time_field (String):
         The name of the field to be added in which the converted time values
         will be stored.
     output_time_type {String}:
         Specifies the field type of the output time field.

         * DATE-The field type will be date. Date fields support date and time
         values.

         * TEXT-The field type will be text. Text fields support a string of
         characters.

         * LONG-The field type will be long. Long fields support whole numbers
         between -2,147,483,648 and 2,147,483,647.

         * SHORT-The field type will be short. Short fields support whole
         numbers between -32,768 and 32,767.

         * DOUBLE-The field type will be double. Double fields support
         fractional numbers between -2.2E308 and 1.8E308.

         * FLOAT-The field type will be float. Float fields support fractional
         numbers between -3.4E38 and 1.2E38.
     output_time_format {String}:
         The format of the output time values. Supported output time formats
         depend on the output_time_type parameter value. A custom format can
         also be used to convert the value to a different format or to extract
         a portion of the value (such as the year). For a list of custom
         formats, see the input_time_format parameter help. This parameter is
         not used when the output_time_type parameter value is DATE.If the data
         type of the output time field isn't long enough to store
         the converted time value, the output value will be truncated."""
    ...

@gptooldoc("ConvertTimeZone_management", None)
def ConvertTimeZone(
    in_table=...,
    input_time_field=...,
    input_time_zone=...,
    output_time_field=...,
    output_time_zone=...,
    input_dst=...,
    output_dst=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConvertTimeZone_management(in_table, input_time_field, input_time_zone, output_time_field, output_time_zone, {input_dst}, {output_dst})

       Converts time values recorded in a date field from one time zone to
       another time zone.

    INPUTS:
     in_table (Table View):
         The input feature class or table that contains the time stamps that
         will be transformed to a different time zone.
     input_time_field (Field):
         The input field that contains the time stamps that will be transformed
         to a different time zone.
     input_time_zone (String):
         The input time zone in which the time stamps were collected.
     output_time_field (String):
         The output field in which the time stamps transformed to the desired
         output time zone will be stored.
     output_time_zone (String):
         The time zone to which the time stamps will be transformed. By
         default, the output time zone is the same as the input time zone.
     input_dst {Boolean}:
         Specifies whether the time stamps were collected while observing
         Daylight Saving Time rules in the input time zone. When reading the
         time values to convert the time zone, the time values will be adjusted
         to account for the shift in time during Daylight Saving Time.By
         default, the input time values are adjusted to account for the
         shift in time due to the Daylight Saving Time rules in the input time
         zone.

         * INPUT_ADJUSTED_FOR_DST-The input time values are adjusted for
         Daylight Saving Time.

         * INPUT_NOT_ADJUSTED_FOR_DST-The input time values are not adjusted
         for Daylight Saving Time.
     output_dst {Boolean}:
         Indicates whether the output time values will account for the shift in
         time due to the Daylight Saving Time rules observed in the output time
         zone.By default, the output time values are adjusted to account for
         the
         shift in time due to the Daylight Saving Time rules observed in the
         output time zone.

         * OUTPUT_ADJUSTED_FOR_DST-The output time values will be adjusted for
         Daylight Saving Time in the output time zone.

         * OUTPUT_NOT_ADJUSTED_FOR_DST-The output time values will not be
         adjusted for Daylight Saving Time in the output time zone."""
    ...

@gptooldoc("DeleteField_management", None)
def DeleteField(
    in_table=..., drop_field=..., method=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteField_management(in_table, drop_field;drop_field..., {method})

       Deletes one or more fields from a table, feature class, feature layer,
       or raster dataset.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the fields to be deleted. The existing input
         table will be modified.
     drop_field (Field):
         The fields to be dropped or kept from the input table, as specified by
         the method parameter. Only nonrequired fields can be deleted.
     method {String}:
         Specifies whether the fields specified by the drop_field parameter
         will be deleted or kept.

         * DELETE_FIELDS-The fields specified by the drop_field parameter will
         be deleted. This is the default.

         * KEEP_FIELDS-The fields specified by the drop_field parameter will be
         kept; all other fields will be deleted."""
    ...

@gptooldoc("DisableCOGO_management", None)
def DisableCOGO(
    in_line_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableCOGO_management(in_line_features)

       Disables COGO on a line feature class and removes COGO fields and
       COGO-enabled labeling and symbology. COGO fields can be deleted.

    INPUTS:
     in_line_features (Feature Layer):
         The line feature class that will have COGO disabled."""
    ...

@gptooldoc("DisableEditorTracking_management", None)
def DisableEditorTracking(
    in_dataset=..., creator=..., creation_date=..., last_editor=..., last_edit_date=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DisableEditorTracking_management(in_dataset, {creator}, {creation_date}, {last_editor}, {last_edit_date})

       Disables editor tracking on a feature class, table, feature dataset,
       or mosaic dataset.

    INPUTS:
     in_dataset (Dataset / Topology / Network Dataset):
         The feature class, table, feature dataset, or mosaic dataset in which
         editor tracking will be disabled.
     creator {Boolean}:
         Specifies whether editor tracking for the creator field will be
         disabled.

         * DISABLE_CREATOR-Editor tracking for the creator field will be
         disabled. This is the default.

         * NO_DISABLE_CREATOR-Editor tracking for the creator field will not be
         disabled.
     creation_date {Boolean}:
         Specifies whether editor tracking for the creation date field will be
         disabled.

         * DISABLE_CREATION_DATE-Editor tracking for the creation date field
         will be disabled. This is the default.

         * NO_DISABLE_CREATION_DATE-Editor tracking for the creation date field
         will not be disabled.
     last_editor {Boolean}:
         Specifies whether editor tracking for the last editor field will be
         disabled.

         * DISABLE_LAST_EDITOR-Editor tracking for the last editor field will
         be disabled. This is the default.

         * NO_DISABLE_LAST_EDITOR-Editor tracking for the last editor field
         will not be disabled.
     last_edit_date {Boolean}:
         Specifies whether editor tracking for the last edit date field will be
         disabled.

         * DISABLE_LAST_EDIT_DATE-Editor tracking for the last edit date field
         will be disabled. This is the default.

         * NO_DISABLE_LAST_EDIT_DATE-Editor tracking for the last edit date
         field will not be disabled."""
    ...

@gptooldoc("EnableCOGO_management", None)
def EnableCOGO(
    in_line_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableCOGO_management(in_line_features)

       Enables COGO on a line feature class and adds COGO fields and COGO-
       enabled labeling to a line feature class. COGO fields store dimensions
       that are used to create line features in relation to each other.

    INPUTS:
     in_line_features (Feature Layer):
         The line feature class that will be COGO enabled."""
    ...

@gptooldoc("EnableEditorTracking_management", None)
def EnableEditorTracking(
    in_dataset=...,
    creator_field=...,
    creation_date_field=...,
    last_editor_field=...,
    last_edit_date_field=...,
    add_fields=...,
    record_dates_in=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableEditorTracking_management(in_dataset, {creator_field}, {creation_date_field}, {last_editor_field}, {last_edit_date_field}, {add_fields}, {record_dates_in})

       Enables editor tracking for a feature class, table, feature dataset,
       or relationship class in a geodatabase.

    INPUTS:
     in_dataset (Dataset / Topology / Network Dataset):
         The feature class, table, feature dataset, or relationship class in
         which editor tracking will be enabled.
     creator_field {String}:
         The name of the field that will store the names of users who create
         features or records. If this field already exists, it must be a string
         field.
     creation_date_field {String}:
         The name of the field that will store the date that features or
         records are created. If this field already exists, it must be a date
         field.
     last_editor_field {String}:
         The name of the field that will store the names of users who last
         edited features or records. If this field already exists, it must be a
         string field.
     last_edit_date_field {String}:
         The name of the field that will store the date that features or
         records were last edited. If this field already exists, it must be a
         date field.
     add_fields {Boolean}:
         Specifies whether fields will be added if they don't exist.

         * NO_ADD_FIELDS-Fields will not be added. Fields specified must
         already exist in the in_dataset parameter value. This is the default.

         * ADD_FIELDS-Fields will be added if they do not exist. You must
         specify the names of the fields to add in the creator_field,
         creation_date_field, last_editor_field, and last_edit_date_field
         parameters.
     record_dates_in {String}:
         Specifies the time format in which the created date and last edited
         date will be recorded.

         * UTC-Dates will be recorded in UTC. This is the default.

         * DATABASE_TIME-Dates will be recorded in the time zone in which the
         database is located."""
    ...

@gptooldoc("EncodeField_management", None)
def EncodeField(
    in_table=...,
    field=...,
    method=...,
    time_step_interval=...,
    time_step_alignment=...,
    reference_time=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EncodeField_management(in_table, field, {method}, {time_step_interval}, {time_step_alignment}, {reference_time})

       Converts categorical values (string, integer, or date) into multiple
       numerical fields, each representing a category. The encoded numerical
       fields can be used in most data science and statistical workflows
       including regression models.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table or feature class containing the field to be encoded.
         Fields will be added to the existing input table and will not create a
         new output table.
     field (Field):
         The field containing the categorical or temporal values to be encoded.
     method {String}:
         Specifies the method to use to encode the values contained in the
         Field to Encode parameter.

         * ONEHOT-Each categorical value will be converted to a new field and
         the values 0 and 1 will be assigned, where 1 represents the presence
         of that categorical value. This is the default.

         * ONECOLD-Each categorical value will be converted to a new field and
         the values 0 and 1 will be assigned, where 0 represents the presence
         of that categorical value.

         * TEMPORAL-Each temporal value in the Field to Encode parameter will
         be converted to an integer based on the time step interval, time step
         alignment, and reference time specified.
     time_step_interval {Time Unit}:
         The number of seconds, minutes, hours, days, weeks, or years that will
         represent a single time step. The temporal value will be aggregated
         into a certain time step it is within. If no value is provided, the
         default time step interval is based on two algorithms that are used to
         determine the optimal number and width of the time step intervals. The
         smaller of the two results is used as the time step interval.
     time_step_alignment {String}:
         Specifies how aggregation will occur based on the Time Step Interval
         parameter value.

         * END_TIME-Time steps will align to the last time event and aggregate
         back in time. This is the default.

         * START_TIME-Time steps will align to the first time event and
         aggregate forward in time.

         * REFERENCE_TIME-Time steps will align to the date and time specified
         in the Reference Time parameter. Aggregation is performed forward and
         backward in time from the reference time until reaching the first and
         last temporal values.
     reference_time {Date}:
         The date and time to which the time-step intervals will align. For
         example, to bin your data weekly from Monday to Sunday, set a
         reference time of Sunday at midnight to ensure that the time steps
         break between Sunday and Monday at midnight.The value can be a date
         and time or solely a date; it cannot be solely
         a time. The expected format is determined by the computer's regional
         time settings."""
    ...

@gptooldoc("FieldStatisticsToTable_management", None)
def FieldStatisticsToTable(
    in_table=...,
    in_fields=...,
    out_location=...,
    out_tables=...,
    group_by_field=...,
    out_statistics=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FieldStatisticsToTable_management(in_table, in_fields;in_fields..., out_location, out_tables;out_tables..., {group_by_field}, {out_statistics;out_statistics...})

       Creates a table of descriptive statistics for one or more input fields
       in a table or feature class.

    INPUTS:
     in_table (Table View):
         The input table containing the fields that will be used to create the
         statistics table.
     in_fields (Field):
         The fields containing the values that will be used to calculate the
         statistics.
     out_location (Workspace):
         The location where the output tables will be created. The location can
         be a geodatabase, folder, or feature dataset.
     out_tables (Value Table):
         The output tables containing the statistics. The field_type column
         specifies the field types that will be included in each output table,
         and the name of each output table is provided in the output_name
         column. For example, you can create a single table with summaries of
         all field types, or you can create separate tables for summaries of
         Numeric, Text, and Date field types. The following choices are
         available for the field_type column:

         * NUMERIC-A table summarizing numeric fields of the input (Short,
         Long, Float, and Double types) will be created.

         * TEXT-A table summarizing text fields of the input (Text type) will
         be created.

         * DATE-A table summarizing date fields of the input (Date type) will
         be created.

         * ALL-A table summarizing all numeric, text, and date fields of the
         input will be created. Output fields containing statistics that apply
         to multiple field types will be saved as type Text. Output statistics
         that do not apply to Text and Date type fields will be empty.
     group_by_field {Field}:
         The field that will be used to group rows into categories. If a group
         by field is provided, each field of the input will appear as a row in
         the output table once per unique value in the group by field.
     out_statistics {Value Table}:
         Specifies the statistics that will be summarized and the names of the
         output fields containing the statistics. The statistic is provided in
         the out_statistic column, and the name of the output field is provided
         in the output_name column. If no values are provided, all applicable
         statistics will be calculated for all input fields. The
         following choices are available for the out_statistic
         column (only statistics applicable to the input fields will be
         available):

         * FIELDNAME-The name of the field.

         * ALIAS-The alias of the field.

         * FIELDTYPE-The field type of the field (Short, Long, Double, Float,
         Text, or Date).

         * NULLS-The number of records containing null values of the field.

         * MINIMUM-The smallest value in the field.

         * MAXIMUM-The largest value in the field.

         * MEAN-The mean (sum divided by total count) of all values in the
         field. To calculate the mean date for date fields, each date is
         converted to a number by calculating the difference between the date
         and a reference date (for example, 1900-01-01), calculated in
         milliseconds.

         * STANDARDDEVIATION-The standard deviation of the values in the field.
         It is calculated as the square root of the variance, in which the
         variance is the average squared difference of each value from the mean
         of the field.

         * MEDIAN-The median for all values in the field. The median is the
         middle value in the sorted list of values. If there is an even number
         of values, the median is the mean of the two middle values in the
         distribution.

         * COUNT-The number of nonnull values in the field.

         * NUMBEROFUNIQUEVALUES-The number of unique values in the field.

         * MODE-The most frequently occurring value in the field.

         * LEASTCOMMON-The least common value in the field.

         * OUTLIERS-The number of records with outlier values in the field.
         Outliers are values that are more than 1.5 times the interquartile
         range above the third quartile or below the first quartile of the
         values of the field.

         * SUM-The sum of all values in the field.

         * RANGE-The difference between the largest and smallest values in the
         field.

         * INTERQUARTILERANGE-The range between the first quartile and the
         third quartile of the values in the field. This represents the range
         of the middle half of the data.

         * FIRSTQUARTILE-The value of the first quartile of the field.
         Quartiles divide the sorted list of values into four groups containing
         equal numbers of values. The first quartile is the upper limit of the
         first group in ascending order.

         * THIRDQUARTILE-The value of the third quartile of the field.
         Quartiles divide the sorted list of values into four groups containing
         equal numbers of values. The third quartile is the upper limit of the
         third group in ascending order.

         * COEFFICIENTOFVARIATION-The coefficient of variation of the values in
         the field. The coefficient of variation is a measure of the relative
         spread of the values. It is calculated as the standard deviation
         divided by the mean of the field.

         * SKEWNESS-The skewness of the values in the field. Skewness measures
         the symmetry of the distribution. The skewness is calculated as the
         third moment (the average of the cubed data values) divided by the
         cubed standard deviation.

         * KURTOSIS-The kurtosis of the values in the field. Kurtosis describes
         the heaviness of the tails of a distribution compared to the normal
         distribution, helping identify the frequency of extreme values. The
         kurtosis is calculated as the fourth moment (the average of the data
         values taken to the fourth power) divided by the fourth power of the
         standard deviation."""
    ...

@gptooldoc("ReclassifyField_management", None)
def ReclassifyField(
    in_table=...,
    field=...,
    method=...,
    classes=...,
    interval=...,
    standard_deviations=...,
    reclass_table=...,
    reverse_values=...,
    output_field_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReclassifyField_management(in_table, field, {method}, {classes}, {interval}, {standard_deviations}, {reclass_table;reclass_table...}, {reverse_values}, {output_field_name})

       Reclassifies values in a numerical or text field into classes based on
       bounds defined manually or using a reclassification method.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table or feature class containing the field that will be
         reclassified.
     field (Field):
         The field that will be reclassified. The field must be numeric or
         text.
     method {String}:
         Specifies the reclassification method that will be used for the field
         values in the field parameter value.

         * DEFINED_INTERVAL-Classes will be created with the same class range
         over the span of the values of the field to reclassify.

         * EQUAL_INTERVAL-Classes will be created with equal class ranges
         divided into a specified number of classes. This is the default.

         * GEOMETRIC_INTERVAL-Classes will be created with geometrically
         increasing or decreasing class ranges into a specified number of
         classes.

         * MANUAL-Class breaks and reclassed values will be manually
         specified.

         * NATURAL_BREAKS-Classes will be created of natural groupings in the
         data using the Jenks natural breaks algorithm.

         * QUANTILE-Classes will be created in which each class includes an
         equal number of values.

         * STANDARD_DEVIATION-Classes will be created by adding and
         subtracting a fraction of the standard deviation above and below the
         average value.

         * UNIQUE_VALUES-Classes will be created in which each unique value of
         the field becomes a class.
     classes {Long}:
         The target number of classes in the reclassified field. The maximum
         number of classes is 256.
     interval {Double}:
         The class interval size for the reclassified field. The provided value
         must result in at least 3 classes and not more than 1,000 classes.
     standard_deviations {String}:
         Specifies the number of standard deviations that will be used for the
         reclassified field. Class breaks and categories will be created with
         equal interval ranges that are a proportion of the standard deviation
         from the mean.

         * ONE-Intervals will be created using one standard deviation. This is
         the default.

         * HALF-Intervals will be created using half of one standard deviation.

         * THIRD-Intervals will be created using a third of one standard
         deviation.

         * QUARTER-Intervals will be created using a quarter of one standard
         deviation.
     reclass_table {Value Table}:
         The upper bound and reclassed value for the manual reclassification
         method.
     reverse_values {Boolean}:
         Specifies the order of the reclassified values.

         * DESC-Classes will be assigned values in descending order; the class
         with the highest values is assigned 1, the next highest class is
         assigned 2, and so on.

         * ASC-Classes will be assigned values in ascending order; the class
         with the lowest values is assigned 1, the next lowest class is
         assigned 2, and so on. This is the default.
     output_field_name {String}:
         The name or prefix of the output field. If the field to reclassify is
         a numerical field, two fields will be created, and this name will
         prefix the field names. If the field to reclassify is a text field,
         one new field will be created with this name."""
    ...

@gptooldoc("StandardizeField_management", None)
def StandardizeField(
    in_table=..., fields=..., method=..., min_value=..., max_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """StandardizeField_management(in_table, fields;fields..., {method}, {min_value}, {max_value})

       Standardizes values in fields by converting them to values that follow
       a specified scale. Standardization methods include z-score, minimum-
       maximum, absolute maximum, and robust standardization.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the field with the values to be standardized.
     fields (Value Table):
         The fields containing the values to be standardized. For each field,
         an output field name can be specified. If an output field name is not
         provided, the tool will create an output field name using the field
         name and selected method.
     method {String}:
         Specifies the method to use to standardize the values contained in the
         specified fields.

         * Z-SCORE-The standard score, which is the number of standard
         deviations above or below the mean, is used. The calculation is the
         Z-Score formula, which calculates the difference between the value and
         the mean of the values in the column, divided by the standard
         deviation of the values in the column. This is the default.

         * MIN-MAX-The values are converted to a scale between the user-
         specified minimum and maximum values.

         * MAXABS-Each value in the column is divided by the maximum absolute
         value in the column.

         * ROBUST-A robust variant of the Z-Score formula is used to
         standardize the values in the specified fields. This variant uses
         median and interquartile range in place of mean and standard
         deviation.
     min_value {Double}:
         The value used by the MIN-MAX method of the method parameter to
         specify the minimum value in the scale of the provided output values.
     max_value {Double}:
         The value used by the MIN-MAX method of the method parameter to
         specify the maximum value in the scale of the provided output values."""
    ...

@gptooldoc("TransformField_management", None)
def TransformField(
    in_table=..., fields=..., method=..., power=..., shift=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TransformField_management(in_table, fields;fields..., {method}, {power}, {shift})

       Transforms continuous values in one or more fields by applying
       mathematical functions to each value and changing the shape of the
       distribution. The transformation methods in the tool include log,
       square root, Box-Cox, multiplicative inverse, square, exponential, and
       inverse Box-Cox.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The input table or feature class containing the fields to be
         transformed. The newly transformed fields are added to the input
         table.
     fields (Value Table):
         The fields containing the values to be transformed. For each field, an
         output field name can be specified. If an output field name is not
         provided, the tool creates an output field name using the field name
         and transformation method.
     method {String}:
         Specifies the method that is used to transform the values contained in
         the specified fields.

         * INVX-The multiplicative inverse (1/x) method is applied to the
         original value (x) in the selected fields.

         * SQRT-The square root method is applied to the original value in the
         selected fields.

         * LOG-The natural logarithmic function, log(x), is applied to the
         original value (x) in the selected fields.

         * BOX-COX-The Box-Cox power function is applied to normally distribute
         the original values in the selected fields. This is the default.

         * INV_BOX-COX-The inverse of the Box-Cox transformation is applied to
         the original values in the selected fields.

         * INV_SQRT-The square method is applied to the original values in the
         selected fields. This transformation is the inverse of square root.

         * INV_LOG-The exponential function, exp(x), is applied to the original
         value (x) in the selected fields. This transformation is the inverse
         of log.
     power {Double}:
         The power parameter ( λ1) of the Box-Cox transformation. If no value
         is provided, an optimal value is determined using maximum likelihood
         estimation (MLE).
     shift {Double}:
         The value by which all the data is shifted (adding a constant value).
         No shift is applied if 0 is specified.For log, Box-Cox and square root
         transformations, a default shift
         value is added prior to the transformation if there are negative or
         zero values.For exponential (inverse log), inverse Box-Cox, and square
         (inverse
         square root) transformations, no shift is applied by default. If a
         shift value is provided, the value is subtracted after the
         transformation is applied. This allows you to use the same shift value
         for transformations and their associated inverses."""
    ...

@gptooldoc("TransposeFields_management", None)
def TransposeFields(
    in_table=...,
    in_field=...,
    out_table=...,
    in_transposed_field_name=...,
    in_value_field_name=...,
    attribute_fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TransposeFields_management(in_table, in_field;in_field..., out_table, in_transposed_field_name, in_value_field_name, {attribute_fields;attribute_fields...})

       Switch data stored in fields or columns to rows in a new table or
       feature class.

    INPUTS:
     in_table (Table View):
         The input feature class or table containing data value fields to be
         transposed.
     in_field (Value Table):
         The fields or columns containing data values in the input table that
         need to be transposed.Depending on your needs, you can select multiple
         fields to be
         transposed. The value here defines what the field name will be in the
         output. When not specified, the value is the same as the field name by
         default. However, you can also specify your own value. For example, if
         the field names to be transposed are Pop1991, Pop1992, and so on, by
         default, the values for these fields in the output will be the same
         (Pop1991, Pop1992, and so forth). However, you can choose to specify
         your own values such as 1991 and 1992.
     in_transposed_field_name (String):
         The name of the field that will be created to store field names of the
         transposed fields. Any valid field name can be used.
     in_value_field_name (String):
         The name of the field that will be created to store the corresponding
         values of the transposed fields. Any valid field name can be set, as
         long as it does not conflict with existing field names from the input
         table or feature class.
     attribute_fields {Field}:
         Additional attribute fields from the input table to be included in the
         output table. If you want to output a feature class, add the Shape
         field.

    OUTPUTS:
     out_table (Table):
         The output feature class or table. The output will contain a
         transposed field, a value field, and any number of specified attribute
         fields that need to be inherited from the input table.By default the
         out_table is a table. The output will be a feature
         class when the in_table is a feature class and the Shape field is
         selected in the attribute_fields parameter."""
    ...

@gptooldoc("Compact_management", None)
def Compact(
    in_workspace=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Compact_management(in_workspace)

       Compacts a file geodatabase. Compacting rearranges how the geodatabase
       is stored on disk, often reducing its size and improving performance.

    INPUTS:
     in_workspace (Workspace):
         The file geodatabase to be compacted."""
    ...

@gptooldoc("CompressFileGeodatabaseData_management", None)
def CompressFileGeodatabaseData(
    in_data=..., lossless=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CompressFileGeodatabaseData_management(in_data, lossless)

       Compresses all the contents in a geodatabase, all the contents in a
       feature dataset, or an individual stand-alone feature class or table.

    INPUTS:
     in_data (Workspace / Feature Dataset / Table View / Raster Layer / Geometric Network):
         The geodatabase, feature dataset, feature class, or table to compress.
     lossless (Boolean):
         Indicates whether lossless compression will be used.

         * Lossless compression-Lossless compression will be used. This is the
         default.

         * Non-lossless compression-Lossless compression will not be used.
         This parameter is ignored for pre-10.0 file geodatabases."""
    ...

@gptooldoc("GenerateFgdbLicense_management", None)
def GenerateFgdbLicense(
    in_lic_def_file=..., out_lic_file=..., allow_export=..., exp_date=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateFgdbLicense_management(in_lic_def_file, out_lic_file, {allow_export}, {exp_date})

       Generates a license file (.sdlic) for displaying the contents in a
       licensed file geodatabase created by the Generate Licensed File
       Geodatabase tool.

    INPUTS:
     in_lic_def_file (File):
         The license definition file (.licdef) created by the Generate Licensed
         File Geodatabase tool.
     allow_export {String}:
         Specifies whether the export of vector data is allowed.

         * DENY_EXPORT-Vector data cannot be exported with the data license
         file (.sdlic) installed. This is the default.

         * ALLOW_EXPORT-Vector data can be exported with the data license file
         (.sdlic) installed.
     exp_date {Date}:
         The expiration date of the data license file, after which the file
         geodatabase's contents can no longer be displayed. The default value
         is empty (blank), which means the data license file will never expire.

    OUTPUTS:
     out_lic_file (File):
         The license file (.sdlic) for distribution."""
    ...

@gptooldoc("GenerateLicensedFgdb_management", None)
def GenerateLicensedFgdb(
    in_fgdb=..., out_fgdb=..., out_lic_def=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateLicensedFgdb_management(in_fgdb, out_fgdb, out_lic_def)

       Generates a license definition file (.licdef) that defines and
       restricts the display of contents in a file geodatabase. The contents
       of the licensed file geodatabase can be viewed by creating a license
       file (*.sdlic) and installing it with ArcGIS Administrator. The
       license file is created using the Generate File Geodatabase License
       tool.

    INPUTS:
     in_fgdb (Workspace):
         The unlicensed file geodatabase to make licensed.

    OUTPUTS:
     out_fgdb (Workspace):
         The name of and location to create the licensed file geodatabase.
     out_lic_def (File):
         The license definition file."""
    ...

@gptooldoc("RecoverFileGDB_management", None)
def RecoverFileGDB(
    input_file_gdb=..., output_location=..., out_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RecoverFileGDB_management(input_file_gdb, output_location, out_name)

       Recovers data from a file geodatabase that has become corrupt.

    INPUTS:
     input_file_gdb (Workspace):
         Input corrupt file geodatabase.
     output_location (Folder):
         Output folder location for the recovered file geodatabase.
     out_name (String):
         Name for the output file geodatabase."""
    ...

@gptooldoc("UncompressFileGeodatabaseData_management", None)
def UncompressFileGeodatabaseData(
    in_data=..., config_keyword=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UncompressFileGeodatabaseData_management(in_data, {config_keyword})

       Uncompresses all the contents in a geodatabase, all the contents in a
       feature dataset, or an individual stand-alone feature class or table.

    INPUTS:
     in_data (Workspace / Feature Dataset / Table View / Raster Layer / Geometric Network):
         The geodatabase, feature dataset, feature class, or table to
         uncompress.
     config_keyword {String}:
         The configuration keyword defining how the data will store once
         uncompressed"""
    ...

@gptooldoc("Append_management", None)
def Append(
    inputs=...,
    target=...,
    schema_type=...,
    field_mapping=...,
    subtype=...,
    expression=...,
    match_fields=...,
    update_geometry=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Append_management(inputs;inputs..., target, {schema_type}, {field_mapping}, {subtype}, {expression}, {match_fields;match_fields...}, {update_geometry})

       Appends to, or optionally updates, an existing target dataset with
       multiple input datasets. Input datasets can be feature classes,
       tables, shapefiles, rasters, or annotation or dimension feature
       classes.

    INPUTS:
     inputs (Table View / Raster Layer):
         The input datasets containing the data to be appended to the target
         dataset. Input datasets can be point, line, or polygon feature
         classes, tables, rasters, annotation feature classes, or dimensions
         feature classes.Tables and feature classes can be combined. If a
         feature class is
         appended to a table, attributes will be transferred; however, the
         features will be dropped. If a table is appended to a feature class,
         the rows from the input table will have null geometry.
     target (Table View / Raster Layer):
         The existing dataset where the data of the input datasets will be
         appended.
     schema_type {String}:
         Specifies whether the fields of the input dataset must match the
         fields of the target dataset for data to be appended.

         * TEST-Fields from the input dataset must match the fields of the
         target dataset. An error will be returned if the fields do not match.

         * NO_TEST-Fields from the input dataset do not need to match the
         fields of the target dataset. Fields from the input datasets that do
         not match the fields of the target dataset will not be mapped to the
         target dataset unless the mapping is explicitly set in the Field Map
         parameter.

         * TEST_AND_SKIP-Fields from the input dataset must match the fields of
         the target dataset. If any of the input datasets contain fields that
         do not match the target dataset, that input dataset will be omitted
         with a warning message.
     field_mapping {Field Mappings}:
         Controls how the attribute fields from the input datasets will be
         transferred or mapped to the target dataset.This parameter can only be
         used if the schema_type parameter is
         NO_TEST.Because the input datasets are appended to an existing target
         dataset
         that has predefined fields, you cannot add, remove, or change the type
         of the fields in the field map. You can set merge rules for each
         output field. Merge rules allow you to specify how values from
         two or more
         input fields will be merged or combined into a single output value.
         The following merge rules can be used to determine how the output
         field will be populated with values:

         * First-Use the input fields' first value.

         * Last-Use the input fields' last value.

         * Join-Concatenate (join) the input field values.

         * Sum-Calculate the total of the input field values.

         * Mean-Calculate the mean (average) of the input field values.

         * Median-Calculate the median (middle) of the input field values.

         * Mode-Use the value with the highest frequency.

         * Min-Use the minimum value of all the input field values.

         * Max-Use the maximum value of all the input field values.

         * Standard deviation-Use the standard deviation classification method
         on all the input field values.

         * Count-Find the number of records included in the calculation.
         In Python, you can use the FieldMappings class to define this
         parameter.
     subtype {String}:
         The subtype description that will be assigned to all new data that is
         appended to the target dataset.
     expression {SQL Expression}:
         The SQL expression that will be used to select a subset of the input
         datasets' records. If multiple input datasets are specified, they will
         all be evaluated using the expression. If no records match the
         expression for an input dataset, no records from that dataset will be
         appended to the target.For more information about SQL syntax, see SQL
         reference for query
         expressions used in ArcGIS.
     match_fields {Value Table}:
         The fields from the input dataset that will be used to match to the
         target dataset. If the values of these fields match, records from the
         input dataset will update the corresponding records of the target
         dataset.
     update_geometry {Boolean}:
         Specifies whether geometry in the target dataset will be updated with
         geometry from the input dataset if the match_fields parameter field
         values match.

         * UPDATE_GEOMETRY-Geometry in the target dataset will be updated if
         the match_fields parameter field values match.

         * NOT_UPDATE_GEOMETRY-Geometry will not be updated. This is the
         default."""
    ...

@gptooldoc("Copy_management", None)
def Copy(
    in_data=..., out_data=..., data_type=..., associated_data=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Copy_management(in_data, out_data, {data_type}, {associated_data;associated_data...})

       Makes a copy of the input data.

    INPUTS:
     in_data (Data Element):
         The data to be copied.
     data_type {String}:
         The type of the data on disk to be copied.This parameter will be used
         in the event of a name conflict. A
         geodatabase may contain different datasets with the same name, for
         example, a feature class, feature dataset, mosaic dataset, and parcel
         fabric with the same name may exist in the same geodatabase.
         Specifying a value will direct the tool to use that data type in the
         event of two or more datasets with the same name.

         * FeatureClass-In the event of duplicate names, the feature class will
         be used.

         * FeatureDataset-In the event of duplicate names, the feature dataset
         will be used.

         * MosaicDataset-In the event of duplicate names, the mosaic dataset
         will be used.

         * ParcelFabric-In the event of duplicate names, the parcel fabric will
         be used.
     associated_data {Value Table}:
         When the input has associated data, this parameter can be used to
         control the associated output data's name and config keyword.

         * from_name-The data associated with the input data, which will also
         be copied.

         * data_type-The type of the data on disk to be copied. The only time
         you need to provide a value is when a geodatabase contains a feature
         dataset and a feature class with the same name. In this case, use the
         correct data type, FeatureDataset or FeatureClass, of the item you
         want to copy.

         * to_name-The name of the copied data in the out_data parameter value.

         * config_keyword-The geodatabase storage parameters (configuration).
         The from_name and to_name column names will be identical if the
         to_name value is not already used in out_data. If a name already
         exists in the out_data value, a unique to_name value will be created
         by appending an underscore plus a number (for example, rivers_1).

    OUTPUTS:
     out_data (Data Element):
         The location and name of the output data. The file name extension of
         the output data must match the extension of the input data. For
         example, if you are copying a file geodatabase, the output data
         element must have .gdb as a suffix."""
    ...

@gptooldoc("CreateDatabaseView_management", None)
def CreateDatabaseView(
    input_database=..., view_name=..., view_definition=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDatabaseView_management(input_database, view_name, view_definition)

       Creates a view in a database based on an SQL expression.

    INPUTS:
     input_database (Workspace):
         The database that contains the tables or feature classes used to
         construct the view. This database is also where the view will be
         created.
     view_name (String):
         The name of the view that will be created in the database.
     view_definition (String):
         An SQL statement used to construct the view."""
    ...

@gptooldoc("Delete_management", None)
def Delete(
    in_data=..., data_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Delete_management(in_data;in_data..., {data_type})

       Permanently deletes data. All types of geographic data supported by
       ArcGIS, as well as toolboxes and workspaces (folders and
       geodatabases), can be deleted. If the specified item is a workspace,
       all contained items are also deleted.

    INPUTS:
     in_data (Data Element / Layer / Table View / Graph / Utility Network):
         The input data to be deleted.
     data_type {String}:
         The type of data on disk to be deleted. This is only necessary when
         the input data is in a geodatabase and naming conflicts exist, for
         example, if the geodatabase contains a feature dataset and a feature
         class with the same name. In this case, the data type is used to
         clarify which dataset to delete."""
    ...

@gptooldoc("DeleteIdentical_management", None)
def DeleteIdentical(
    in_dataset=..., fields=..., xy_tolerance=..., z_tolerance=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteIdentical_management(in_dataset, fields;fields..., {xy_tolerance}, {z_tolerance})

       Deletes records in a feature class or table which have identical
       values in a list of fields. If the geometry field is selected, feature
       geometries are compared.

    INPUTS:
     in_dataset (Table View):
         The table or feature class that will have its identical records
         deleted.
     fields (Field):
         The field or fields whose values will be compared to find identical
         records.
     xy_tolerance {Linear Unit}:
         The xy tolerance that will be applied to each vertex when evaluating
         if there is an identical vertex in another feature.
     z_tolerance {Double}:
         The z tolerance that will be applied to each vertex when evaluating if
         there is an identical vertex in another feature."""
    ...

@gptooldoc("ExportReportToPDF_management", None)
def ExportReportToPDF(
    in_report=...,
    out_pdf_file=...,
    expression=...,
    resolution=...,
    image_quality=...,
    embed_font=...,
    compress_vector_graphics=...,
    image_compression=...,
    password_protect=...,
    pdf_password=...,
    page_range_type=...,
    custom_page_range=...,
    initial_page_number=...,
    final_page_number=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportReportToPDF_management(in_report, out_pdf_file, {expression}, {resolution}, {image_quality}, {embed_font}, {compress_vector_graphics}, {image_compression}, {password_protect}, {pdf_password}, {page_range_type}, {custom_page_range}, {initial_page_number}, {final_page_number})

       Exports an ArcGIS Pro report to a PDF file.

    INPUTS:
     in_report (Report / File):
         The input report or .rptx file.
     expression {SQL Expression}:
         An SQL expression used to select a subset of records. This expression
         is applied in addition to any existing expressions. For more
         information on SQL syntax, see SQL reference for query expressions
         used in ArcGIS.
     resolution {Long}:
         The resolution of the exported PDF in dots per inch (dpi).
     image_quality {String}:
         Specifies the output image quality of the PDF. The image quality
         option controls the quality of rasterized data going into the export.

         * BEST-The highest available image quality. This is the default.

         * BETTER-High image quality.

         * NORMAL-A compromise between image quality and speed.

         * FASTER-Lower image quality to generate the report faster.

         * FASTEST-The lowest image quality to create the report the fastest.
     embed_font {Boolean}:
         Specifies whether fonts are embedded in the output report. Font
         embedding allows text and markers built from font glyphs to be
         displayed correctly when the PDF is viewed on a computer that does not
         have the necessary fonts installed.

         * EMBED_FONTS-Fonts will be embedded in the output report. This is the
         default.

         * NO_EMBED_FONTS-Fonts will not be embedded in the output report.
     compress_vector_graphics {Boolean}:
         Specifies whether to compress the vector content streams in the PDF.

         * COMPRESS_GRAPHICS-Vector graphics will be compressed. This option
         should be set unless clear text is desired for troubleshooting. This
         is the default.

         * NO_COMPRESS_GRAPHICS-Vector graphics will not be compressed.
     image_compression {String}:
         Specifies the compression scheme used to compress image or raster data
         in the output PDF file.

         * NONE-Do not compress image or raster data.

         * RLE-Uses Run-length encoded compression.

         * DEFLATE-Uses Deflate, a lossless data compression.

         * LZW-Uses Lempel-Ziv-Welch, a lossless data compression.

         * JPEG-Uses JPEG, a lossy data compression.

         * ADAPTIVE-Uses Adaptive, which automatically selects the best
         compression type for each image on the page. JPEG will be used for
         large images with many unique colors. Deflate will be used for all
         other images. This is the default.
     password_protect {Boolean}:
         Specifies whether password protection is needed to view the output PDF
         report.

         * PASSWORD_PROTECT-The output PDF report document will require a
         password to open.

         * NO_PASSWORD_PROTECT-The output PDF report document can be opened
         without providing a password. This is the default.
     pdf_password {Encrypted String}:
         A password to restrict opening the PDF.
     page_range_type {String}:
         Specifies the page range of the report to export.

         * ALL-Export all pages. This is the default.

         * LAST-Export the last page only.

         * ODD-Export the odd numbered pages.

         * EVEN-Export the even numbered pages.

         * CUSTOM-Export a custom page range.
     custom_page_range {String}:
         The pages to be exported when the page_range_type parameter is set to
         CUSTOM. You can set individual pages, ranges, or a combination of both
         separated by commas, such as 1, 3-5, 10.
     initial_page_number {Long}:
         The initial page number of the report to create a page numbering
         offset to add additional pages to the beginning of the report.
     final_page_number {Long}:
         The page number to display on the last page of the exported PDF.

    OUTPUTS:
     out_pdf_file (File):
         The output PDF file."""
    ...

@gptooldoc("FindIdentical_management", None)
def FindIdentical(
    in_dataset=...,
    out_dataset=...,
    fields=...,
    xy_tolerance=...,
    z_tolerance=...,
    output_record_option=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FindIdentical_management(in_dataset, out_dataset, fields;fields..., {xy_tolerance}, {z_tolerance}, {output_record_option})

       Reports any records in a feature class or table that have identical
       values in a list of fields, and generates a table listing these
       identical records. If the field Shape is selected, feature geometries
       are compared.

    INPUTS:
     in_dataset (Table View):
         The table or feature class for which identical records will be found.
     fields (Field):
         The field or fields whose values will be compared to find identical
         records.
     xy_tolerance {Linear Unit}:
         The xy tolerance that will be applied to each vertex when evaluating
         if there is an identical vertex in another feature. This parameter is
         enabled only when Shape is selected as one of the fields.
     z_tolerance {Double}:
         The Z tolerance that will be applied to each vertex when evaluating if
         there is an identical vertex in another feature. This parameter is
         enabled only when Shape is selected as one of the fields.
     output_record_option {Boolean}:
         Choose if you want only duplicated records in the output table.

         * ALL-All input records will have corresponding records in the output
         table. This is the default.

         * ONLY_DUPLICATES-Only duplicate records will have corresponding
         records in the output table. The output will be empty if no duplicate
         is found.

    OUTPUTS:
     out_dataset (Table):
         The output table reporting identical records. The FEAT_SEQ field in
         the output table will have the same value for identical records."""
    ...

@gptooldoc("Merge_management", None)
def Merge(
    inputs=..., output=..., field_mappings=..., add_source=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Merge_management(inputs;inputs..., output, {field_mappings}, {add_source})

       Combines multiple input datasets into a single, new output dataset.
       This tool can combine point, line, or polygon feature classes or
       tables.

    INPUTS:
     inputs (Table View):
         The input datasets that will be merged into a new output dataset.
         Input datasets can be point, line, or polygon feature classes or
         tables. Input feature classes must all be of the same geometry
         type.Tables and feature classes can be combined in a single output
         dataset.
         The output type is determined by the first input. If the first input
         is a feature class, the output will be a feature class; if the first
         input is a table, the output will be a table. If a table is merged
         into a feature class, the rows from the input table will have null
         geometry.
     field_mappings {Field Mappings}:
         The attribute fields that will be in the output with the corresponding
         field properties and source fields. By default, all fields from the
         inputs will be included.Fields can be added, deleted, renamed, and
         reordered, and you can
         change their properties. Merge rules allow you to specify how
         values from two or more
         input fields will be merged or combined into a single output value.
         The following merge rules can be used to determine how the output
         field will be populated with values:

         * First-Use the input fields' first value.

         * Last-Use the input fields' last value.

         * Join-Concatenate (join) the input field values.

         * Sum-Calculate the total of the input field values.

         * Mean-Calculate the mean (average) of the input field values.

         * Median-Calculate the median (middle) of the input field values.

         * Mode-Use the value with the highest frequency.

         * Min-Use the minimum value of all the input field values.

         * Max-Use the maximum value of all the input field values.

         * Standard deviation-Use the standard deviation classification method
         on all the input field values.

         * Count-Find the number of records included in the calculation.
         In Python, you can use the FieldMappings class to define this
         parameter.
     add_source {Boolean}:
         Specifies whether source information will be added to the output
         dataset in a new text field, MERGE_SRC. The values in the MERGE_SRC
         field will indicate the input dataset path or layer name that is the
         source of each record in the output.

         * NO_SOURCE_INFO-Source information will not be added to the output
         dataset in a MERGE_SRC field. This is the default.

         * ADD_SOURCE_INFO-Source information will be added to the output
         dataset in a MERGE_SRC field.

    OUTPUTS:
     output (Feature Class / Table):
         The output dataset that will contain all combined input datasets."""
    ...

@gptooldoc("Rename_management", None)
def Rename(
    in_data=..., out_data=..., data_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Rename_management(in_data, out_data, {data_type})

       Changes the name of a dataset. This includes a variety of data types,
       including feature dataset, raster, table, and shapefile.

    INPUTS:
     in_data (Data Element):
         The input data to be renamed.
     data_type {String}:
         The type of data to be renamed.This parameter will be used in the
         event of a name conflict. A
         geodatabase may contain different datasets with the same name, for
         example, a feature class, feature dataset, mosaic dataset, and parcel
         fabric with the same name may exist in the same geodatabase.
         Specifying a value will direct the tool to use that data type in the
         event of two or more datasets with the same name.

         * FeatureClass-In the event of duplicate names, the feature class will
         be used.

         * FeatureDataset-In the event of duplicate names, the feature dataset
         will be used.

         * MosaicDataset-In the event of duplicate names, the mosaic dataset
         will be used.

         * ParcelFabric-In the event of duplicate names, the parcel fabric will
         be used.

    OUTPUTS:
     out_data (Data Element):
         The name of the output data."""
    ...

@gptooldoc("Sort_management", None)
def Sort(
    in_dataset=..., out_dataset=..., sort_field=..., spatial_sort_method=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Sort_management(in_dataset, out_dataset, sort_field;sort_field..., {spatial_sort_method})

       Reorders records in a feature class or table, in ascending or
       descending order, based on one or multiple fields. The reordered
       result is written to a new dataset.

    INPUTS:
     in_dataset (Table View):
         The input dataset whose records will be reordered based on the field
         values in the sort field or fields.
     sort_field (Value Table):
         Specifies the field or fields whose values will be used to reorder the
         input records and the direction the records will be sorted.Sorting by
         the Shape field or by multiple fields is only available
         with an ArcGIS Pro Advanced license. Sorting by a single attribute
         field (excluding Shape) is available at all license levels.

         * Ascending-Records will be sorted from low value to high value.

         * Descending-Records will be sorted from high value to low value.
     spatial_sort_method {String}:
         Specifies how features will be spatially sorted. The sort method is
         only enabled when the Shape field is selected as one of the sort
         fields.

         * UR-Sorting will start at the upper right corner. This is the
         default.

         * UL-Sorting will start at the upper left corner.

         * LR-Sorting will start at the lower right corner.

         * LL-Sorting will start at the lower left corner.

         * PEANO-A space filling curve algorithm, also known as a Peano curve,
         will be used to sort.

    OUTPUTS:
     out_dataset (Feature Class / Table):
         The output feature class or table."""
    ...

@gptooldoc("TransferFiles_management", None)
def TransferFiles(
    input_paths=..., output_folder=..., file_filter=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TransferFiles_management(input_paths;input_paths..., output_folder, {file_filter})

       Transfers files between a file system and a cloud storage workspace.

    INPUTS:
     input_paths (Raster Dataset / File / Folder):
         The list of input files or folders that will be copied to the output
         folder. The path can be a file system path or cloud storage path where
         the .acs file can be used.
     output_folder (Folder):
         The output folder path where the files will be copied.
     file_filter {String}:
         A file pattern filter that will limit the number of files that need to
         be copied, such as .tif, .crf, and similar image file types."""
    ...

@gptooldoc("Dissolve_management", None)
def Dissolve(
    in_features=...,
    out_feature_class=...,
    dissolve_field=...,
    statistics_fields=...,
    multi_part=...,
    unsplit_lines=...,
    concatenation_separator=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Dissolve_management(in_features, out_feature_class, {dissolve_field;dissolve_field...}, {statistics_fields;statistics_fields...}, {multi_part}, {unsplit_lines}, {concatenation_separator})

       Aggregates features based on specified attributes.

    INPUTS:
     in_features (Feature Layer):
         The features to be aggregated.
     dissolve_field {Field}:
         The field or fields on which features will be aggregated.
     statistics_fields {Value Table}:
         Specifies the numeric field or fields containing the attribute values
         that will be used to calculate the specified statistic. Multiple
         statistic and field combinations can be specified. Null values are
         excluded from all calculations.Text attribute fields can be summarized
         using first and last
         statistics. Numeric attribute fields can be summarized using any
         statistic.Available statistics types are as follows:

         * SUM-The values for the specified field will be added together.

         * MEAN-The average for the specified field will be calculated.

         * MIN-The smallest value for all records of the specified field will
         be found.

         * MAX-The largest value for all records of the specified field will be
         found.

         * RANGE-The range of values (maximum minus minimum) for the specified
         field will be calculated.

         * STD-The standard deviation of values in the specified field will be
         calculated.

         * COUNT-The number of values included in the calculations will be
         found. Each value will be counted except null values. To determine the
         number of null values in a field, create a count on the field in
         question, create a count on a different field that does not contain
         null values (for example, the OID if present), and subtract the two
         values.

         * FIRST-The specified field value of the first record in the input
         will be used.

         * LAST-The specified field value of the last record in the input will
         be used.

         * MEDIAN-The median for all records of the specified field will be
         calculated.

         * VARIANCE-The variance for all records of the specified field will be
         calculated.

         * UNIQUE-The number of unique values of the specified field will be
         counted.

         * CONCATENATE-The values for the specified field will be concatenated.
         The values can be separated using the concatenation_separator
         parameter.
     multi_part {Boolean}:
         Specifies whether multipart features will be allowed in the output
         feature class.

         * MULTI_PART-Multipart features will be allowed in the output feature
         class. This is the default.

         * SINGLE_PART-Multipart features will not be allowed in the output
         feature class. Individual features will be created for each part.
     unsplit_lines {Boolean}:
         Specifies how line features will be dissolved.

         * DISSOLVE_LINES-Lines will be dissolved into a single feature. This
         is the default.

         * UNSPLIT_LINES-Lines will only be dissolved when two lines have an
         end vertex in common.
     concatenation_separator {String}:
         A character or characters that will be used to concatenate values when
         the CONCATENATION option is used for the statistics_fields parameter.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class to be created that will contain the aggregated
         features."""
    ...

@gptooldoc("Eliminate_management", None)
def Eliminate(
    in_features=...,
    out_feature_class=...,
    selection=...,
    ex_where_clause=...,
    ex_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Eliminate_management(in_features, out_feature_class, {selection}, {ex_where_clause}, {ex_features})

       Eliminates polygons by merging them with neighboring polygons that
       have the largest area or the longest shared border. Eliminate is often
       used to remove small sliver polygons that are the result of overlay
       operations, such as those performed by Intersect and Union tools.

    INPUTS:
     in_features (Feature Layer):
         The layer with the polygons that will be merged with neighboring
         polygons.
     selection {Boolean}:
         Specifies whether the selected polygon will be merged with a polygon
         with the longest shared border or the largest area.

         * LENGTH-The selected polygon will be merged with the neighboring
         polygon with the longest shared border. This is the default.

         * AREA-The selected polygon will be merged with the neighboring
         polygon with the largest area.
     ex_where_clause {SQL Expression}:
         An SQL expression that will be used to identify features that will not
         be altered. For more information on SQL syntax, see the SQL reference
         for elements used in query expressions help topic.
     ex_features {Feature Layer}:
         An input polyline or polygon feature class or layer that defines
         polygon boundaries, or portions thereof, that will not be eliminated.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class to be created."""
    ...

@gptooldoc("EliminatePolygonPart_management", None)
def EliminatePolygonPart(
    in_features=...,
    out_feature_class=...,
    condition=...,
    part_area=...,
    part_area_percent=...,
    part_option=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EliminatePolygonPart_management(in_features, out_feature_class, {condition}, {part_area}, {part_area_percent}, {part_option})

       Creates a new output feature class containing the features from the
       input polygons with some parts or holes of a specified size deleted.

    INPUTS:
     in_features (Feature Layer):
         The input feature class or layer whose features will be copied to the
         output feature class, with some parts or holes eliminated.
     condition {String}:
         Specify how the parts to be eliminated will be determined.

         * AREA-Parts with an area less than that specified will be eliminated.

         * PERCENT-Parts with a percent of the total outer area less than that
         specified will be eliminated.

         * AREA_AND_PERCENT-Parts with an area and percent less than that
         specified will be eliminated. Only if a polygon part meets both the
         area and percent criteria will it be deleted.

         * AREA_OR_PERCENT-Parts with an area or percent less than that
         specified will be eliminated. If a polygon part meets either the area
         or percent criteria, it will be deleted.
     part_area {Areal Unit}:
         Eliminate parts smaller than this area.
     part_area_percent {Double}:
         Eliminate parts smaller than this percentage of a feature's total
         outer area.
     part_option {Boolean}:
         Determines what parts can be eliminated.

         * CONTAINED_ONLY-Only parts totally contained by other parts can be
         eliminated. This is the default.

         * ANY-Any parts can be eliminated.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output polygon feature class containing the remaining parts."""
    ...

@gptooldoc("AnalyzeDatasets_management", None)
def AnalyzeDatasets(
    input_database=...,
    include_system=...,
    in_datasets=...,
    analyze_base=...,
    analyze_delta=...,
    analyze_archive=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AnalyzeDatasets_management(input_database, include_system, {in_datasets;in_datasets...}, {analyze_base}, {analyze_delta}, {analyze_archive})

       Updates database statistics of base tables, delta tables, and archive
       tables, along with the statistics on the indexes of those tables. This
       tool is used in enterprise geodatabases to help get optimal
       performance from the relational database management system (RDBMS)
       query optimizer. Stale statistics can affect geodatabase performance.

    INPUTS:
     input_database (Workspace):
         The database that contains the data to be analyzed.
     include_system (Boolean):
         Specifies whether statistics will be gathered on the states and state
         lineages tables.You must be the geodatabase administrator to use this
         parameter.This
         parameter only applies to geodatabases. If the input workspace is
         a database, this parameter will be ignored.

         * NO_SYSTEM-Statistics will not be gathered on the states and state
         lineages tables. This is the default.

         * SYSTEM-Statistics will be gathered on the states and state lineages
         tables.
     in_datasets {String}:
         The names of the datasets that will be analyzed. An individual dataset
         or a Python list of datasets can be used. Dataset names use paths
         relative to the input workspace; full paths are not valid input.The
         connected user must be the owner of the datasets provided.
     analyze_base {Boolean}:
         Specifies whether the selected dataset base tables will be
         analyzed.This parameter only applies to geodatabases. If the input
         workspace is
         a database, this parameter will be ignored.

         * ANALYZE_BASE-Statistics will be gathered for the base tables for
         the selected datasets. This is the default.

         * NO_ANALYZE_BASE-Statistics will not be gathered for the base tables
         for the selected datasets.
     analyze_delta {Boolean}:
         Specifies whether the selected dataset delta tables will be
         analyzed.This parameter only applies to geodatabases that contain
         traditional
         versions. If the input workspace is a database or does not participate
         in traditional versioning, this parameter will be ignored.

         * ANALYZE_DELTA-Statistics will be gathered for the delta tables for
         the selected datasets. This is the default.

         * NO_ANALYZE_DELTA-Statistics will not be gathered for the delta
         tables for the selected datasets.
     analyze_archive {Boolean}:
         Specifies whether the selected dataset archive tables will be
         analyzed.This parameter only applies to geodatabases that contain
         archive-
         enabled datasets. If the input workspace is a database, this parameter
         will be ignored.

         * ANALYZE_ARCHIVE-Statistics will be gathered for the archive tables
         for the selected datasets. This is the default.

         * NO_ANALYZE_ARCHIVE-Statistics will not be gathered for the archive
         tables for the selected datasets."""
    ...

@gptooldoc("ChangePrivileges_management", None)
def ChangePrivileges(
    in_dataset=..., user=..., View=..., Edit=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ChangePrivileges_management(in_dataset;in_dataset..., user, {View}, {Edit})

       Establishes or changes user access privileges on the input enterprise
       database datasets, stand-alone feature classes, or tables.

    INPUTS:
     in_dataset (Layer / Table View / Dataset / Address Locator):
         The datasets, feature classes, or tables whose access privileges will
         be changed.
     user (String):
         The database username whose privileges will be modified.
     View {String}:
         Specifies the user's view privileges.

         * AS_IS-No changes will be made to the user's existing view
         privileges. If the user has view privileges, they will continue to
         have view privileges. If the user doesn't have view privileges, they
         will continue to not have view privileges.

         * GRANT-The user will be allowed to view the datasets.

         * REVOKE-The user's view privileges will be removed.
     Edit {String}:
         Specifies the user's edit privileges.

         * AS_IS-No changes will be made to the user's existing edit
         privileges. If the user has edit privileges, they will continue to
         have edit privileges. If the user doesn't have edit privileges, they
         will continue to not have edit privileges.

         * GRANT-The user will be allowed to edit the input datasets.

         * REVOKE-The user's edit privileges will removed. The user can still
         view the input dataset."""
    ...

@gptooldoc("Compress_management", None)
def Compress(
    in_workspace=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Compress_management(in_workspace)

       Compresses an enterprise geodatabase by removing states not referenced
       by a version and redundant rows.

    INPUTS:
     in_workspace (Workspace):
         The database connection file that connects to the enterprise
         geodatabase to be compressed. Connect as the geodatabase
         administrator."""
    ...

@gptooldoc("ConfigureGeodatabaseLogFileTables_management", None)
def ConfigureGeodatabaseLogFileTables(
    input_database=..., log_file_type=..., log_file_pool_size=..., use_tempdb=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConfigureGeodatabaseLogFileTables_management(input_database, log_file_type, {log_file_pool_size}, {use_tempdb})

       Alters the type of log file tables used by an earlier release
       enterprise geodatabase to maintain lists of records cached by ArcGIS.

    INPUTS:
     input_database (Workspace):
         A database connection (.sde file) to the enterprise geodatabase where
         the log file table configuration will be changed. The connection must
         be made as the geodatabase administrator.
     log_file_type (String):
         Specifies the type of log file tables the geodatabase will use.

         * SESSION_LOG_FILE-Session-based log file tables for selection sets
         will be used. Session-based log file tables are dedicated to a single
         session and may contain multiple selection sets.

         * SHARED_LOG_FILE-Shared log file tables for selection sets will be
         used. Shared log file tables are shared by all sessions that connect
         as the same user.
     log_file_pool_size {Long}:
         The number of tables included in the pool that the geodatabase will
         use if a pool of session-based log file tables owned by the
         geodatabase administrator is used.
     use_tempdb {Boolean}:
         This parameter is no longer applicable in any supported ArcGIS
         release."""
    ...

@gptooldoc("CreateDatabaseSequence_management", None)
def CreateDatabaseSequence(
    in_workspace=..., seq_name=..., seq_start_id=..., seq_inc_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDatabaseSequence_management(in_workspace, seq_name, {seq_start_id}, {seq_inc_value})

       Creates a database sequence in a geodatabase. You can use the
       sequences in custom applications that access the geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The database connection file (.sde) to connect to the enterprise
         geodatabase in which you want to create a sequence or the path to the
         file geodatabase (including the file geodatabase name). For
         database connections, the user specified in the database
         connection will be the owner of the sequence and, therefore, must have
         the following permissions in the database:

         * Db2-CREATEIN privilege on their schema

         * Oracle-CREATE SEQUENCE system privilege

         * PostgreSQL-Authority on their schema

         * SAP HANA-Must be a standard user

         * SQL Server-CREATE SEQUENCE privilege and ALTER OR CONTROL permission
         on their schema
     seq_name (String):
         The name you want to assign to the database sequence. For enterprise
         geodatabases, the name must meet sequence name requirements for the
         database platform you're using and must be unique in the database. For
         file geodatabases, the name must be unique to the file geodatabase. It
         is important that you remember this name, as it is the name you use in
         your custom applications and expressions to invoke the sequence.
     seq_start_id {Long}:
         The starting number of the sequence. If you do not provide a starting
         number, the sequence starts with 1. If you do provide a starting
         number, it must be greater than 0.
     seq_inc_value {Long}:
         Describes how the sequence numbers will increment. For example, if the
         sequence starts at 10 and the increment value is 5, the next value in
         the sequence is 15, and the next value after that is 20. If you do not
         specify an increment value, sequence values will increment by 1."""
    ...

@gptooldoc("CreateDatabaseUser_management", None)
def CreateDatabaseUser(
    input_database=...,
    user_authentication_type=...,
    user_name=...,
    user_password=...,
    role=...,
    tablespace_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDatabaseUser_management(input_database, {user_authentication_type}, user_name, {user_password}, {role}, {tablespace_name})

       Creates a database user with privileges sufficient to create data in
       the database.

    INPUTS:
     input_database (Workspace):
         The connection file to an enterprise geodatabase in Oracle,
         PostgreSQL, or SQL Server. Ensure that the connection is made as a
         database administrator user. When connecting to Oracle, you must
         connect as the sys user.
     user_authentication_type {Boolean}:
         Specifies the authentication type for the user. If you specify
         OPERATING_SYSTEM_USER, an operating system login must already exist
         for the user you will create. Operating system users are only
         supported for SQL Server and Oracle databases.

         * DATABASE_USER-A database-authenticated user will be created. This is
         the default. If your database management system is not configured to
         allow database authentication, do not use this option.

         * OPERATING_SYSTEM_USER-An operating system-authenticated user will be
         created. The corresponding login must already exist. If your database
         management system is not configured to allow operating system
         authentication, do not use this option.
     user_name (String):
         The name of the new database user.If you chose to create a database
         user for an operating system login,
         the user name must match the login name.
     user_password {Encrypted String}:
         The password for the new user. The password policy of the underlying
         database is enforced.If you chose to create a database user for an
         operating system login,
         no input is required.
     role {String}:
         The name of the existing database role to which the new user will be
         added.
     tablespace_name {String}:
         The name of the tablespace that will be used as the default tablespace
         for the new user in an Oracle database. You can specify a
         preconfigured tablespace, or, if the tablespace does not exist, it
         will be created in the Oracle default storage location with its size
         set to 400 MB. If no tablespace is specified, the user's default
         tablespace will be set to the Oracle default tablespace."""
    ...

@gptooldoc("CreateEnterpriseGeodatabase_management", None)
def CreateEnterpriseGeodatabase(
    database_platform=...,
    instance_name=...,
    database_name=...,
    account_authentication=...,
    database_admin=...,
    database_admin_password=...,
    sde_schema=...,
    gdb_admin_name=...,
    gdb_admin_password=...,
    tablespace_name=...,
    authorization_file=...,
    spatial_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateEnterpriseGeodatabase_management(database_platform, instance_name, {database_name}, {account_authentication}, {database_admin}, {database_admin_password}, {sde_schema}, {gdb_admin_name}, {gdb_admin_password}, {tablespace_name}, authorization_file, {spatial_type})

       Creates a database, storage locations, and a database user to act as
       the geodatabase administrator and owner of the geodatabase.
       Functionality varies depending on the database management system used.
       The tool grants the geodatabase administrator the privileges required
       to create a geodatabase; it then creates a geodatabase in the
       database.

    INPUTS:
     database_platform (String):
         Specifies the type of database management system to which a connection
         will be made to create a geodatabase.

         * Oracle-Connection to an Oracle instance will be made.

         * PostgreSQL-Connection to a PostgreSQL database cluster will be made.

         * SQL_Server-Connection to a Microsoft SQL Server instance will be
         made.
     instance_name (String):
         The name of the instance.For SQL Server, provide the SQL Server
         instance name. Case-sensitive
         or binary collation SQL Server instances are not supported.For Oracle,
         provide either the TNS name or the Oracle Easy Connection
         string.For PostgreSQL, provide the name of the server where PostgreSQL
         is
         installed.
     database_name {String}:
         The name of the database.This parameter is valid for PostgreSQL and
         SQL Server. You can provide
         either the name of an existing, preconfigured database or a name for a
         database that the tool will create.If the tool creates the database in
         SQL Server, the file sizes will
         either be the same as defined for the SQL Server model database or 500
         MB for the .mdf file and 125 MB for the .ldf file, whichever is
         greater. Both the .mdf and .ldf files are created in the default SQL
         Server location on the database server. Do not name the database
         sde.If the tool creates the database in PostgreSQL, it uses the
         template1
         database as the template for the database. If you need a different
         template-for example, you require a template that is enabled for
         PostGIS-you must create the database before running this tool and
         provide the name of the existing database. Always use lowercase
         characters for the database name. If you use uppercase letters, the
         tool will convert them to lowercase.
     account_authentication {Boolean}:
         Specifies the type of authentication that will be used for the
         database connection.

         * OPERATING_SYSTEM_AUTH-Operating system authentication will be used.
         The login information that you provide for the computer where you run
         the tool is the login that will be used to authenticate the database
         connection. If the database management system is not configured to
         allow operating system authentication, authentication will fail.

         * DATABASE_AUTH-Database authentication will be used. You must provide
         a valid database username and password for authentication in the
         database. This is the default. If the database management system is
         not configured to allow database authentication, authentication will
         fail.
     database_admin {String}:
         If you use database authentication, specify a database administrator
         user. For Oracle, use the sys user. For PostgreSQL, specify a user
         with superuser status. For SQL Server, specify any member of the
         sysadmin fixed server role.
     database_admin_password {Encrypted String}:
         If you use database authentication, provide the password for the
         database administrator.
     sde_schema {Boolean}:
         This parameter is only relevant to SQL Server and specifies whether
         the geodatabase will be created in the schema of a user named sde or
         in the dbo schema in the database. If creating a dbo-schema
         geodatabase, connect as a user who is dbo in the SQL Server instance.

         * SDE_SCHEMA-The geodatabase repository is owned by and will be stored
         in the schema of a user named sde. This is the default.

         * DBO_SCHEMA-The geodatabase repository will be stored in the dbo
         schema in the database.
     gdb_admin_name {String}:
         The name of the geodatabase administrator user.If you are using
         PostgreSQL, this value must be sde. If the sde login
         role does not exist, this tool will create it and grant it superuser
         status in the database cluster. If the sde login role exists, this
         tool will grant it superuser status if it does not already have it.
         The tool also creates an sde schema in the database and grants usage
         on the schema to public.If you are using Oracle, the value is sde. If
         the sde user does not
         exist in the Oracle database, the tool will create it and grant it the
         privileges required to create and upgrade a geodatabase and disconnect
         users from the database. If you run this tool in an Oracle 12c or
         later release database, the tool also grants privileges to allow data
         imports using Oracle Data Pump. If the sde user exists, the tool will
         grant these same privileges to the existing user.Creating or upgrading
         user-schema geodatabases in Oracle is no longer
         supported.If you are using SQL Server and specified an sde-schema
         geodatabase,
         this value must be sde. The tool will create an sde login, database
         user, and schema and grant it privileges to create a geodatabase and
         remove connections from the SQL Server instance. If you specified a
         dbo schema, do not provide a value for this parameter.
     gdb_admin_password {Encrypted String}:
         The password for the geodatabase administrator user. If the
         geodatabase administrator user exists in the database management
         system, the password you provide must match the existing password. If
         the geodatabase administrator user does not exist, provide a valid
         database password for the new user. The password must meet the
         password policy enforced by the database.The password is an encrypted
         string.
     tablespace_name {String}:
         The name of the tablespace. This parameter is only valid for
         Oracle and PostgreSQL
         database management system types. For Oracle, do one of the following:

         * Provide the name of an existing tablespace. This tablespace will be
         used as the default tablespace for the geodatabase administrator user.

         * Provide a valid name for a new tablespace. The tool will create a
         400 MB tablespace in the Oracle default storage location and set it as
         the geodatabase administrator's default tablespace.

         * Leave the tablespace blank. The tool will create a 400 MB tablespace
         named SDE_TBS in the Oracle default storage location. The SDE_TBS
         tablespace will be set as the geodatabase administrator's default
         tablespace.
         This tool does not create a tablespace in PostgreSQL. You must either
         provide the name of an existing tablespace to be used as the
         database's default tablespace or leave this parameter blank. If you
         leave the parameter blank, the tool will create a database in the
         pg_default tablespace.
     authorization_file (File):
         The keycodes file that was created when ArcGIS Server was authorized.
         If you have not done so, authorize ArcGIS Server to create this file.
         This file is in the <drive>\\Program
         Files\\ESRI\\License<release#>\\sysgen folder on Windows or the
         /arcgis/server/framework/runtime/.wine/drive_c/Program
         Files/ESRI/License<release#>/sysgen directory on Linux. The
         /.wine directory is a hidden directory.You may need to copy the
         keycodes file from the ArcGIS Server machine
         to a location that is accessible to the tool.
     spatial_type {String}:
         Specifies the spatial type that will be used. This is only applicable
         to PostgreSQL databases.

         * ST_GEOMETRY-The ST_Geometry spatial type will be used. This is the
         default.

         * POSTGIS-The PostGIS spatial type will be used."""
    ...

@gptooldoc("CreateRasterType_management", None)
def CreateRasterType(
    input_database=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRasterType_management(input_database)

    INPUTS:
     input_database (Workspace):
         Specify the database connection (.sde) file for the geodatabase in
         which you want to install the ST_Raster data type. You must connect as
         the geodatabase administrator."""
    ...

@gptooldoc("CreateRole_management", None)
def CreateRole(
    input_database=..., role=..., grant_revoke=..., user_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRole_management(input_database, role, {grant_revoke}, {user_name})

       Creates a database role, allowing you to add users to or remove them
       from the role.

    INPUTS:
     input_database (Workspace):
         The connection file to a database or enterprise geodatabase. Connect
         as a database administrator user.
     role (String):
         The name of the database role to create. If it's an existing role,
         type the name for the role you want to add users to or remove them
         from.
     grant_revoke {String}:
         Specifies whether the role will be added to a user or list of users or
         a user or list of users will be removed from the role.

         * GRANT-The role will be granted to the specified user or users,
         making them a member of the role. This is the default.

         * REVOKE-The role will be revoked from the specified user or users,
         removing them from the role.
     user_name {String}:
         The name of the user whose role membership will change. To specify
         multiple users, type the user names separated by commas (no spaces)."""
    ...

@gptooldoc("DeleteDatabaseSequence_management", None)
def DeleteDatabaseSequence(
    in_workspace=..., seq_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteDatabaseSequence_management(in_workspace, seq_name)

       Deletes a database sequence from a geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The full path to the location of the file geodatabase from
         which you want to delete a sequence or the database connection file
         (.sde) to connect to the enterprise geodatabase from which you want to
         delete a sequence. The user specified in the database connection must
         have the following permissions in the database:

         * Db2-DBADM authority

         * Oracle-Must be the sequence owner or have the DROP ANY SEQUENCE
         system privilege

         * PostgreSQL-Must be the sequence owner

         * SAP HANA-Must be a standard user

         * SQL Server-ALTER OR CONTROL permission on the database schema where
         the sequence is stored
     seq_name (String):
         The name of the database sequence you want to delete. Once deleted,
         the sequence cannot be used to generate sequence IDs when called from
         existing custom applications or expressions."""
    ...

@gptooldoc("DeleteSchemaGeodatabase_management", None)
def DeleteSchemaGeodatabase(
    input_database=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteSchemaGeodatabase_management(input_database)

       Deletes a geodatabase from a user's schema in Oracle.

    INPUTS:
     input_database (Workspace):
         The database connection file (.sde) of the user-schema geodatabase to
         be deleted. You must connect as the schema owner."""
    ...

@gptooldoc("DiagnoseVersionMetadata_management", None)
def DiagnoseVersionMetadata(
    input_database=..., out_log=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DiagnoseVersionMetadata_management(input_database, out_log)

       Identifies inconsistencies in the system tables used to manage
       traditional versions and states in a geodatabase.

    INPUTS:
     input_database (Workspace):
         The database connection (.sde file) to the enterprise geodatabase in
         which traditional versioning system table inconsistencies may
         exist.The connection must be made as the geodatabase administrator.

    OUTPUTS:
     out_log (File):
         The name and location of the output log file.The log file is an ASCII
         file containing a list of the system tables
         in the specified version that contain inconsistent records, as well as
         the database connection file used."""
    ...

@gptooldoc("DiagnoseVersionTables_management", None)
def DiagnoseVersionTables(
    input_database=..., out_log=..., target_version=..., input_tables=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DiagnoseVersionTables_management(input_database, out_log, {target_version}, {input_tables;input_tables...})

       Identifies inconsistencies in the delta (A and D) tables of datasets
       that are registered for traditional versioning.

    INPUTS:
     input_database (Workspace):
         The database connection (.sde file) to the enterprise geodatabase in
         which delta table inconsistencies may exist. The connection must be
         made as the geodatabase administrator.
     target_version {String}:
         The geodatabase version with the delta tables that will be checked for
         inconsistencies. If no version is specified, all versions are
         processed.
     input_tables {String}:
         A single table or a text file containing a list of versioned tables
         with the associated delta tables to be checked for inconsistencies.
         Use fully-qualified table names in the text file, and place one table
         name per line. If no file is specified, all tables in the geodatabase
         are processed.

    OUTPUTS:
     out_log (File):
         The path and name of the output log file. The log file is an ASCII
         file containing a list of the tables in the specified version that
         contain inconsistent records, as well as information about the
         connection file, geodatabase version, and tables for which the tool
         was run."""
    ...

@gptooldoc("EnableEnterpriseGeodatabase_management", None)
def EnableEnterpriseGeodatabase(
    input_database=..., authorization_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EnableEnterpriseGeodatabase_management(input_database, authorization_file)

       Creates geodatabase system tables, stored procedures, functions, and
       types in an existing database, which enable geodatabase functionality
       in the database.

    INPUTS:
     input_database (Workspace):
         The path and database connection file (.sde) name for the database in
         which geodatabase functionality will be enabled. The database
         connection must connect as a user that qualifies as a geodatabase
         administrator.
     authorization_file (File):
         The keycodes file that was created when ArcGIS Server was authorized.
         If you have not done so, authorize ArcGIS Server to create this file.
         This file is in the <drive>\\Program
         Files\\ESRI\\License<release#>\\sysgen folder on Windows or the
         /arcgis/server/framework/runtime/.wine/drive_c/Program
         Files/ESRI/License<release#>/sysgen directory on Linux. The
         /.wine directory is a hidden directory.You may need to copy the
         keycodes file from the ArcGIS Server machine
         to a location that is accessible to the tool."""
    ...

@gptooldoc("ExportGeodatabaseConfigurationKeywords_management", None)
def ExportGeodatabaseConfigurationKeywords(
    input_database=..., out_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportGeodatabaseConfigurationKeywords_management(input_database, out_file)

       Exports the configuration keywords, parameters, and values from the
       specified enterprise geodatabase to an editable file. Change parameter
       values or add custom configuration keywords to the file and use the
       Import Geodatabase Configuration Keywords tool to import the changes
       to the geodatabase.

    INPUTS:
     input_database (Workspace):
         The connection file for the enterprise geodatabase from which you want
         to export configuration keywords, parameters, and values. You must
         connect as the geodatabase administrator.

    OUTPUTS:
     out_file (File):
         The full path to and name of the ASCII text file to be created. The
         file will contain all the configuration keywords, parameters, and
         values from the enterprise geodatabase's DBTUNE (or SDE_DBTUNE) system
         table."""
    ...

@gptooldoc("ImportGeodatabaseConfigurationKeywords_management", None)
def ImportGeodatabaseConfigurationKeywords(
    input_database=..., in_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportGeodatabaseConfigurationKeywords_management(input_database, in_file)

       Defines data storage parameters for an enterprise geodatabase by
       importing a file containing configuration keywords, parameters, and
       values.

    INPUTS:
     input_database (Workspace):
         The connection file for the enterprise geodatabase to which you want
         to import the configuration file. You must connect as the geodatabase
         administrator.
     in_file (File):
         The path to and name of the ASCII text file containing configuration
         keywords, parameters, and values to import."""
    ...

@gptooldoc("MigrateStorage_management", None)
def MigrateStorage(
    in_datasets=..., config_keyword=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MigrateStorage_management(in_datasets;in_datasets..., config_keyword)

       Moves the data from a binary, spatial, or spatial attribute column of
       one data type to a new column of a different data type in geodatabases
       in Oracle and SQL Server. The configuration keyword you specify when
       migrating determines the data type used for the new column.

    INPUTS:
     in_datasets (Table View / Raster Layer / Feature Dataset):
         Datasets to be migrated. The connection you use to access the datasets
         must be connecting as the dataset owner.
     config_keyword (String):
         Configuration keyword containing the appropriate parameter values for
         the migration. Parameter values are set by the geodatabase
         administrator. Contact your geodatabase administrator if you are
         unsure which configuration keyword to use."""
    ...

@gptooldoc("RebuildIndexes_management", None)
def RebuildIndexes(
    input_database=..., include_system=..., in_datasets=..., delta_only=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RebuildIndexes_management(input_database, include_system, {in_datasets;in_datasets...}, {delta_only})

       Rebuild existing attribute or spatial indexes in enterprise
       geodatabases. Indexes can also be rebuilt on states and state_lineage
       geodatabase system tables and the delta tables of datasets that are
       registered to participate in traditional versioning. Out-of-date
       indexes can lead to poor query performance.

    INPUTS:
     input_database (Workspace):
         The connection (.sde file) to the database or geodatabase that
         contains the data for which you want to rebuild indexes.
     include_system (Boolean):
         Indicates whether indexes will be rebuilt on the states and state
         lineages tables.You must connect as the geodatabase administrator in
         the connection
         file you specified for the input_database for this option to be
         executed successfully.This option only applies to geodatabases. This
         option is ignored if
         you connect to a database.

         * NO_SYSTEM-Indexes will not be rebuilt on the states and state
         lineages table. This is the default.

         * SYSTEM-Indexes will be rebuilt on the states and state lineages
         tables.
     in_datasets {String}:
         Names of the datasets that will have their indexes rebuilt. Dataset
         names use paths relative to the input_database; full paths are not
         accepted as input.
     delta_only {Boolean}:
         Indicates how the indexes will be rebuilt on the selected datasets.
         This option has no effect if in_datasets is empty.This option only
         applies to geodatabases. If the input workspace is a
         database, this option will be ignored.

         * ALL-Indexes will be rebuilt on all indexes for the selected
         datasets. This includes spatial indexes as well as user-created
         attribute indexes and any geodatabase-maintained indexes for the
         dataset.

         * ONLY_DELTAS-Indexes will only be rebuilt for the delta tables of the
         selected datasets. This option can be used for cases where the
         business tables for the selected datasets are not updated often and
         there is a high volume of edits in the delta tables. This is the
         default."""
    ...

@gptooldoc("RegisterWithGeodatabase_management", None)
def RegisterWithGeodatabase(
    in_dataset=...,
    in_object_id_field=...,
    in_shape_field=...,
    in_geometry_type=...,
    in_spatial_reference=...,
    in_extent=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RegisterWithGeodatabase_management(in_dataset, {in_object_id_field}, {in_shape_field}, {in_geometry_type}, {in_spatial_reference}, {in_extent})

       Registers with the geodatabase the feature classes, tables, views, and
       raster layers that were created in the database using third-party
       tools or views created with the Create Database View tool. Once
       registered, information about the items-such as table and column
       names, spatial extent, and geometry type-is stored in the
       geodatabase's system tables, allowing these registered items to
       participate in geodatabase functionality.

    INPUTS:
     in_dataset (Table View / Raster Layer):
         The feature class, table, view, or raster created using third-party
         tools or SQL, or the view created using the Create Database View tool
         that will be registered with the geodatabase. The dataset must exist
         in the same database as the geodatabase.
     in_object_id_field {Field}:
         The field that will be used as the ObjectID field. This input is
         required when registering a view, and you must supply an existing
         integer field. This parameter is optional when registering other
         dataset types; if you use an existing field, it must be an integer
         data type. If an existing field is not supplied when registering these
         other dataset types, an ObjectID field will be created and populated.
     in_shape_field {Field}:
         The field that identifies the shape of the features. If the input
         dataset contains a spatial data type column, include this field during
         the registration process.
     in_geometry_type {String}:
         Specifies the geometry type. If the in_shape_field parameter value is
         present, you must specify a geometry type. If the dataset being
         registered contains existing features, the geometry type specified
         must match the entity type of these features.

         * POINT-The geometry type will be point.

         * MULTIPOINT-The geometry type will be multipoint.

         * POLYGON-The geometry type will be polygon.

         * POLYLINE-The geometry type will be polyline.

         * MULTIPATCH-The geometry type will be multipatch.
     in_spatial_reference {Spatial Reference}:
         If the in_shape_field parameter value is present and the table is
         empty, specify the coordinate system to be used for features. If the
         dataset being registered contains existing features, the coordinate
         system specified must match the coordinate system of the existing
         features. Valid values are a Spatial Reference object, a file with a
         .prj extension, or a string representation of a coordinate system.
     in_extent {Envelope}:
         If the in_shape_field parameter value is present, specify the
         allowable coordinate range for x,y coordinates in the following order:
         "XMin YMin XMax YMax". If the dataset being registered contains
         existing features, the extent of the existing features will be used."""
    ...

@gptooldoc("RepairVersionMetadata_management", None)
def RepairVersionMetadata(
    input_database=..., out_log=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RepairVersionMetadata_management(input_database, out_log)

       Repairs inconsistencies in the versioning system tables of a
       geodatabase that contains traditional versions.

    INPUTS:
     input_database (Workspace):
         The database connection (.sde file) to the enterprise geodatabase in
         which versioning system table inconsistencies exist. The connection
         must be made as the geodatabase administrator.

    OUTPUTS:
     out_log (File):
         The output log file. The log file is an ASCII file containing the
         results of the repair operation."""
    ...

@gptooldoc("RepairVersionTables_management", None)
def RepairVersionTables(
    input_database=..., out_log=..., target_version=..., input_tables=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RepairVersionTables_management(input_database, out_log, {target_version}, {input_tables;input_tables...})

       Repairs inconsistencies in the delta (A and D) tables of datasets that
       are registered for traditional versioning.

    INPUTS:
     input_database (Workspace):
         The database connection (.sde file) to the enterprise geodatabase in
         which delta table inconsistencies exist. The connection must be made
         as the geodatabase administrator.
     target_version {String}:
         The geodatabase version to be repaired. If no version is specified,
         all versions are processed.
     input_tables {String}:
         A single table or a text file containing a list of versioned tables
         with the associated delta tables to be repaired. Use fully-qualified
         table names in the text file, and place one table name per line. If no
         table or file is specified, all tables are processed.

    OUTPUTS:
     out_log (File):
         The location where the log file will be written and the name of the
         log file. The log file is an ASCII file containing the results of the
         repair operation."""
    ...

@gptooldoc("UpdateEnterpriseGeodatabaseLicense_management", None)
def UpdateEnterpriseGeodatabaseLicense(
    input_database=..., authorization_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpdateEnterpriseGeodatabaseLicense_management(input_database, authorization_file)

       Updates the ArcGIS Server license in an enterprise geodatabase.

    INPUTS:
     input_database (Workspace):
         A database connection (.sde file) to the enterprise geodatabase to
         authorize with a new ArcGIS Server enterprise authorization file.The
         database connection file must connect to the database as the
         geodatabase administrator.
     authorization_file (File):
         The path and file name of the keycodes file generated when ArcGIS
         Server enterprise was authorized. If necessary, copy the file from the
         ArcGIS Server machine to a directory that the tool can access.ArcGIS
         Server creates the keycodes file in the following location:
         \\\\Program Files\\ESRI\\License<release#>\\sysgen (Microsoft Windows
         servers) or /arcgis/server/framework/runtime/.wine/drive_c/Program
         Files/ESRI/License<release#>/sysgen (Linux servers)."""
    ...

@gptooldoc("UpdatePortalDatasetOwner_management", None)
def UpdatePortalDatasetOwner(
    in_dataset=..., target_owner=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpdatePortalDatasetOwner_management(in_dataset, target_owner)

       Updates the portal owner of a dataset to another user.

    INPUTS:
     in_dataset (Utility Network / Utility Network Layer / Trace Network / Trace Network Layer):
         The input dataset for which the portal owner will be updated.
     target_owner (String):
         The name of the portal user who will be the new portal owner of the
         dataset."""
    ...

@gptooldoc("UpgradeDataset_management", None)
def UpgradeDataset(
    in_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpgradeDataset_management(in_dataset)

       Upgrades the schema of a mosaic dataset, network dataset, annotation
       dataset, dimension dataset, parcel fabric, trace network, or utility
       network to the current ArcGIS release. Upgrading the dataset allows
       the dataset to use new functionality in the current software release.

    INPUTS:
     in_dataset (Parcel Fabric Layer for ArcMap / Parcel Layer / Mosaic Layer / Network Dataset Layer / Utility Network Layer / Trace Network Layer / Annotation Layer / Dimension Layer / 3D Object Feature Layer):
         The dataset that will be upgraded to the current ArcGIS client
         release."""
    ...

@gptooldoc("UpgradeGDB_management", None)
def UpgradeGDB(
    input_workspace=..., input_prerequisite_check=..., input_upgradegdb_check=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpgradeGDB_management(input_workspace, input_prerequisite_check, input_upgradegdb_check)

       Upgrades a geodatabase to the latest ArcGIS release to take advantage
       of new functionality.

    INPUTS:
     input_workspace (Workspace):
         The geodatabase to upgrade. When upgrading an enterprise geodatabase,
         specify a database connection file (.sde) that connects to the
         geodatabase as the geodatabase administrator.
     input_prerequisite_check (Boolean):
         Specifies whether the prerequisite check will be run before upgrading
         the geodatabase.

         * NO_PREREQUISITE_CHECK-The prerequisite check will not be run.

         * PREREQUISITE_CHECK-The prerequisite check will be run before
         upgrading the geodatabase. This is the default.
     input_upgradegdb_check (Boolean):
         Specifies whether the input geodatabase will be upgraded.

         * NO_UPGRADE-The geodatabase will not be upgraded.

         * UPGRADE-The geodatabase will be upgraded. This is the default."""
    ...

@gptooldoc("AddIndex_management", None)
def AddIndex(
    in_table=..., fields=..., index_name=..., unique=..., ascending=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddIndex_management(in_table, fields;fields..., {index_name}, {unique}, {ascending})

       Adds an attribute index to an existing table, feature class,
       shapefile, or attributed relationship class.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the fields to be indexed.
     fields (Field):
         The list of fields that will participate in the index. Any number of
         fields can be specified.
     index_name {String}:
         The name of the new index. An index name is necessary when adding an
         index to geodatabase feature classes and tables. For other types of
         input, the name is ignored.
     unique {Boolean}:
         Specifies whether the values in the index are unique.

         * NON_UNIQUE-All values in the index are not unique. This is the
         default.

         * UNIQUE-All values in the index are unique.
     ascending {Boolean}:
         Specifies whether values are indexed in ascending order.

         * NON_ASCENDING-Values are not indexed in ascending order. This is the
         default.

         * ASCENDING-Values are indexed in ascending order."""
    ...

@gptooldoc("AddSpatialIndex_management", None)
def AddSpatialIndex(
    in_features=..., spatial_grid_1=..., spatial_grid_2=..., spatial_grid_3=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddSpatialIndex_management(in_features, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})

       Adds a spatial index to a shapefile, file geodatabase, mobile
       geodatabase, or enterprise geodatabase feature class. Use this tool
       to either add a spatial index to a shapefile or feature class that
       does not already have one or to re-create an existing spatial index.

    INPUTS:
     in_features (Feature Layer / Mosaic Layer):
         An enterprise geodatabase feature class, file geodatabase feature
         class, mobile geodatabase feature class, or shapefile to which a
         spatial index is to be added or rebuilt.
     spatial_grid_1 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         will be ignored.
     spatial_grid_2 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         will be ignored.
     spatial_grid_3 {Double}:
         This parameter has been deprecated in ArcGIS Pro. Any value you enter
         will be ignored."""
    ...

@gptooldoc("RemoveIndex_management", None)
def RemoveIndex(
    in_table=..., index_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveIndex_management(in_table, index_name;index_name...)

       This tool deletes an attribute index from an existing table, feature
       class, shapefile, or attributed relationship class.

    INPUTS:
     in_table (Table View / Raster Layer / Mosaic Layer):
         The table containing the index or indexes to be deleted. Table can
         refer to an actual table, a feature class attribute table, or an
         attributed relationship class.
     index_name (String):
         The name of the index or indexes to be deleted."""
    ...

@gptooldoc("RemoveSpatialIndex_management", None)
def RemoveSpatialIndex(
    in_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveSpatialIndex_management(in_features)

       Deletes the spatial index from a shapefile or file geodatabase, mobile
       geodatabase, or an enterprise geodatabase feature class.

    INPUTS:
     in_features (Feature Layer / Mosaic Layer):
         The shapefile or file geodatabase, mobile geodatabase, or an
         enterprise geodatabase feature class from which a spatial index will
         be removed."""
    ...

@gptooldoc("AddJoin_management", None)
def AddJoin(
    in_layer_or_view=...,
    in_field=...,
    join_table=...,
    join_field=...,
    join_type=...,
    index_join_fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddJoin_management(in_layer_or_view, in_field, join_table, join_field, {join_type}, {index_join_fields})

       Joins a layer to another layer or table based on a common field.
       Feature layers, table views, and raster layers with a raster attribute
       table are supported.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The layer or table view to which the join table will be joined.
     in_field (Field):
         The field in the input layer or table view on which the join will be
         based.
     join_table (Table View / Raster Layer / Mosaic Layer):
         The table or table view to be joined to the input layer or table view.
     join_field (Field):
         The field in the join table that contains the values on which the join
         will be based.
     join_type {Boolean}:
         Specifies whether only records in the input that match a record in the
         join table will be included in the output.

         * KEEP_ALL-All records in the input layer or table view will be
         included in the output. This is also known as an outer join. This is
         the default.

         * KEEP_COMMON-Only those records in the input that match a row in the
         join table will be included in the output. This is also known as an
         inner join.
     index_join_fields {Boolean}:
         Specifies whether table attribute indexes will be added to both
         joining fields.

         * INDEX_JOIN_FIELDS-Both join fields will be indexed. If the table has
         an existing index, a new index will not be added.

         * NO_INDEX_JOIN_FIELDS-Indexes will not be added. This is the default."""
    ...

@gptooldoc("AddRelate_management", None)
def AddRelate(
    in_layer_or_view=...,
    in_field=...,
    relate_table=...,
    relate_field=...,
    relate_name=...,
    cardinality=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddRelate_management(in_layer_or_view, in_field, relate_table, relate_field, relate_name, {cardinality})

       Relates a layer to another layer or table based on a field value.
       Feature layers, table views, and raster layers with a raster attribute
       table are supported.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The layer or table view to which the relate table will be related.
     in_field (Field):
         The field in the input layer or table view on which the relate will be
         based.
     relate_table (Table View / Raster Layer / Mosaic Layer):
         The table or table view to be related to the input layer or table
         view.
     relate_field (Field):
         The field in the relate table that contains the values on which the
         relate will be based.
     relate_name (String):
         The unique name given to a relate.
     cardinality {String}:
         The cardinality of the relationship.

         * ONE_TO_ONE-Specifies that the relationship between the input table
         and related table is one to one. For example, one record in the input
         table will only have one matching record in the related table.

         * ONE_TO_MANY-Specifies that the relationship between the input table
         and related table is one to many. For example, one record in the input
         table can have multiple matching records in the related table. This is
         the default.

         * MANY_TO_MANY-Specifies that the relationship between the input table
         and related table is many to many. For example, many records with the
         same value in the input table can have multiple matching records in
         the related table."""
    ...

@gptooldoc("AddSpatialJoin_management", None)
def AddSpatialJoin(
    target_features=...,
    join_features=...,
    join_operation=...,
    join_type=...,
    field_mapping=...,
    match_option=...,
    search_radius=...,
    distance_field_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddSpatialJoin_management(target_features, join_features, {join_operation}, {join_type}, {field_mapping}, {match_option}, {search_radius}, {distance_field_name})

       Joins attributes from one feature to another based on the spatial
       relationship. The target features and the joined attributes from the
       join features will be joined. The join is temporary.

    INPUTS:
     target_features (Feature Layer):
         The attributes from the target features and the attributes from the
         joined features will be joined to the target features layer. However,
         a subset of attributes can be defined using the field_mapping
         parameter.
     join_features (Feature Layer):
         The attributes from the join features will be joined to the attributes
         of the target features. See the explanation of the join_operation
         parameter for details on how the aggregation of joined attributes are
         affected by the type of join operation.
     join_operation {String}:
         This parameter is not supported. All joins will be performed as a one-
         to-one join. If you are using positional arguments in Python, use a
         None type, an empty string ("" or ''), or the JOIN_ONE_TO_ONE
         keyword.To achieve a one-to-many join when the output is created in an
         output
         feature class, use the Spatial Join tool.
     join_type {Boolean}:
         Specifies whether only target features with a spatial relationship
         with a join feature (known as an inner join) will be preserved or all
         target features will be preserved, even without a spatial relationship
         with the join features (known as an outer join).

         * KEEP_ALL-All features in the target features layer will be
         preserved. This is known as an outer join. This is the default.

         * KEEP_COMMON-Only those features in the target features layer with a
         spatial relationship with a join feature will be preserved. This is
         known as an inner join.
     field_mapping {Field Mappings}:
         The attribute fields that will be in the output with the corresponding
         field properties and source fields. By default, all fields from the
         inputs will be included.Fields can be added, deleted, renamed, and
         reordered, and you can
         change their properties. Merge rules allow you to specify how
         values from two or more
         input fields will be merged or combined into a single output value.
         The following merge rules can be used to determine how the output
         field will be populated with values:

         * First-Use the input fields' first value.

         * Last-Use the input fields' last value.

         * Join-Concatenate (join) the input field values.

         * Sum-Calculate the total of the input field values.

         * Mean-Calculate the mean (average) of the input field values.

         * Median-Calculate the median (middle) of the input field values.

         * Mode-Use the value with the highest frequency.

         * Min-Use the minimum value of all the input field values.

         * Max-Use the maximum value of all the input field values.

         * Standard deviation-Use the standard deviation classification method
         on all the input field values.

         * Count-Find the number of records included in the calculation.
         In Python, you can use the FieldMappings class to define this
         parameter.
     match_option {String}:
         Specifies the criteria that will be used to match rows.

         * INTERSECT-The features in the join features will be matched if they
         intersect a target feature. This is the default. Specify a distance in
         the search_radius parameter.

         * INTERSECT_3D-The features in the join features will be matched if
         they intersect a target feature in three-dimensional space (x, y, and
         z). Specify a distance in the search_radius parameter.

         * WITHIN_A_DISTANCE-The features in the join features will be matched
         if they are within a specified distance of a target feature. Specify a
         distance in the search_radius parameter.

         * WITHIN_A_DISTANCE_GEODESIC-Same as WITHIN_A_DISTANCE except that
         geodesic distance is used rather than planar distance. Choose this if
         your data covers a large geographic extent or the coordinate system of
         the inputs is unsuitable for distance calculations.

         * WITHIN_A_DISTANCE_3D-The features in the join features will be
         matched if they are within a specified distance of a target feature in
         three-dimensional space. Specify a distance in the search_radius
         parameter.

         * CONTAINS-The features in the join features will be matched if a
         target feature contains them. The target features must be polygons or
         polylines. For this option, the target features cannot be points, and
         the join features can only be polygons when the target features are
         also polygons.

         * COMPLETELY_CONTAINS-The features in the join features will be
         matched if a target feature completely contains them. Polygon can
         completely contain any feature. Point cannot completely contain any
         feature, not even a point. Polyline can completely contain only
         polyline and point.

         * CONTAINS_CLEMENTINI-This spatial relationship yields the same
         results as COMPLETELY_CONTAINS with the exception that if the join
         feature is entirely on the boundary of the target feature (no part is
         properly inside or outside) the feature will not be matched.
         Clementini defines the boundary polygon as the line separating inside
         and outside, the boundary of a line is defined as its end points, and
         the boundary of a point is always empty.

         * WITHIN-The features in the join features will be matched if a target
         feature is within them. It is opposite to CONTAINS. For this option,
         the target features can only be polygons when the join features are
         also polygons. Point can be join feature only if point is target.

         * COMPLETELY_WITHIN-The features in the join features will be matched
         if a target feature is completely within them. This is opposite to
         COMPLETELY_CONTAINS.

         * WITHIN_CLEMENTINI-The result will be identical to WITHIN except if
         the entirety of the feature in the join features is on the boundary of
         the target feature, the feature will not be matched. Clementini
         defines the boundary polygon as the line separating inside and
         outside, the boundary of a line is defined as its end points, and the
         boundary of a point is always empty.

         * ARE_IDENTICAL_TO-The features in the join features will be matched
         if they are identical to a target feature. Both join and target
         feature must be of same shape type-point-to-point, line-to-line, and
         polygon-to-polygon.

         * BOUNDARY_TOUCHES-The features in the join features will be matched
         if they have a boundary that touches a target feature. When the target
         and join features are lines or polygons, the boundary of the join
         feature can only touch the boundary of the target feature and no part
         of the join feature can cross the boundary of the target feature.

         * SHARE_A_LINE_SEGMENT_WITH-The features in the join features will be
         matched if they share a line segment with a target feature. The join
         and target features must be lines or polygons.

         * CROSSED_BY_THE_OUTLINE_OF-The features in the join features will be
         matched if a target feature is crossed by their outline. The join and
         target features must be lines or polygons. If polygons are used for
         the join or target features, the polygon's boundary (line) will be
         used. Lines that cross at a point will be matched, not lines that
         share a line segment.

         * HAVE_THEIR_CENTER_IN-The features in the join features will be
         matched if a target feature's center falls within them. The center of
         the feature is calculated as follows: for polygon and multipoint the
         geometry's centroid is used, and for line input the geometry's
         midpoint is used. Specify a distance in the search_radius parameter.

         * CLOSEST-The feature in the join features that is closest to a target
         feature is matched. See the usage tip for more information. Specify a
         distance in the search_radius parameter.

         * CLOSEST_GEODESIC-Same as CLOSEST except that geodesic distance is
         used rather than planar distance. Choose this if your data covers a
         large geographic extent or the coordinate system of the inputs is
         unsuitable for distance calculations

         * LARGEST_OVERLAP-The feature in the join features will be matched
         with the target feature with the largest overlap.
     search_radius {Linear Unit}:
         Join features within this distance of a target feature will be
         considered for the spatial join. A search radius is only valid when
         the spatial relationship is specified (the match_option parameter is
         set to INTERSECT, WITHIN_A_DISTANCE, WITHIN_A_DISTANCE_GEODESIC,
         HAVE_THEIR_CENTER_IN, CLOSEST, or CLOSEST_GEODESIC). For example,
         using a search radius of 100 meters with the WITHIN_A_DISTANCE spatial
         relationship will join feature within 100 meters of a target feature.
         For the three WITHIN_A_DISTANCE relationships, if no value is
         specified for search_radius, a distance of 0 is used.
     distance_field_name {String}:
         The name of a field, which will be added to the join, that contains
         the distance between the target feature and the closest join feature.
         This parameter is only valid when the spatial relationship is
         specified (match_option is set to CLOSEST or CLOSEST_GEODESIC. The
         value of this field is -1 if no feature is matched within a search
         radius. If no field name is specified, the field will not be added to
         the join."""
    ...

@gptooldoc("JoinField_management", None)
def JoinField(
    in_data=...,
    in_field=...,
    join_table=...,
    join_field=...,
    fields=...,
    fm_option=...,
    field_mapping=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """JoinField_management(in_data, in_field, join_table, join_field, {fields;fields...}, {fm_option}, {field_mapping})

       Permanently joins the contents of a table to another table based on a
       common attribute field. The input table is updated to contain the
       fields from the join table. You can select which fields from the join
       table will be added to the input table.

    INPUTS:
     in_data (Table View / Raster Layer / Mosaic Layer):
         The table or feature class to which the join table will be joined.
     in_field (Field):
         The field in the input table on which the join will be based.
     join_table (Table View / Raster Layer / Mosaic Layer):
         The table that will be joined to the input table.
     join_field (Field):
         The field in the join table that contains the values on which the join
         will be based.
     fields {Field}:
         The fields from the join table that will be transferred to the input
         table based on a join between the input table and the join table.
     fm_option {String}:
         Specifies how joining fields and field types will be transferred to
         the output.

         * NOT_USE_FM-Fields and field types from the joined table will be
         transferred to the output. This is the default.

         * USE_FM-The transfer of fields and field types from the joined table
         to the output will be controlled by the field_mapping parameter.
     field_mapping {Field Mappings}:
         The attribute fields that will be in the output with the corresponding
         field properties and source fields. By default, all fields from the
         inputs will be included.Fields can be added, deleted, renamed, and
         reordered, and you can
         change their properties. Merge rules allow you to specify how
         values from two or more
         input fields will be merged or combined into a single output value.
         The following merge rules can be used to determine how the output
         field will be populated with values:

         * First-Use the input fields' first value.

         * Last-Use the input fields' last value.

         * Join-Concatenate (join) the input field values.

         * Sum-Calculate the total of the input field values.

         * Mean-Calculate the mean (average) of the input field values.

         * Median-Calculate the median (middle) of the input field values.

         * Mode-Use the value with the highest frequency.

         * Min-Use the minimum value of all the input field values.

         * Max-Use the maximum value of all the input field values.

         * Standard deviation-Use the standard deviation classification method
         on all the input field values.

         * Count-Find the number of records included in the calculation.
         In Python, you can use the FieldMappings class to define this
         parameter."""
    ...

@gptooldoc("RemoveJoin_management", None)
def RemoveJoin(
    in_layer_or_view=..., join_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveJoin_management(in_layer_or_view, {join_name})

       Removes a join from a feature layer or table view.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The layer or table view from which the join will be removed.
     join_name {String}:
         The join to be removed."""
    ...

@gptooldoc("RemoveRelate_management", None)
def RemoveRelate(
    in_layer_or_view=..., relate_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveRelate_management(in_layer_or_view, {relate_name})

       Removes a relate from a feature layer or a table view.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The layer or table view from which to remove the relate.
     relate_name {String}:
         The name of the relate to remove."""
    ...

@gptooldoc("ValidateJoin_management", None)
def ValidateJoin(
    in_layer_or_view=..., in_field=..., join_table=..., join_field=..., output_msg=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ValidateJoin_management(in_layer_or_view, in_field, join_table, join_field, {output_msg})

       Validates a join between two layers or tables to determine if the
       layers or tables have valid field names and Object ID fields, if the
       join produces matching records, if the join is a one-to-one or one-to-
       many join, and other properties of the join.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The layer or table view with the join to the join table that will be
         validated.
     in_field (Field):
         The field in the input layer or table view on which the join will be
         based.
     join_table (Table View / Raster Layer / Mosaic Layer):
         The table or table view with the join to the input layer or table view
         that will be validated.
     join_field (Field):
         The field in the join table that contains the values on which the join
         will be based.

    OUTPUTS:
     output_msg {Table}:
         The output table containing the validation messages in a tabular form."""
    ...

@gptooldoc("AddFilesToLasDataset_management", None)
def AddFilesToLasDataset(
    in_las_dataset=..., in_files=..., folder_recursion=..., in_surface_constraints=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddFilesToLasDataset_management(in_las_dataset, {in_files;in_files...}, {folder_recursion}, {in_surface_constraints;in_surface_constraints...})

       Adds references for one or more LAS files and surface constraint
       features to a LAS dataset.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     in_files {LAS Dataset Layer / Folder / File}:
         Inputs that can include any combination of .las files, .zlas files,
         LAS datasets, and folders containing .las or .zlas data. When a LAS
         dataset is specified as input, all .las and .zlas files that have a
         valid path reference will be added to the input LAS dataset.In the
         tool dialog box, a folder can also be specified as an input by
         selecting the folder in Windows Explorer and dragging it onto the
         parameter's input box.
     folder_recursion {Boolean}:
         Specifies whether lidar data residing in the subdirectories of an
         input folder will be added to the LAS dataset.

         * NO_RECURSION-Only lidar files residing in an input folder will be
         added to the LAS dataset. This is the default.

         * RECURSION-All lidar files residing in the subdirectories of an input
         folder will be added to the LAS dataset.
     in_surface_constraints {Value Table}:
         The features that will be referenced by the LAS dataset when
         generating a triangulated surface. Each feature must have the
         following properties defined:

         * in_feature_class-The feature to be referenced by the LAS dataset.

         * height_field-Any numeric field in the feature's attribute table can
         be used to define the height source. If the feature's geometry
         contains z-values, it can be selected by specifying Shape.Z. If no
         height is necessary, specify the keyword <None> to create z-less
         features with elevation that will be interpolated from the surface.

         * SF_type-The surface feature type that defines how the
         feature geometry will be incorporated into the triangulation for the
         surface. Options with hard or soft designation refer to whether the
         feature edges represent distinct breaks in slope or a gradual change.

         * anchorpoints-Elevation points that will not be thinned away. This
         option is only available for single-point feature geometry.

         * hardline or softline-Breaklines that enforce a height value.

         * hardclip or softclip-Polygon dataset that defines the boundary of
         the LAS dataset.

         * harderase or softerase-Polygon dataset that defines holes in the LAS
         dataset.

         * hardreplace or softreplace-Polygon dataset that defines areas of
         constant height."""
    ...

@gptooldoc("BuildLasDatasetPyramid_management", None)
def BuildLasDatasetPyramid(
    in_las_dataset=..., point_selection_method=..., class_codes_weights=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildLasDatasetPyramid_management(in_las_dataset, {point_selection_method}, {class_codes_weights;class_codes_weights...})

       Constructs or updates a LAS dataset display cache, which optimizes its
       rendering performance.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     point_selection_method {String}:
         Specifies how the point in each binned region will be selected to
         construct the pyramid. This parameter is disabled if the LAS dataset
         contains a pyramid.

         * Z_MIN-The point with the lowest z-value will be selected.

         * Z_MAX-The point with the highest z-value will be selected.

         * CLOSEST_TO_CENTER-The point that is closest to the center of the
         binned region will be selected.

         * CLASS_CODE-The point with the highest weight value will be selected.
     class_codes_weights {Value Table}:
         The weights assigned to each class code that determine which points
         are retained in each thinning region. This parameter is only enabled
         when the Class Code Weights option is specified in the Point Selection
         Method parameter. The class code with the highest weight will be
         retained in the thinning region. If two class codes with the same
         weight exist in a given thinning region, the class code with the
         smallest point source ID will be retained."""
    ...

@gptooldoc("CreateLasDataset_management", None)
def CreateLasDataset(
    input=...,
    out_las_dataset=...,
    folder_recursion=...,
    in_surface_constraints=...,
    spatial_reference=...,
    compute_stats=...,
    relative_paths=...,
    create_las_prj=...,
    extent=...,
    boundary=...,
    add_only_contained_files=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateLasDataset_management(input;input..., out_las_dataset, {folder_recursion}, {in_surface_constraints;in_surface_constraints...}, {spatial_reference}, {compute_stats}, {relative_paths}, {create_las_prj}, {extent}, {boundary}, {add_only_contained_files})

       Creates a LAS dataset referencing one or more .las files and optional
       surface constraint features.

    INPUTS:
     input (LAS Dataset Layer / Folder / File):
         The .las files, LAS datasets, and folders containing .las files that
         will be referenced by the LAS dataset. This information can be
         supplied as a string containing all the input data or a list of
         strings containing specific data elements (for example, "lidar1.las;
         lidar2.las; folder1; folder2" or ["lidar1.las", "lidar2.las",
         "folder1", "folder2"]).
     folder_recursion {Boolean}:
         Specifies whether lidar data residing in the subdirectories of an
         input folder will be added to the LAS dataset.

         * NO_RECURSION-Only lidar files residing in an input folder will be
         added to the LAS dataset. This is the default.

         * RECURSION-All lidar files residing in the subdirectories of an input
         folder will be added to the LAS dataset.
     in_surface_constraints {Value Table}:
         The features that will be referenced by the LAS dataset when
         generating a triangulated surface. Each feature must have the
         following properties defined:

         * in_feature_class-The feature to be referenced by the LAS dataset.

         * height_field-Any numeric field in the feature's attribute table can
         be used to define the height source. If the feature's geometry
         contains z-values, it can be selected by specifying Shape.Z. If no
         height is necessary, specify the keyword <None> to create z-less
         features with elevation that will be interpolated from the surface.

         * SF_type-The surface feature type that defines how the
         feature geometry will be incorporated into the triangulation for the
         surface. Options with hard or soft designation refer to whether the
         feature edges represent distinct breaks in slope or a gradual change.

         * anchorpoints-Elevation points that will not be thinned away. This
         option is only available for single-point feature geometry.

         * hardline or softline-Breaklines that enforce a height value.

         * hardclip or softclip-Polygon dataset that defines the boundary of
         the LAS dataset.

         * harderase or softerase-Polygon dataset that defines holes in the LAS
         dataset.

         * hardreplace or softreplace-Polygon dataset that defines areas of
         constant height.
     spatial_reference {Coordinate System}:
         The spatial reference of the LAS dataset. If no spatial reference is
         explicitly assigned, the LAS dataset will use the coordinate system of
         the first input .las file. If the input files do not contain spatial
         reference information and the coordinate system is not set, the
         coordinate system of the LAS dataset will be listed as unknown.
     compute_stats {Boolean}:
         Specifies whether statistics for the .las files will be computed and a
         spatial index generated for the LAS dataset. The presence of
         statistics allows the LAS dataset layer's filtering and symbology
         options to only show LAS attribute values that exist in the .las
         files. A .lasx auxiliary file is created for each .las file.

         * COMPUTE_STATS-Statistics will be computed.

         * NO_COMPUTE_STATS-Statistics will not be computed. This is the
         default.
     relative_paths {Boolean}:
         Specifies whether lidar files and surface constraint features will be
         referenced by the LAS dataset through relative or absolute paths.
         Using relative paths may be convenient for cases in which the LAS
         dataset and its associated data will be relocated in the file system
         using the same relative location to one another.

         * ABSOLUTE_PATHS-Absolute paths will be used for the data referenced
         by the LAS dataset. This is the default.

         * RELATIVE_PATHS-Relative paths will be used for the data referenced
         by the LAS dataset.
     create_las_prj {String}:
         Specifies whether .prj files will be created for the .las files
         referenced by the LAS dataset.

         * NO_FILES-No .prj files will be created. This is the default.

         * FILES_MISSING_PROJECTION-Corresponding .prj files will be created
         for .las files with no spatial reference.

         * ALL_FILES-Corresponding .prj files will be created for all .las
         files.
     extent {Extent}:
         The processing extent will be used to select a subset of .las files
         from the list of files and folders in the input parameter value. Any
         .las files that fall entirely outside of this extent will be excluded
         from the resulting LAS dataset. Additionally, .las files that fall
         partially outside the extent will be excluded if the
         add_only_contained_files parameter is set to INTERSECTED_FILES.
     boundary {Feature Layer}:
         The polygon features whose boundary will be used to select a subset of
         .las files from the list of files and folders in the input parameter.
         Any .las files that fall entirely outside of the polygon features will
         be excluded from the resulting LAS dataset. Additionally, .las files
         that fall partially outside the polygons will be excluded if the
         add_only_contained_files parameter is set to INTERSECTED_FILES.
     add_only_contained_files {Boolean}:
         Specifies whether the .las files that will be added to the LAS dataset
         must be fully or partially contained by either the processing extent,
         the processing boundary polygon, or the intersection of both.

         * CONTAINED_FILES-All files that intersect the processing extent,
         processing boundary, or the intersection of both will be added to the
         LAS dataset. This is the default.

         * INTERSECTED_FILES-Only files that are entirely contained by the
         processing extent, processing boundary, or the intersection of both
         will be added to the LAS dataset.

    OUTPUTS:
     out_las_dataset (LAS Dataset):
         The LAS dataset that will be created."""
    ...

@gptooldoc("LasDatasetStatistics_management", None)
def LasDatasetStatistics(
    in_las_dataset=...,
    calculation_type=...,
    out_file=...,
    summary_level=...,
    delimiter=...,
    decimal_separator=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """LasDatasetStatistics_management(in_las_dataset, {calculation_type}, {out_file}, {summary_level}, {delimiter}, {decimal_separator})

       Calculates or updates statistics for a LAS dataset and generates an
       optional statistics report.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     calculation_type {Boolean}:
         Specifies whether statistics will be calculated for all lidar files or
         only for those that do not have statistics:

         * SKIP_EXISTING_STATS-LAS files with up-to-date statistics will be
         skipped, and statistics will only be calculated for newly added LAS
         files or ones that were updated since the initial calculation. This is
         the default.

         * OVERWRITE_EXISTING_STATS-Statistics will be calculated for all LAS
         files, including ones that have up-to-date statistics. This is useful
         if the LAS files were modified in an external application that went
         undetected by ArcGIS.
     summary_level {String}:
         Specify the type of summary contained in the report.

         * DATASET-The report will summarize statistics for the entire LAS
         dataset. This is the default.

         * LAS_FILES-The report will summarize statistics for the LAS files
         referenced by the LAS dataset.
     delimiter {String}:
         The delimiter that will be used to indicate the separation of entries
         in the columns of the text file table.

         * SPACE-A space will be used to delimit field values. This is the
         default.

         * COMMA-A comma will be used to delimit field values. This option is
         not applicable if the decimal separator is also a comma.
     decimal_separator {String}:
         The decimal character that will be used in the text file to
         differentiate the integer of a number from its fractional part.

         * DECIMAL_POINT-A point will be used as the decimal character. This is
         the default.

         * DECIMAL_COMMA-A comma will be used as the decimal character.

    OUTPUTS:
     out_file {Text File}:
         The output text file that will contain the summary of the LAS dataset
         statistics."""
    ...

@gptooldoc("LasPointStatsAsRaster_management", None)
def LasPointStatsAsRaster(
    in_las_dataset=...,
    out_raster=...,
    method=...,
    sampling_type=...,
    sampling_value=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """LasPointStatsAsRaster_management(in_las_dataset, out_raster, {method}, {sampling_type}, {sampling_value})

       Creates a raster whose cell values reflect statistical information
       about LAS points.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     method {String}:
         Specifies the type of statistics that will be collected about the LAS
         points in each cell of the output raster.

         * PULSE_COUNT-The number of last return points will be collected.

         * POINT_COUNT-The number of points from all returns will be collected.

         * PREDOMINANT_LAST_RETURN-The most frequent last return value will be
         collected.

         * PREDOMINANT_CLASS-The most frequent class code will be collected.

         * INTENSITY_RANGE-The range of intensity values will be collected.

         * Z_RANGE-The range of elevation values will be collected.
     sampling_type {String}:
         Specifies the method that will be used for interpreting the Sampling
         Value parameter value to define the resolution of the output raster.

         * OBSERVATIONS-The number of cells that divide the lengthiest side of
         the LAS dataset extent will be used.

         * CELLSIZE-The cell size of the output raster will be used. This is
         the default.
     sampling_value {Double}:
         The value used in conjunction with the Sampling Type parameter to
         define the resolution of the output raster.

    OUTPUTS:
     out_raster (Raster Dataset):
         The location and name of the output raster. When storing a raster
         dataset in a geodatabase or in a folder such as an Esri Grid, do not
         add a file extension to the name of the raster dataset. A file
         extension can be provided to define the raster's format when storing
         it in a folder, such as .tif to generate a GeoTIFF or .img to generate
         an ERDAS IMAGINE format file.If the raster is stored as a TIFF file or
         in a geodatabase, its raster
         compression type and quality can be specified using geoprocessing
         environment settings."""
    ...

@gptooldoc("RemoveFilesFromLasDataset_management", None)
def RemoveFilesFromLasDataset(
    in_las_dataset=..., in_files=..., in_surface_constraints=..., delete_pyramid=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveFilesFromLasDataset_management(in_las_dataset, {in_files;in_files...}, {in_surface_constraints;in_surface_constraints...}, {delete_pyramid})

       Removes one or more LAS files and surface constraint features from a
       LAS dataset.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     in_files {String}:
         The name of the LAS files or folders containing LAS files whose
         reference will be removed from the LAS dataset.
     in_surface_constraints {String}:
         The name of the surface constraint features that will be removed from
         the LAS dataset.
     delete_pyramid {Boolean}:
         Specifies whether the LAS dataset's display pyramid will be deleted.

         * DELETE_PYRAMID-The LAS dataset's display pyramid will be deleted.

         * NO_DELETE_PYRAMID-The LAS dataset's display pyramid will not be
         deleted. This is the default."""
    ...

@gptooldoc("ApplySymbologyFromLayer_management", None)
def ApplySymbologyFromLayer(
    in_layer=..., in_symbology_layer=..., symbology_fields=..., update_symbology=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ApplySymbologyFromLayer_management(in_layer, in_symbology_layer, {symbology_fields;symbology_fields...}, {update_symbology})

       Applies the symbology from a specified layer or layer file to the
       input. It can be applied to feature, raster, network analysis, TIN,
       and geostatistical layers.

    INPUTS:
     in_layer (Feature Layer / Raster Layer / Layer):
         The layer to which the symbology will be applied.
     in_symbology_layer (Layer):
         The layer containing the symbology that will be applied to the input
         layer. Both .lyrx and .lyr files are supported.
     symbology_fields {Value Table}:
         The fields from the input layer that match the symbology
         fields used in the symbology layer. Symbology fields contain three
         properties:

         * Field type-The field type: symbology value, normalization, or other
         type.

         * Source field-The symbology field used by the symbology layer. Use a
         blank value or "#" if you do not know the source field and want to use
         the default.

         * Target field-The field from the input layer to use when applying the
         symbology.
                 Supported field types are as follows:

         * VALUE_FIELD-Primary field used to symbolize values

         * NORMALIZATION_FIELD-Field used to normalize quantitative values

         * EXCLUSION_CLAUSE_FIELD-Field used for the symbology exclusion clause

         * CHART_RENDERER_PIE_SIZE_FIELD-Field used to set the size of pie
         chart symbols

         * ROTATION_XEXPRESSION_FIELD-Field used to set the rotation of symbols
         on the x-axis

         * ROTATION_YEXPRESSION_FIELD-Field used to set the rotation of symbols
         on the y-axis

         * ROTATION_ZEXPRESSION_FIELD-Field used to set the rotation of symbols
         on the z-axis

         * TRANSPARENCY_EXPRESSION_FIELD-Field used to set the transparency of
         symbols

         * TRANSPARENCY_NORMALIZATION_FIELD-Field used to normalize
         transparency values

         * SIZE_EXPRESSION_FIELD-Field used to set the size or width of symbols

         * COLOR_EXPRESSION_FIELD-Field used to set the color of symbols

         * PRIMITIVE_OVERRIDE_EXPRESSION_FIELD-Field used to set various
         properties on individual symbol layers
     update_symbology {String}:
         Specifies whether symbology ranges will be updated.

         * DEFAULT-Symbology ranges will be updated, except in the
         following situations:

         * When the input layer is empty

         * When the symbology layer uses class breaks (for example, graduated
         colors or graduated symbols) and the classification method is manual
         or defined interval

         * When the symbology layer uses unique values and the Show all other
         values option is checked

         * UPDATE-Symbology ranges will be updated.

         * MAINTAIN-Symbology ranges will not be updated; they will be
         maintained."""
    ...

@gptooldoc("MakeAggregationQueryLayer_management", None)
def MakeAggregationQueryLayer(
    target_feature_class=...,
    target_join_field=...,
    related_table=...,
    related_join_field=...,
    out_layer=...,
    statistics=...,
    parameter_definitions=...,
    oid_fields=...,
    shape_type=...,
    srid=...,
    spatial_reference=...,
    m_values=...,
    z_values=...,
    extent=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeAggregationQueryLayer_management(target_feature_class, target_join_field, related_table, related_join_field, out_layer, {statistics;statistics...}, {parameter_definitions;parameter_definitions...}, {oid_fields;oid_fields...}, {shape_type}, {srid}, {spatial_reference}, {m_values}, {z_values}, {extent})

       Creates a query layer that summarizes, aggregates, and filters DBMS
       tables dynamically based on time, range, and attribute queries from a
       related table, and joins the result to a feature layer.

    INPUTS:
     target_feature_class (Feature Class):
         The feature class or spatial table from an enterprise database.
     target_join_field (Field):
         The field in the target feature class on which the join will be based.
     related_table (Table / Feature Class):
         The input table containing the fields that will be used to calculate
         statistics. Statistics are joined to the out_layer value.
     related_join_field (Field):
         A field in the summary table that contains the values on which the
         join will be based. Aggregation or summary statistics are also
         calculated separately for each unique attribute value from this field.
     statistics {Value Table}:
         Specifies the numeric field or fields containing the attribute values
         that will be used to calculate the specified statistic. Multiple
         statistic and field combinations can be specified. Null values are
         excluded from all statistical calculations.The output layer will
         include a ROW_COUNT field showing total count
         (or frequency) of each unique value from the related_join_field value.
         The difference between the ROW_COUNT field and the COUNT statistic
         type is that ROW_COUNT includes null values while COUNT excludes null
         values.

         * COUNT-The number of values included in the statistical calculations
         will be found. Each value will be counted except null values.

         * SUM-The values for the specified field will be added together.

         * AVG-The average for the specified field will be calculated.

         * MIN-The smallest value for all records of the specified field will
         be found.

         * MAX-The largest value for all records of the specified field will be
         found.

         * STDDEV-The standard deviation of values in the specified field will
         be calculated.
     parameter_definitions {Value Table}:
         Specifies one or more query parameters for criteria or conditions;
         records matching these criteria are used while computing aggregated
         results. A query parameter is similar to an SQL statement variable for
         which the value is defined when the query is run. This allows you to
         dynamically change query filters for the output layer. You can think
         of a parameter as a predicate or condition in a SQL where clause. For
         example Country_Name = 'Nigeria' in a SQL where clause is called a
         predicate in which the = is a comparison operator, Country_Name is a
         field name on the left, and 'Nigeria' is a value on the right. When
         you define more than one parameter, you must specify a logical
         operator between them (such as AND, OR, and so on).When not specified,
         all records from the related table will be used in
         computing aggregated or summary results. The two parameter
         definition types are the following:

         * Range-Connect numeric or temporal values dynamically to the range
         and time sliders.

         * Discrete-Update a query with literal values when the query is run.
         The following properties are available:

         * Parameter Type-The parameter type can be RANGE or DISCRETE.

         * Name-The name of the parameter, which is similar to a variable name.
         A name cannot contain spaces or special characters. Once the output
         query layer is created and the layer source SQL statement is checked,
         this name in the SQL statement that defines the output query layer
         source,will be prefixed with either ::r: (for range parameter) or ::
         (for discrete parameter).

         * Alias-The alias for the parameter name. The alias can include spaces
         and special characters.

         * Field or Expression-A field name or a valid SQL expression that will
         be used in the left side of a predicate or condition in a where
         clause.

         * Data type-The data type of the field or expression
         specified in the Field or Expression column. When the Parameter Type
         value is RANGE, the Data type column value cannot be STRING.

         * DATE-The data type of the field or expression will be Date (date
         time).

         * STRING-The data type of the field or expression will be String
         (text).

         * INTEGER-The data type of the field or expression will be Integer
         (whole numbers).

         * DOUBLE-The data type of the field or expression will be Double
         (fractional numbers).

         * Start Value-The default start value for the RANGE column. This is
         the value that will be used when the time or range slider is not
         enabled. When the Start Value and End Value column values are omitted
         and the time or range slider is disabled, all records from the related
         table will be used to compute aggregated results. This value is
         ignored when the Parameter Type column is set to DISCRETE.

         * End Value-The default end value for the RANGE parameter. This is the
         value that will be used when the time or range slider is not enabled.
         When the Start Value and End Value column values are omitted and the
         time or range slider is disabled, all records from the related table
         will be used to compute aggregated results. This value is ignored when
         the Parameter Type column is set to DISCRETE.

         * Operator for Discrete Parameter-The comparison operator
         that will be used between the Field or Expression column value and a
         value in an SQL predicate or condition.

         * NONE-Choose NONE when Parameter Type is set to RANGE.

         * EQUAL TO-Compare the equality of a field or expression to a value.

         * NOT EQUAL TO-Test whether a field or expression is not equal to a
         value.

         * GREATER THAN-Test whether a field or expression is higher than a
         value.

         * LESS THAN-Test whether a field or expression is lower than a value.

         * INCLUDE VALUES-Determine whether a value from a field or expression
         matches any value in a list.

         * Default Discrete Values-When the Parameter Type value is DISCRETE,
         you must provide a default value. When Operator for Discrete Parameter
         is INCLUDE VALUES, you can provide multiple values separated by
         commas, for example VANDALISM,BURGLARY/THEFT.

         * Operator for Next Parameter-The logical operator between
         this operator and the next one. This column is only applicable when
         you have more than one parameter definition.

         * NONE-Choose NONE when there are no more parameters.

         * AND-Combine two conditions and select a record if both conditions
         are true.

         * OR-Combine two conditions and select a record if at least one
         condition is true.
     oid_fields {String}:
         The unique identifier fields that will be used to uniquely identify
         each row in the table.
     shape_type {String}:
         Specifies the shape type of the query layer. Only those records from
         the result set of the query that match the specified shape type will
         be used in the output query layer. By default, the shape type of the
         first record in the result set will be used. This parameter is ignored
         if the result set of the query does not return a geometry field.

         * POINT-The output query layer will use point geometry.

         * MULTIPOINT-The output query layer will use multipoint geometry.

         * POLYGON-The output query layer will use polygon geometry.

         * POLYLINE-The output query layer will use polyline geometry.
     srid {String}:
         The spatial reference identifier (SRID) value for queries that return
         geometry. Only those records from the result set of the query that
         match the specified SRID value will be used in the output query layer.
         By default, the SRID value of the first record in the result set will
         be used. This parameter is ignored if the result set of the query does
         not return a geometry field.
     spatial_reference {Spatial Reference}:
         The coordinate system that will be used by the output query layer. By
         default, the spatial reference of the first record in the result set
         will be used. This parameter is ignored if the result set of the query
         does not return a geometry field.
     m_values {Boolean}:
         Specifies whether the output layer will include linear measurements
         (m-values).

         * INCLUDE_M_VALUES-The layer will include m-values.

         * DO_NOT_INCLUDE_M_VALUES-The layer will not include m-values. This is
         the default.
     z_values {Boolean}:
         Specifies whether the output layer will include elevation values
         (z-values).

         * INCLUDE_Z_VALUES-The layer will include z-values.

         * DO_NOT_INCLUDE_Z_VALUES-The layer will not include z-values. This is
         the default.
     extent {Extent}:
         Specifies the extent of the layer. The extent must include all
         features in the table.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.

    OUTPUTS:
     out_layer (Feature Layer):
         The output name of the query layer that will be created."""
    ...

@gptooldoc("MakeBuildingLayer_management", None)
def MakeBuildingLayer(
    in_feature_dataset=..., out_layer=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeBuildingLayer_management(in_feature_dataset, out_layer)

       Creates a composite building layer from a dataset, either a BIM file
       workspace or a geodatabase dataset, such as the output of the BIM File
       To Geodatabase tool.

    INPUTS:
     in_feature_dataset (Feature Dataset / BIM File Workspace):
         The input dataset from which the new building feature layers will be
         made. The building layer keeps the structure and the symbology grouped
         together.

    OUTPUTS:
     out_layer (Building Layer):
         The name of the feature layer that will be created. The layer can be
         used as input to any geoprocessing tool that accepts a feature layer
         as input."""
    ...

@gptooldoc("MakeFeatureLayer_management", None)
def MakeFeatureLayer(
    in_features=..., out_layer=..., where_clause=..., workspace=..., field_info=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeFeatureLayer_management(in_features, out_layer, {where_clause}, {workspace}, {field_info})

       Creates a feature layer from an input feature class or layer file. The
       layer that is created is temporary and will not persist after the
       session ends unless the layer is saved to disk or the map document is
       saved.

    INPUTS:
     in_features (Feature Layer):
         The input feature class or layer from which the new layer will be
         made. Complex feature classes, such as annotation and dimensions, are
         not valid inputs.
     where_clause {SQL Expression}:
         An SQL expression used to select a subset of features. For more
         information on SQL syntax see the help topic SQL reference for query
         expressions used in ArcGIS.If the input is a layer with an existing
         definition query and a where
         clause is specified with this parameter, both where clauses will be
         combined with an AND operator for the output layer. For example, if
         the input layer has a where clause of ID > 10 and this parameter is
         set to ID < 20, the resulting layer's where clause will be ID > 10 AND
         ID < 20.
     workspace {Workspace / Feature Dataset}:
         The input workspace used to validate the field names. If the input is
         a geodatabase table and the output workspace is a dBASE table, the
         field names may be truncated, since dBASE fields can only have names
         of ten characters or less.
     field_info {Field Info}:
         The fields from the input that will be renamed and made visible in the
         output. A split policy can be specified.

    OUTPUTS:
     out_layer (Feature Layer):
         The name of the feature layer to be created. The newly created layer
         can be used as input to any geoprocessing tool that accepts a feature
         layer as input."""
    ...

@gptooldoc("MakeImageServerLayer_management", None)
def MakeImageServerLayer(
    in_image_service=...,
    out_imageserver_layer=...,
    template=...,
    band_index=...,
    mosaic_method=...,
    order_field=...,
    order_base_value=...,
    lock_rasterid=...,
    cell_size=...,
    where_clause=...,
    processing_template=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeImageServerLayer_management(in_image_service, out_imageserver_layer, {template}, {band_index;band_index...}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {cell_size}, {where_clause}, {processing_template})

       Creates a temporary raster layer from an image service. The layer that
       is created will not persist after the session ends unless the document
       is saved.

    INPUTS:
     in_image_service (Image Service / String):
         The name of the input image service or the SOAP URL that references
         the image service.An example of using the image service name called
         ProjectX is:
         C:\\MyProject\\ServerConnection.ags\\ProjectX.ImageServer.An example of a
         URL is
         http://AGSServer:8399/arcgis/services/ISName/ImageServer.
     template {Extent}:
         The output extent of the image layer.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     band_index {Value Table}:
         The bands that will be exported for the layer. If no bands are
         specified, all the bands will be used in the output.
     mosaic_method {String}:
         The mosaic method defines how the mosaic is created from different
         rasters.

         * SEAMLINE-Smooth transitions between images using seamlines.

         * NORTH_WEST-Display imagery that is closest to the northwest corner
         of the mosaic dataset boundary.

         * CLOSEST_TO_CENTER-Display imagery that is closest to the center of
         the screen.

         * LOCK_RASTER-Select specific raster datasets to display.

         * BY_ATTRIBUTE-Display and prioritize imagery based on a field in the
         attribute table.

         * CLOSEST_TO_NADIR-Display the rasters with viewing angles closest to
         zero.

         * CLOSEST_TO_VIEWPOINT-Display imagery that is closest to a selected
         viewing angle.

         * NONE-Order rasters based on the ObjectID in the mosaic dataset
         attribute table.
     order_field {String}:
         The default field to use to order the rasters when the mosaic method
         is By_Attribute. The list of fields is defined as those in the service
         table that are of type metadata and are integer (for example, the
         values can represent dates or cloud cover percentage).
     order_base_value {String}:
         The images are sorted based on the difference between this input value
         and the attribute value in the specified field.
     lock_rasterid {String}:
         The raster ID or raster name to which the service should be locked,
         such that only the specified rasters are displayed. If left blank
         (undefined), it will be similar to the system default. Multiple IDs
         can be defined as a semicolon-delimited list.
     cell_size {Double}:
         The cell size for the output image service layer.
     where_clause {SQL Expression}:
         Define a query using SQL.
     processing_template {String}:
         The raster function processing template that can be applied on the
         output image service layer.

         * None-No processing template.

    OUTPUTS:
     out_imageserver_layer (Raster Layer):
         The name of the output image layer."""
    ...

@gptooldoc("MakeLasDatasetLayer_management", None)
def MakeLasDatasetLayer(
    in_las_dataset=...,
    out_layer=...,
    class_code=...,
    return_values=...,
    no_flag=...,
    synthetic=...,
    keypoint=...,
    withheld=...,
    surface_constraints=...,
    overlap=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeLasDatasetLayer_management(in_las_dataset, out_layer, {class_code;class_code...}, {return_values;return_values...}, {no_flag}, {synthetic}, {keypoint}, {withheld}, {surface_constraints;surface_constraints...}, {overlap})

       Creates a LAS dataset layer that can apply filters to LAS points and
       control the enforcement of surface constraint features.

    INPUTS:
     in_las_dataset (LAS Dataset Layer):
         The LAS dataset to process.
     class_code {String}:
         Specifies the classification codes that will be used to filter LAS
         points. All class codes will be selected by default.

         * 0-Never processed by a classification method

         * 1-Processed by a classification method but could not be determined

         * 2-Bare earth measurements

         * 3-Vegetation whose height is considered to be low for the area

         * 4-Vegetation whose height is considered to be intermediate for the
         area

         * 5-Vegetation whose height is considered to be high for the area

         * 6-Structure with roof and walls

         * 7-Erroneous or undesirable data that is closer to the ground

         * 8-Reserved for later use, but used for model key points in LAS 1.1 -
         1.3

         * 9-Water

         * 10-Railway tracks used by trains

         * 11-Road surfaces

         * 12-Reserved for later use, but used for overlap points in LAS 1.1 -
         1.3

         * 13-Shielding around electrical wires

         * 14-Power lines

         * 15-A lattice tower used to support an overhead power line

         * 16-A mechanical assembly that joins an electrical circuit

         * 17-The surface of a bridge

         * 18-Erroneous or undesirable data that is far from the ground

         * 19 - 63-Reserved class codes for ASPRS designation

         * 64 - 255-User-definable class codes
     return_values {String}:
         Specifies the ordinal pulse return values that will be used to filter
         LAS points. All returns will be used when no value is specified.
         Return information is only available for LAS point clouds collected
         from a lidar scanner. The return number reflects the order of discrete
         points obtained from the lidar pulse, whereby the first return is
         closest to the scanner and the last return is farthest from the
         scanner.

         * LAST-The last point from all lidar pulses will be used.

         * FIRST_OF_MANY-The first point from each lidar pulse with multiple
         returns will be used.

         * LAST_OF_MANY-The last point from each lidar pulse with multiple
         returns will be used.

         * SINGLE-All points from lidar pulses with only one return will be
         used.

         * 1-All points with a return value of 1 will be used.

         * 2-All points with a return value of 2 will be used.

         * 3-All points with a return value of 3 will be used.

         * 4-All points with a return value of 4 will be used.

         * 5-All points with a return value of 5 will be used.

         * 6-All points with a return value of 6 will be used.

         * 7-All points with a return value of 7 will be used.

         * 8-All points with a return value of 8 will be used.

         * 9-All points with a return value of 9 will be used.

         * 10-All points with a return value of 10 will be used.

         * 11-All points with a return value of 11 will be used.

         * 12-All points with a return value of 12 will be used.

         * 13-All points with a return value of 13 will be used.

         * 14-All points with a return value of 14 will be used.

         * 15-All points with a return value of 15 will be used.
     no_flag {Boolean}:
         Specifies whether data points that do not have classification flags
         assigned will be included for display and analysis.

         * INCLUDE_UNFLAGGED-Unflagged points will be included. This is the
         default.

         * EXCLUDE_UNFLAGGED-Unflagged points will be excluded.
     synthetic {Boolean}:
         Specifies whether data points flagged as synthetic will be included.
         Synthetic points refer to LAS points that originated from a data
         source other than a lidar scanner.

         * INCLUDE_SYNTHETIC-Synthetic points will be included. This is the
         default.

         * EXCLUDE_SYNTHETIC-Synthetic points will be excluded.
     keypoint {Boolean}:
         Specifies whether data points flagged as model key points will be
         included. Model key points refer to LAS points that are significant
         for modeling the object they are associated with.

         * INCLUDE_KEYPOINT-Model key points will be included. This is the
         default.

         * EXCLUDE_KEYPOINT-Model key points will be excluded.
     withheld {Boolean}:
         Specifies whether data points flagged as withheld will be included.
         Withheld points represent erroneous or undesired measurements captured
         in the LAS points.

         * INCLUDE_WITHHELD-Withheld points will be included.

         * EXCLUDE_WITHHELD-Withheld points will be excluded. This is the
         default.
     surface_constraints {String}:
         The name of the surface constraint features that will be enabled in
         the layer. All constraints are enabled by default.
     overlap {Boolean}:
         Specifies whether data points flagged as overlap will be included.
         Overlap points refer to points collected in overlapping scans that
         typically have a larger scan angle. Filtering overlap points can help
         ensure a regular distribution of LAS points is achieved across the
         extent of the data.

         * INCLUDE_OVERLAP-Overlap points will be included. This is the
         default.

         * EXCLUDE_OVERLAP-Overlap points will be excluded.

    OUTPUTS:
     out_layer (LAS Dataset Layer):
         The name of the resulting LAS dataset layer. A backslash or forward
         slash can be used to denote a group layer."""
    ...

@gptooldoc("MakeMosaicLayer_management", None)
def MakeMosaicLayer(
    in_mosaic_dataset=...,
    out_mosaic_layer=...,
    where_clause=...,
    template=...,
    band_index=...,
    mosaic_method=...,
    order_field=...,
    order_base_value=...,
    lock_rasterid=...,
    sort_order=...,
    mosaic_operator=...,
    cell_size=...,
    processing_template=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeMosaicLayer_management(in_mosaic_dataset, out_mosaic_layer, {where_clause}, {template}, {band_index;band_index...}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {sort_order}, {mosaic_operator}, {cell_size}, {processing_template})

       Creates a mosaic layer from a mosaic dataset or layer file. The layer
       that is created by the tool is temporary and will not persist after
       the session ends unless the layer is saved as a layer file or the map
       is saved.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The path and name of the input mosaic dataset.
     where_clause {SQL Expression}:
         Define a query using SQL.
     template {Extent}:
         The output extent can be specified by defining the four coordinates or
         by using the extent of an existing layer.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     band_index {Value Table}:
         The bands that will be exported for the layer. If no bands are
         specified, all the bands will be used in the output.
     mosaic_method {String}:
         Choose the mosaic method. The mosaic method defines how the layer is
         created from different rasters in the mosaic dataset.

         * CLOSEST_TO_CENTER-Sorts rasters based on an order where rasters that
         have their center closest to the view center are placed on top.

         * NORTH_WEST-Sorts rasters based on an order where rasters that have
         their center closest to the northwest are placed on top.

         * LOCK_RASTER-Enables a user to lock the display of single or multiple
         rasters, based on an ID or name. When you choose this option, you need
         to specify the Lock Raster ID.

         * BY_ATTRIBUTE-Sorts rasters based on an attribute field and its
         difference from the base value. When this option is chosen, the order
         field and order base value parameters also need to be set.

         * CLOSEST_TO_NADIR-Sorts rasters based on an order where rasters that
         have their nadir position closest to the view center are placed on
         top. The nadir point can be different from the center point,
         especially in oblique imagery.

         * CLOSEST_TO_VIEWPOINT-Sorts rasters based on an order where the nadir
         position is closest to the user-defined viewpoint location and are
         placed on top.

         * SEAMLINE-Cuts the rasters using the predefined seamline shape for
         each raster using optional feathering along the seams. The ordering is
         predefined during seamline generation. The LAST mosaic operator is not
         valid with this mosaic method.
     order_field {String}:
         Choose the order field. When the mosaic method is BY_ATTRIBUTE, the
         default field to use when ordering rasters needs to be set. The list
         of fields is defined as those in the service table that are of type
         metadata.
     order_base_value {String}:
         The order base value. The images are sorted based on the difference
         between this value and the attribute value in the specified field.
     lock_rasterid {String}:
         The Raster ID or raster name to which the service should be locked so
         that only the specified rasters are displayed. If left undefined, it
         will be similar to the system default. Multiple IDs can be defined as
         a semicolon-delimited list.
     sort_order {String}:
         Choose whether the sort order is ascending or descending.

         * ASCENDING-The sort order will be ascending. This is the default.

         * DESCENDING-The sort order will be descending.
     mosaic_operator {String}:
         Choose the mosaic operator to use. When two or more rasters have the
         same sort priority, this parameter is used to further refine the sort
         order.

         * FIRST-The first raster in the list will be on top. This is the
         default.

         * LAST-The last raster in the list will be on top.

         * MIN-The raster with the lowest value will be on top.

         * MAX-The raster with the highest value will be on top.

         * MEAN-The average pixel value will be on top.

         * BLEND-The output cell value will be a blend of values; this blend
         value relies on an algorithm that is weight based and dependent on the
         distance from the pixel to the edge within the overlapping area.

         * SUM-The output cell value will be the aggregate of all overlapping
         cells.
     cell_size {Double}:
         The cell size of the output mosaic layer.
     processing_template {String}:
         The raster function processing template that can be applied on the
         output mosaic layer.

         * None-No processing template.

    OUTPUTS:
     out_mosaic_layer (Mosaic Layer):
         The name of the output mosaic layer."""
    ...

@gptooldoc("MakeQueryLayer_management", None)
def MakeQueryLayer(
    input_database=...,
    out_layer_name=...,
    query=...,
    oid_fields=...,
    shape_type=...,
    srid=...,
    spatial_reference=...,
    spatial_properties=...,
    m_values=...,
    z_values=...,
    extent=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeQueryLayer_management(input_database, out_layer_name, query, {oid_fields;oid_fields...}, {shape_type}, {srid}, {spatial_reference}, {spatial_properties}, {m_values}, {z_values}, {extent})

       Creates a query layer from a DBMS table based on an input SQL select
       statement.

    INPUTS:
     input_database (Workspace):
         The database connection file that contains the data to be queried.
     out_layer_name (String):
         The output name of the feature layer or table view to be created.
     query (String):
         The SQL statement that defines the select query to be issued to the
         database.
     oid_fields {String}:
         One or more fields from the SELECT statement SELECT list that will
         generate a dynamic, unique row identifier.
     shape_type {String}:
         Specifies the shape type of the query layer. Only those records from
         the result set of the query that match the specified shape type will
         be used in the output query layer. Tool validation will attempt to set
         this property based on the first record in the result set. This can be
         changed before executing the tool if it is not the desired output
         shape type. This parameter is ignored if the result set of the query
         does not return a geometry field.

         * POINT-The output query layer will use point geometry.

         * MULTIPOINT-The output query layer will use multipoint geometry.

         * POLYGON-The output query layer will use polygon geometry.

         * POLYLINE-The output query layer will use polyline geometry.
     srid {String}:
         The spatial reference identifier (SRID) value for queries that return
         geometry. Only those records from the result set of the query that
         match the specified SRID value will be used in the output query layer.
         Tool validation will attempt to set this property based on the first
         record in the result set. This can be changed before executing the
         tool if it is not the desired output SRID value. This parameter is
         ignored if the result set of the query does not return a geometry
         field.
     spatial_reference {Spatial Reference}:
         The coordinate system that will be used by the output query layer.
         Tool validation will attempt to set this property based on the first
         record in the result set. This can be changed before executing the
         tool if it is not the desired output coordinate system. This parameter
         is ignored if the result set of the query does not return a geometry
         field.
     spatial_properties {Boolean}:
         Specifies how the spatial properties for the layer will be
         defined.During the validation process, dimensionality, geometry type,
         spatial
         reference, SRID, and unique identifier properties will be set on the
         query layer. These values are based on the first row returned in the
         query. To manually define these properties instead of the tool
         querying the table to get them, use the default value for this
         parameter.

         * DEFINE_SPATIAL_PROPERTIES-Manually define the spatial properties of
         the layer. This is the default.

         * DO_NOT_DEFINE_SPATIAL_PROPERTIES-Layer properties will be determined
         based on the first row returned in the query.
     m_values {Boolean}:
         Specifies whether the layer will have m-values.

         * INCLUDE_M_VALUES-The layer will have m-values.

         * DO_NOT_INCLUDE_M_VALUES-The layer will not have m-values. This is
         the default.
     z_values {Boolean}:
         Specifies whether the layer will have z-values.

         * INCLUDE_Z_VALUES-The layer will have z-values.

         * DO_NOT_INCLUDE_Z_VALUES-The layer will not have z-values. This is
         the default.
     extent {Extent}:
         The extent of the layer. This parameter is only used if Define spatial
         properties for the layer is checked (spatial_properties =
         DEFINE_SPATIAL_PROPERTIES in Python). The extent must include all
         features in the table.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max."""
    ...

@gptooldoc("MakeQueryTable_management", None)
def MakeQueryTable(
    in_table=...,
    out_table=...,
    in_key_field_option=...,
    in_key_field=...,
    in_field=...,
    where_clause=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeQueryTable_management(in_table;in_table..., out_table, in_key_field_option, {in_key_field;in_key_field...}, {in_field;in_field...}, {where_clause})

       Applies an SQL query to a database, and the results are represented in
       either a layer or table view. The query can be used to join several
       tables or return a subset of fields or rows from the original data in
       the database.

    INPUTS:
     in_table (Table View / Raster Layer):
         The name of the table or tables to be used in the query. If several
         tables are listed, the where_clause parameter can be used to define
         how they will be joined.The input table can be from a geodatabase or a
         database connection.
     in_key_field_option (String):
         Specifies how an Object ID field will be generated (if at all) for the
         query. Layers and table views in ArcGIS require an Object ID field. An
         Object ID field is an integer field that uniquely identifies rows in
         the data being used.

         * USE_KEY_FIELDS-Specified fields in the in_key_field parameter will
         be used to uniquely identify a row in the output table. This can be a
         single field or multiple fields, which, when combined, uniquely
         identify a row in the output table. If no fields are specified in the
         key fields list, the ADD VIRTUAL_KEY_FIELD option will be applied.

         * ADD_VIRTUAL_KEY_FIELD-If no key fields have been specified, an
         Object ID field that uniquely identifies each row in the output table
         will be generated.

         * NO_KEY_FIELD-No Object ID field will be generated.
         Selections will not be supported for the table view. If there
         is an existing Object ID field, it will be used even if this
         option is chosen.
     in_key_field {Field}:
         A field or combination of fields that will be used to uniquely
         identify a row in the query. This parameter is used only when the
         in_key_field_option parameter is set to USE_KEY_FIELDS.
     in_field {Value Table}:
         The fields that will be included in the layer or table view. If an
         alias is set for a field, this is the name that appears. If no fields
         are specified, all fields from all tables are included. If a Shape
         field is added to the field list, the result is a layer; otherwise it
         is a table view.
     where_clause {SQL Expression}:
         An SQL expression used to select a subset of records.

    OUTPUTS:
     out_table (Table View / Raster Layer):
         The name of the layer or table view that will be created."""
    ...

@gptooldoc("MakeRasterLayer_management", None)
def MakeRasterLayer(
    in_raster=..., out_rasterlayer=..., where_clause=..., envelope=..., band_index=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeRasterLayer_management(in_raster, out_rasterlayer, {where_clause}, {envelope}, {band_index;band_index...})

       Creates a raster layer from an input raster dataset or layer file. The
       layer created by the tool is temporary and will not persist after the
       session ends unless the layer is saved to disk or the map document is
       saved.

    INPUTS:
     in_raster (Composite Geodataset):
         The path and name of the input raster dataset.You can use a raster
         layer from a GeoPackage as the input. To
         reference a raster within a GeoPackage, type the name of the path,
         followed by the name of the GeoPackage and the name of the raster. For
         example, c:\\data\\sample.gpkg\\raster_tile would be your input raster,
         where sample.gpkg is the name of the GeoPackage and raster_tile is the
         raster dataset within the package.
     where_clause {SQL Expression}:
         Define a query using SQL.
     envelope {Extent}:
         The output extent can be specified by defining the four coordinates or
         by using the extent of an existing layer.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     band_index {Value Table}:
         The bands that will be exported for the layer. If no bands are
         specified, all the bands will be used in the output.

    OUTPUTS:
     out_rasterlayer (Raster Layer):
         The name of the layer to create."""
    ...

@gptooldoc("MakeSceneLayer_management", None)
def MakeSceneLayer(
    in_dataset=..., out_layer=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeSceneLayer_management(in_dataset, out_layer)

       Creates a scene layer from a scene layer package (.slpk) or scene
       service.

    INPUTS:
     in_dataset (Scene Layer / Building Scene Layer / File):
         The input scene layer package (.slpk) or scene service from which the
         new scene layer will be created.

    OUTPUTS:
     out_layer (Scene Layer):
         The name of the scene layer to be created."""
    ...

@gptooldoc("MakeTableView_management", None)
def MakeTableView(
    in_table=..., out_view=..., where_clause=..., workspace=..., field_info=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeTableView_management(in_table, out_view, {where_clause}, {workspace}, {field_info})

       Creates a table view from an input table or feature class. The table
       view that is created by the tool is temporary and will not persist
       after the session ends unless the document is saved.

    INPUTS:
     in_table (Table View / Raster Layer):
         The input table or feature class.
     where_clause {SQL Expression}:
         An SQL expression used to select a subset of features. For more
         information on SQL syntax see the help topic SQL reference for query
         expressions used in ArcGIS.
     workspace {Workspace}:
         The input workspace used to validate the field names. If the input is
         a geodatabase table and the output workspace is a dBASE table, the
         field names may be truncated, since dBASE fields can only have names
         of ten characters or less.
     field_info {Field Info}:
         Specifies which fields from the input table to make visible in the
         output table view.

    OUTPUTS:
     out_view (Table View / Raster Layer):
         The name of the table view to be created."""
    ...

@gptooldoc("MakeTinLayer_management", None)
def MakeTinLayer(
    in_tin=..., out_layer=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeTinLayer_management(in_tin, out_layer)

       Creates a triangulated irregular network (TIN) layer from an input TIN
       dataset or layer file. The layer that is created by the tool is
       temporary and will not persist after the session ends unless the layer
       is saved to disk or the map document is saved.

    INPUTS:
     in_tin (TIN Layer):
         The input TIN dataset or layer from which the new layer will be
         created.

    OUTPUTS:
     out_layer (TIN Layer):
         The name of the TIN layer to be created. The output layer can be used
         as an input to any geoprocessing tool that accepts a TIN layer as
         input."""
    ...

@gptooldoc("MakeTrajectoryLayer_management", None)
def MakeTrajectoryLayer(
    in_trajectory_file=...,
    out_trajectory_layer=...,
    dimension=...,
    predefined_variables=...,
    variables=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeTrajectoryLayer_management(in_trajectory_file, out_trajectory_layer, {dimension}, {predefined_variables;predefined_variables...}, {variables;variables...})

       Generates a feature layer from selected variables in a trajectory
       file.

    INPUTS:
     in_trajectory_file (File / Trajectory Layer):
         The input trajectory file.
     dimension {String}:
         The dimension name. The first dimension is used by default.
     predefined_variables {String}:
         The predefined variables available for different sensor types.

         * SIGMA0-A variable that contains Surface Backscatter Coefficient.

         * SSH-A variable that contains Sea Surface Height.

         * SSHA-A variable that contains Sea Surface Height Anomaly

         * SWH-A variable that contains Significant Wave Height

         * SIGMA0_OCEAN-A variable that contains Ocean Surface Backscatter
         Coefficient

         * H_SEA_ICE-A variable that contains Sea Ice Surface Elevation

         * H_SEA_ICE_ANOMALY-A variable that contains Sea Ice Surface Height
         Anomaly

         * SIC-A variable that contains Sea Ice Concentration

         * SIGMA0_SEA_ICE-A variable that contains Sea Ice Surface Backscatter
         Coefficient

         * H_ICE_SHEET-A variable that contains Ice Sheet Surface Elevation

         * SIGMA0_ICE_SHEET-A variable that contains Ice Sheet Surface
         Backscatter Coefficient

         * H_ICE-A variable that contains Ice Sheet Surface Elevation

         * SIGMA0_ICE-A variable that contains Ice Sheet Surface Backscatter
         Coefficient

         * WS-A variable that contains Wind Speed

         * H_ICEBERG-A variable that contains Iceberg Elevation

         * H_OCEAN-A variable that contains Ocean Surface Elevation

         * H_OCEAN_HIGHEST-A variable that contains Ocean Highest Elevation

         * H_OCEAN_LOWEST-A variable that contains Ocean Lowest Elevation

         * H_MSS-A variable that contains Mean Sea Surface Elevation

         * H_IWS-A variable that contains Inland Water Surface Height
     variables {String}:
         The variables that will be included in the output layer. All variables
         are selected by default.

    OUTPUTS:
     out_trajectory_layer (Trajectory Layer):
         The output feature layer that contains the selected variables."""
    ...

@gptooldoc("MakeWCSLayer_management", None)
def MakeWCSLayer(
    in_wcs_coverage=..., out_wcs_layer=..., template=..., band_index=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeWCSLayer_management(in_wcs_coverage, out_wcs_layer, {template}, {band_index;band_index...})

       Creates a temporary raster layer from a WCS service.

    INPUTS:
     in_wcs_coverage (WCS Coverage / String):
         The name of the input WCS service, or the URL that references the WCS
         service.If a WCS server URL is used, the URL should include the
         coverage name
         and version information. If only the URL is entered, the tool will
         automatically take the first coverage and use the default version
         (1.0.0) to create the WCS layer.An example of a URL that includes the
         coverage name and version is htt
         p://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?cover
         age=rasterDRGs&version=1.1.1.In this example,
         http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?
         is the URL. The coverage specified is coverage=rasterDRGs, and the
         version is &version=1.1.1.To get the coverage names on a WCS server,
         use the WCS GetCapabilities
         request. An example of WCS request is http://ServerName/arcgis/service
         s/serviceName/ImageServer/WCSServer?request=getcapabilities&service=wc
         s.
     template {Extent}:
         The output extent of the WCS layer.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     band_index {Value Table}:
         The bands that will be exported for the layer. If no bands are
         specified, all the bands will be used in the output.

    OUTPUTS:
     out_wcs_layer (Raster Layer):
         The name of the output WCS layer."""
    ...

@gptooldoc("MakeXYEventLayer_management", None)
def MakeXYEventLayer(
    table=...,
    in_x_field=...,
    in_y_field=...,
    out_layer=...,
    spatial_reference=...,
    in_z_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MakeXYEventLayer_management(table, in_x_field, in_y_field, out_layer, {spatial_reference}, {in_z_field})

       Creates a new point feature layer based on x- and y-coordinates
       defined in a table. If the source table contains z-coordinates
       (elevation values), that field can also be specified in the creation
       of the event layer. The layer created by this tool is temporary.

    INPUTS:
     table (Table View):
         The table containing the x- and y-coordinates that define the
         locations of the point features that will be created.
     in_x_field (Field):
         The field in the input table that contains the x-coordinates (or
         longitude).
     in_y_field (Field):
         The field in the input table that contains the y-coordinates (or
         latitude).
     spatial_reference {Spatial Reference}:
         The spatial reference of the coordinates specified in the in_x_field
         and in_y_field parameters. This will be the output event layer's
         spatial reference.
     in_z_field {Field}:
         The field in the input table that contains the z-coordinates.

    OUTPUTS:
     out_layer (Feature Layer):
         The name of the output point event layer."""
    ...

@gptooldoc("MatchLayerSymbologyToAStyle_management", None)
def MatchLayerSymbologyToAStyle(
    in_layer=..., match_values=..., in_style=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MatchLayerSymbologyToAStyle_management(in_layer, match_values, in_style)

       Creates unique value symbology for the input layer based on the input
       field or expression by matching input field or expression strings to
       symbol names from the input style.

    INPUTS:
     in_layer (Feature Layer):
         The input layer or layer file to which matched symbols are applied as
         unique values symbol classes. The input layer can contain point, line,
         polygon, multipoint, or multipatch symbology. Existing symbology on
         the layer is overwritten.
     match_values (Calculator Expression):
         The field or expression on which the input layer is symbolized. The
         field values or resultant expression values are matched to symbol
         names in the specified style to assign symbols to the resulting symbol
         classes.
     in_style (String):
         The style containing symbols with names matching the field or
         expression values."""
    ...

@gptooldoc("SaveToLayerFile_management", None)
def SaveToLayerFile(
    in_layer=..., out_layer=..., is_relative_path=..., version=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SaveToLayerFile_management(in_layer, out_layer, {is_relative_path}, {version})

       Creates an output layer file (.lyrx) from a map layer. The layer file
       stores many properties of the input layer such as symbology, labeling,
       and custom pop-ups.

    INPUTS:
     in_layer (Layer / Table View):
         The map layer to be saved to disk as a layer file.
     is_relative_path {Boolean}:
         Specifies whether the output layer file will store a relative path to
         the source data stored on disk or an absolute path.

         * ABSOLUTE-The output layer file will store an absolute path to the
         source data stored on disk. This is the default.

         * RELATIVE-The output layer file will store a relative path to the
         source data stored on disk. If the output layer file is moved, its
         source path will update to where the source data should be in relation
         to the new path.
     version {String}:
         Specifies the version of the output layer file.

         * CURRENT-The current version. This is the default.

    OUTPUTS:
     out_layer (Layer File):
         The output layer file (.lyrx) to be created."""
    ...

@gptooldoc("SelectLayerByAttribute_management", None)
def SelectLayerByAttribute(
    in_layer_or_view=..., selection_type=..., where_clause=..., invert_where_clause=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SelectLayerByAttribute_management(in_layer_or_view, {selection_type}, {where_clause}, {invert_where_clause})

       Adds, updates, or removes a selection based on an attribute query.

    INPUTS:
     in_layer_or_view (Table View / Raster Layer / Mosaic Layer):
         The data to which the selection will be applied.
     selection_type {String}:
         Specifies how the selection will be applied and what to do if a
         selection already exists.

         * NEW_SELECTION-The resulting selection will replace the current
         selection. This is the default.

         * ADD_TO_SELECTION-The resulting selection will be added to the
         current selection if one exists. If no selection exists, this is the
         same as the new selection option.

         * REMOVE_FROM_SELECTION-The resulting selection will be removed from
         the current selection. If no selection exists, this option has no
         effect.

         * SUBSET_SELECTION-The resulting selection will be combined with the
         current selection. Only records that are common to both remain
         selected.

         * SWITCH_SELECTION-The selection will be switched. All records that
         were selected will be removed from the current selection, and all
         records that were not selected will be added to the current selection.
         The where_clause parameter is ignored when this option is specified.

         * CLEAR_SELECTION-The selection will be cleared or removed. The
         where_clause parameter is ignored when this option is specified.
     where_clause {SQL Expression}:
         An SQL expression used to select a subset of records.
     invert_where_clause {Boolean}:
         Specifies whether the expression will be used as is, or the opposite
         of the expression will be used.

         * NON_INVERT-The query will be used as is. This is the default.

         * INVERT-The opposite of the query will be used. If the selection_type
         parameter is used, the reversal of the selection occurs before it is
         combined with existing selections."""
    ...

@gptooldoc("SelectLayerByLocation_management", None)
def SelectLayerByLocation(
    in_layer=...,
    overlap_type=...,
    select_features=...,
    search_distance=...,
    selection_type=...,
    invert_spatial_relationship=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SelectLayerByLocation_management(in_layer;in_layer..., {overlap_type}, {select_features}, {search_distance}, {selection_type}, {invert_spatial_relationship})

       Selects features based on a spatial relationship to features in
       another dataset or the same dataset.

    INPUTS:
     in_layer (Feature Layer / Raster Layer / Mosaic Layer):
         The features that will be evaluated using the select_features
         parameter values. The selection will be applied to these features.
     overlap_type {String}:
         Specifies the spatial relationship to be evaluated.

         * INTERSECT-The features in the input layer will be selected if they
         intersect a selecting feature. This is the default.

         * INTERSECT_3D-The features in the input layer will be selected if
         they intersect a selecting feature in three-dimensional space (x, y,
         and z).

         * INTERSECT_DBMS-The features in the input layer will be selected if
         they intersect a selecting feature.This option applies to enterprise
         geodatabases only. The selection will be processed in the enterprise
         geodatabase DBMS rather than on the client when all requirements are
         met (see usage notes).This option may provide better performance than
         performing the selection on the client.

         * WITHIN_A_DISTANCE-The features in the input layer will be selected
         if they are within the specified distance (using Euclidean distance)
         of a selecting feature. Use the search_distance parameter to specify
         the distance.

         * WITHIN_A_DISTANCE_3D-The features in the input layer will be
         selected if they are within a specified distance of a selecting
         feature in three-dimensional space. Use the search_distance parameter
         to specify the distance.

         * WITHIN_A_DISTANCE_GEODESIC-The features in the input layer will be
         selected if they are within a specified distance of a selecting
         feature. Distance between features will be calculated using a geodesic
         formula that takes into account the curvature of the spheroid and
         correctly handles data near and across the dateline and poles. Use the
         search_distance parameter to specify the distance.

         * CONTAINS-The features in the input layer will be selected if they
         contain a selecting feature.

         * COMPLETELY_CONTAINS-The features in the input layer will be selected
         if they completely contain a selecting feature.

         * CONTAINS_CLEMENTINI-This spatial relationship yields the same
         results as the CONTAINS option with the exception that if the
         selecting feature is entirely on the boundary of the input feature (no
         part is properly inside or outside), the feature will not be
         selected.Clementini defines the boundary polygon as the line
         separating inside and outside, the boundary of a line is defined as
         its end points, and the boundary of a point is always empty.

         * WITHIN-The features in the input layer will be selected if they are
         within a selecting feature.

         * COMPLETELY_WITHIN-The features in the input layer will be selected
         if they are completely within or contained by a selecting feature.

         * WITHIN_CLEMENTINI-The result will be identical to the WITHIN option
         result with the exception that if the entirety of the feature in the
         input layer is on the boundary of the feature in the selecting layer,
         the feature will not be selected.Clementini defines the boundary
         polygon as the line separating inside and outside, the boundary of a
         line is defined as its end points, and the boundary of a point is
         always empty.

         * ARE_IDENTICAL_TO-The features in the input layer will be selected if
         they are identical (in geometry) to a selecting feature.

         * BOUNDARY_TOUCHES-The features in the input layer will be selected if
         they have a boundary that touches a selecting feature. When the input
         features are lines or polygons, the boundary of the input feature can
         only touch the boundary of the selecting feature, and no part of the
         input feature can cross the boundary of the selecting feature.

         * SHARE_A_LINE_SEGMENT_WITH-The features in the input layer will be
         selected if they share a line segment with a selecting feature. The
         input and selecting features must be line or polygon.

         * CROSSED_BY_THE_OUTLINE_OF-The features in the input layer will be
         selected if they are crossed by the outline of a selecting feature.
         The input and selecting features must be lines or polygons. If
         polygons are used for the input or selecting layer, the polygon's
         boundary (line) will be used. Lines that cross at a point will be
         selected; lines that share a line segment will not be selected.

         * HAVE_THEIR_CENTER_IN-The features in the input layer will be
         selected if their center falls within a selecting feature. The center
         of the feature is calculated as follows: for polygon and multipoint,
         the geometry's centroid is used; for line input, the geometry's
         midpoint is used.
     select_features {Feature Layer}:
         The features in the Input Features parameter will be selected based on
         their relationship to the features from this layer or feature class.
     search_distance {Linear Unit}:
         The distance that will be searched. This parameter is only valid if
         the overlap_type parameter is set to WITHIN_A_DISTANCE,
         WITHIN_A_DISTANCE_GEODESIC, WITHIN_A_DISTANCE_3D, INTERSECT,
         INTERSECT_3D, HAVE_THEIR_CENTER_IN, or CONTAINS.If the
         WITHIN_A_DISTANCE_GEODESIC option is selected, use a linear
         unit such as kilometers or miles.
     selection_type {String}:
         Specifies how the selection will be applied to the input and how it
         will be combined with an existing selection. This tool does not
         include an option to clear an existing selection; use the Select Layer
         By Attribute tool with the selection_type parameter set to
         CLEAR_SELECTION to do that.

         * NEW_SELECTION-The resulting selection will replace any existing
         selection. This is the default.

         * ADD_TO_SELECTION-The resulting selection will be added to an
         existing selection. If no selection exists, this is the same as the
         NEW_SELECTION option.

         * REMOVE_FROM_SELECTION-The resulting selection will be removed from
         an existing selection. If no selection exists, the operation will have
         no effect.

         * SUBSET_SELECTION-The resulting selection will be combined with the
         existing selection. Only records that are common to both will remain
         selected.

         * SWITCH_SELECTION-The selection will be switched. All records that
         were selected will be removed from the selection, and all records that
         were not selected will be added to the selection.The select_features
         and overlap_type parameters are ignored when this option is selected.
     invert_spatial_relationship {Boolean}:
         Specifies whether the spatial relationship evaluation result or the
         opposite result will be used. For example, this parameter can be used
         to get a list of features that do not intersect or are not within a
         given distance of features in another dataset.

         * NOT_INVERT-The query result will be used. This is the default.

         * INVERT-The opposite of the query result will be used. If the
         selection_type parameter is set, the reversal of the selection will
         occur before it is combined with existing selections."""
    ...

@gptooldoc("ConsolidateLayer_management", None)
def ConsolidateLayer(
    in_layer=...,
    output_folder=...,
    convert_data=...,
    convert_arcsde_data=...,
    extent=...,
    apply_extent_to_arcsde=...,
    schema_only=...,
    select_related_rows=...,
    preserve_sqlite=...,
    exclude_network_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConsolidateLayer_management(in_layer;in_layer..., output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {select_related_rows}, {preserve_sqlite}, {exclude_network_dataset})

       Consolidates one or more layers by copying all referenced data sources
       into a single folder.

    INPUTS:
     in_layer (Layer):
         The input layers that will be consolidated.
     convert_data {Boolean}:
         Specifies whether input layers will be converted to a file geodatabase
         or preserved in their original format.

         * CONVERT-Data will be converted to a file geodatabase. This option
         does not apply to enterprise geodatabase data sources. To convert
         enterprise geodatabase data, set the convert_arcsde_data parameter to
         CONVERT_ARCSDE.

         * PRESERVE-Data formats will be preserved when possible. This is the
         default.
     convert_arcsde_data {Boolean}:
         Specifies whether input enterprise geodatabase layers will be
         converted to a file geodatabase or preserved in their original format.

         * CONVERT_ARCSDE-Enterprise geodatabase data will be converted to a
         file geodatabase and will be included in the consolidated folder or
         package. This is the default.

         * PRESERVE_ARCSDE-Enterprise geodatabase data will be preserved and
         will be referenced in the consolidated folder or package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ARCSDE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     schema_only {Boolean}:
         Specifies whether only the schema of the input layers will be
         consolidated or packaged.

         * ALL-All features and records will be consolidated or packaged. This
         is the default.

         * SCHEMA_ONLY-Only the schema of the input layers will be
         consolidated or packaged.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     preserve_sqlite {Boolean}:
         Specifies whether mobile geodatabase data will be preserved in the
         output or written to file geodatabase format. If the input data is a
         mobile geodatabase network dataset, the output will be a mobile
         geodatabase.This parameter overrides the convert_data parameter when
         the input
         data is a mobile geodatabase.

         * CONVERT_SQLITE-Mobile geodatabase data will be converted to file
         geodatabase format. This is the default.

         * PRESERVE_SQLITE-Mobile geodatabase data will be preserved in the
         output. The geodatabase will be included in its entirety.
     exclude_network_dataset {Boolean}:
         For network analysis layers, specifies whether the network dataset
         will also be consolidated.

         * INCLUDE_NETWORK_DATASET-The network dataset will be included and
         consolidated. This is the default.

         * EXCLUDE_NETWORK_DATASET-The network dataset will not be included.
         Only the network analysis layers will be consolidated.

    OUTPUTS:
     output_folder (Folder):
         The output folder that will contain the layer files and consolidated
         data.If the specified folder does not exist, a new folder will be
         created."""
    ...

@gptooldoc("ConsolidateLocator_management", None)
def ConsolidateLocator(
    in_locator=..., output_folder=..., copy_arcsde_locator=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConsolidateLocator_management(in_locator, output_folder, {copy_arcsde_locator})

       Consolidate a locator or composite locator by copying all locators
       into a single folder.

    INPUTS:
     in_locator (Address Locator):
         The input locator or composite locator that will be consolidated.
     copy_arcsde_locator {Boolean}:
         This parameter has no effect in ArcGIS Pro. It remains only to support
         backward compatibility.

    OUTPUTS:
     output_folder (Folder):
         The output folder that will contain the consolidated locator or
         composite locator with its participating locators.If the specified
         folder does not exist, a new folder will be created."""
    ...

@gptooldoc("ConsolidateMap_management", None)
def ConsolidateMap(
    in_map=...,
    output_folder=...,
    convert_data=...,
    convert_arcsde_data=...,
    extent=...,
    apply_extent_to_arcsde=...,
    preserve_sqlite=...,
    select_related_rows=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConsolidateMap_management(in_map;in_map..., output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {preserve_sqlite}, {select_related_rows})

       Consolidates a map and all referenced data sources to a specified
       output folder.

    INPUTS:
     in_map (Map):
         The map (.mapx) to be consolidated. When running this tool within the
         ArcGIS Pro application, the input can be a map, scene, or basemap.
     convert_data {Boolean}:
         Specifies whether input layers will be converted to a file geodatabase
         or preserved in their original format.

         * CONVERT-Data will be converted to a file geodatabase. This option
         does not apply to enterprise geodatabase data sources. To convert
         enterprise geodatabase data, set the convert_arcsde_data parameter to
         CONVERT_ARCSDE.

         * PRESERVE-Data formats will be preserved when possible. This is the
         default.
     convert_arcsde_data {Boolean}:
         Specifies whether input enterprise geodatabase layers will be
         converted to a file geodatabase or preserved in their original format.

         * CONVERT_ARCSDE-Enterprise geodatabase data will be converted to a
         file geodatabase and will be included in the consolidated folder or
         package. This is the default.

         * PRESERVE_ARCSDE-Enterprise geodatabase data will be preserved and
         will be referenced in the consolidated folder or package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ARCSDE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     preserve_sqlite {Boolean}:
         Specifies whether input mobile geodatabase data will be preserved as
         mobile geodatabase in the output. If the input data is a mobile
         geodatabase network dataset, the output will always be mobile
         geodatabase.

         * PRESERVE_SQLITE-Mobile geodatabase data will be preserved as SQLite
         in the consolidated folder.

         * CONVERT_SQLITE-Mobile geodatabase data will be converted to file
         geodatabase. This is the default.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.

    OUTPUTS:
     output_folder (Folder):
         The output folder that will contain the consolidated map and data.If
         the specified folder does not exist, a new folder will be created."""
    ...

@gptooldoc("ConsolidateProject_management", None)
def ConsolidateProject(
    in_project=...,
    output_folder=...,
    sharing_internal=...,
    extent=...,
    apply_extent_to_enterprise_geo=...,
    package_as_template=...,
    preserve_sqlite=...,
    version=...,
    select_related_rows=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConsolidateProject_management(in_project, output_folder, {sharing_internal}, {extent}, {apply_extent_to_enterprise_geo}, {package_as_template}, {preserve_sqlite}, {version}, {select_related_rows})

       Consolidates a project (an .aprx file) and referenced maps and data
       into a specified output folder.

    INPUTS:
     in_project (File):
         The project (.aprx file) to be consolidated.
     sharing_internal {Boolean}:
         Specifies whether the project and all data will be consolidated into
         the output folder so it can be shared externally.

         * INTERNAL-The project and its data sources will not be
         consolidated into the output folder. This is the default.
         This parameter applies to enterprise geodatabase data sources,
         including enterprise geodatabases and folders referenced through a UNC
         path.

         * EXTERNAL-The project and its data sources will be consolidated
         (copied) into the output folder when possible.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_enterprise_geo {Boolean}:
         Specifies whether the extent parameter will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The extent will be applied to all layers. This is the default.

         * ENTERPRISE_ONLY-The extent will be applied to enterprise
         geodatabase layers only.
     package_as_template {Boolean}:
         Specifies whether the project will be consolidated as a template or a
         regular project. Templates can include maps, layouts, connections to
         databases and servers, and so on. A project template allows you to
         standardize a series of maps for use in a project and ensure that the
         correct layers are immediately available for use.

         * PROJECT_PACKAGE-The project will be consolidated as a project into
         a folder. This is the default.

         * PROJECT_TEMPLATE-The project will be consolidated as a template
         into a folder
     preserve_sqlite {Boolean}:
         Specifies whether mobile geodatabases will be preserved or
         converted to file geodatabases. This parameter applies only to
         mobile geodatabases (.geodatabase),
         used primarily for offline workflows in ArcGIS Runtime apps. SQLite
         databases with .sqlite or .gpkg file extensions will be converted to
         file geodatabases.

         * CONVERT_SQLITE-Mobile geodatabases will be converted to file
         geodatabases. This is the default.

         * PRESERVE_SQLITE-Mobile geodatabases will be preserved.
     version {String}:
         Specifies the ArcGIS Pro version that the consolidated project will be
         saved as. Saving to an earlier version will ensure tool backward
         compatibility. If you attempt to consolidate a toolbox to an earlier
         version and capabilities that are only available in the newer version
         are included, an error will occur. You must remove tools that are
         incompatible with the earlier version, or specify a compatible
         version.

         * CURRENT-The consolidated folder will contain geodatabases and maps
         compatible with the version of the current release.

         * 2.2-The consolidated folder will contain geodatabases and maps
         compatible with version 2.2.

         * 2.3-The consolidated folder will contain geodatabases and maps
         compatible with version 2.3.

         * 2.4-The consolidated folder will contain geodatabases and maps
         compatible with version 2.4.

         * 2.5-The consolidated folder will contain geodatabases and maps
         compatible with version 2.5.

         * 2.6-The consolidated folder will contain geodatabases and maps
         compatible with version 2.6.

         * 2.7-The consolidated folder will contain geodatabases and maps
         compatible with version 2.7.

         * 2.8-The consolidated folder will contain geodatabases and maps
         compatible with version 2.8.

         * 2.9-The consolidated folder will contain geodatabases and maps
         compatible with version 2.9.

         * 3.0-The consolidated folder will contain geodatabases and maps
         compatible with version 3.0.

         * 3.1-The consolidated folder will contain geodatabases and maps
         compatible with version 3.1.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.

    OUTPUTS:
     output_folder (Folder):
         The output folder that will contain the consolidated project and data.
         If the specified folder does not exist, a folder will be created."""
    ...

@gptooldoc("ConsolidateToolbox_management", None)
def ConsolidateToolbox(
    in_toolbox=..., output_folder=..., version=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConsolidateToolbox_management(in_toolbox;in_toolbox..., output_folder, {version})

       Consolidates one or more toolboxes into a specified output folder.

    INPUTS:
     in_toolbox (Toolbox):
         The toolboxes to be consolidated.
     version {String}:
         Specifies the version of the consolidated toolbox. Specifying a
         version allows toolboxes to be shared with previous versions of ArcGIS
         and supports backward compatibility.

         * CURRENT-The consolidated folder will contain tools compatible with
         the version of the current release. This is the default.

         * 2.2-The consolidated folder will contain tools compatible with
         version 2.2.

         * 2.3-The consolidated folder will contain tools compatible with
         version 2.3.

         * 2.4-The consolidated folder will contain tools compatible with
         version 2.4.

         * 2.5-The consolidated folder will contain tools compatible with
         version 2.5.

         * 2.6-The consolidated folder will contain tools compatible with
         version 2.6.

         * 2.7-The consolidated folder will contain tools compatible with
         version 2.7.

         * 2.8-The consolidated folder will contain tools compatible with
         version 2.8.

         * 2.9-The consolidated folder will contain tools compatible with
         version 2.9.

         * 3.0-The consolidated folder will contain tools compatible with
         version 3.0.

         * 3.1-The consolidated folder will contain tools compatible with
         version 3.1.

    OUTPUTS:
     output_folder (Folder):
         The output folder that will contain the consolidated toolbox.If the
         specified folder does not exist, a folder will be created."""
    ...

@gptooldoc("CreateMapTilePackage_management", None)
def CreateMapTilePackage(
    in_map=...,
    service_type=...,
    output_file=...,
    format_type=...,
    level_of_detail=...,
    service_file=...,
    summary=...,
    tags=...,
    extent=...,
    compression_quality=...,
    package_type=...,
    min_level_of_detail=...,
    area_of_interest=...,
    create_multiple_packages=...,
    output_folder=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateMapTilePackage_management(in_map, service_type, {output_file}, format_type, level_of_detail, {service_file}, {summary}, {tags}, {extent}, {compression_quality}, {package_type}, {min_level_of_detail}, {area_of_interest}, {create_multiple_packages}, {output_folder})

       Generates tiles from a map and packages them as a single tile package
       or multiple smaller tile packages.

    INPUTS:
     in_map (Map):
         The map from which tiles will be generated and packaged.
     service_type (Boolean):
         Specifies whether the tiling scheme will be generated from an existing
         map service or whether map tiles will be generated for ArcGIS Online,
         Bing Maps, and Google Maps.

         * EXISTING-A tiling scheme from an existing map service will be used.
         You must specify a map service in the service_file parameter.Choose
         this option if your organization has created a tiling scheme for an
         existing service on the server and you want to match it. Matching
         tiling schemes ensures that the tiles will overlay correctly in your
         ArcGIS Runtime application.If you choose this option, use the same
         coordinate system for the source map as the map with the tiling scheme
         you're importing.

         * ONLINE-The ArcGIS Online/Bing Maps/Google Maps tiling scheme will
         be used. This is the default.The ArcGIS Online/Bing Maps/Google Maps
         tiling scheme allows you to overlay cache tiles with tiles from these
         online mapping services. ArcGIS Desktop includes this tiling scheme as
         a built-in option when loading a tiling scheme. When you choose this
         tiling scheme, the source map must use the WGS 1984 Web Mercator
         (Auxiliary Sphere) projected coordinate system.The ArcGIS Online/Bing
         Maps/Google Maps tiling scheme is required if you'll be overlaying the
         package with ArcGIS Online, Bing Maps, or Google Maps. One advantage
         of the ArcGIS Online/Bing Maps/Google Maps tiling scheme is that it is
         widely known in the web mapping world, so the tiles will match those
         of other organizations that have used this tiling scheme. Even if you
         don't plan to overlay any of these well-known map services, you may
         choose the tiling scheme for its interoperability potential.The ArcGIS
         Online/Bing Maps/Google Maps tiling scheme may contain scales that
         will be zoomed in too far to be of use in your map. Packaging for
         large scales can take up time and disk storage space. For example, the
         largest scale in the tiling scheme is about 1:1,000. Packaging the
         entire continental United States at this scale can take weeks and
         require hundreds of gigabytes of storage. If you aren't prepared to
         package at this scale level, remove this scale level when you create
         the tile package.
     format_type (String):
         Specifies the format that will be used for the generated tiles.

         * PNG-The correct format (PNG 8, PNG 24, or PNG 32) will be used based
         on the specified Maximum Level Of Detail parameter value. This is the
         default.

         * PNG8-PNG8 format will be used. Use this format for overlay services
         that need to have a transparent background, such as roads and
         boundaries. PNG8 creates tiles of very small size on disk with no loss
         of information. Do not use PNG8 if the map contains more than 256
         colors. Imagery, hillshades, gradient fills, transparency, and
         antialiasing can use more than 256 colors in a map. Even symbols such
         as highway shields may have subtle antialiasing around the edges that
         unexpectedly adds colors to a map.

         * PNG24-PNG24 format will be used. Use this format for overlay
         services, such as roads and boundaries, that have more than 256 colors
         (if fewer than 256 colors, use PNG8).

         * PNG32-PNG32 format will be used. Use this format for overlay
         services, such as roads and boundaries, that have more than 256
         colors. PNG32 works well for overlay services that have antialiasing
         enabled on lines or text. PNG32 creates larger tiles on disk than
         PNG24.

         * JPEG-JPEG format will be used. Use this format for basemap services
         that have large color variation and do not need a transparent
         background. For example, raster imagery and detailed vector basemaps
         work well with JPEG. JPEG is a lossy image format. It attempts to
         selectively remove data without affecting the appearance of the image.
         This can cause very small tile sizes on disk, but if a map contains
         vector line work or labels, it may produce too much noise or blurry
         areas around the lines. If this is the case, you can raise the
         compression value from the default of 75. A higher value, such as 90,
         may balance an acceptable quality of line work with the small tile
         size benefit of the JPEG.If you are willing to accept a minor amount
         of noise in the images, you may save large amounts of disk space with
         JPEG. The smaller tile size also means the application can download
         the tiles faster.

         * MIXED-JPEG format will be used in the center of the package and
         PNG32 will be used on the edge of the package. Use mixed mode when you
         want to cleanly overlay raster packages on other layers.When a mixed
         package is created, PNG32 tiles are created where transparency is
         detected (in other words, where the map background is visible). The
         rest of the tiles are built using JPEG. This keeps the average file
         size down while providing a clean overlay on top of other packages. If
         you do not use the mixed mode package in this scenario, a
         nontransparent collar around the periphery of the image where it
         overlaps the other package will be visible.
     level_of_detail (Long):
         The integer representation corresponding to the number of scales used
         to define a cache tiling scheme. This scale value defines the maximum
         level up to which the cache tiles will be generated in the tile
         package. Larger values reflect larger scales that show more detail but
         require more storage space. Smaller values reflect smaller scales that
         show less detail and require less storage space. Possible values are
         from 1 to 23. The default value is 1. The maximum level of detail
         value must be greater than the minimum level of detail value.
     service_file {Map Server / File}:
         The name of the map service or the .xml files that will be used for
         the tiling scheme. This parameter is required only when the
         service_type parameter is set to EXISTING.
     summary {String}:
         The summary information that will be added to the properties of the
         package.
     tags {String}:
         The tag information that will be added to the properties of the
         package. Multiple tags can be added, separated by a comma or
         semicolon.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     compression_quality {Long}:
         A value between 1 and 100 for the JPEG compression quality. The
         default value is 75 for JPEG tile format and zero for other
         formats.Compression is supported only for JPEG and mixed formats.
         Choosing a
         higher value will result in a larger file size with a higher-quality
         image. Choosing a lower value will result in a smaller file size with
         a lower-quality image.
     package_type {String}:
         Specifies the type of tile package that will be created.

         * tpk-A .tpk file will be created. Tiles will be stored using Compact
         storage format. This format is supported across ArcGIS.

         * tpkx-A .tpkx file will be created. Tiles will be stored using
         CompactV2 storage format, which provides better performance on network
         shares and cloud store directories. This package structure type is
         supported by newer versions of ArcGIS products such as ArcGIS Online
         7.1, ArcGIS Enterprise 10.7, and ArcGIS Runtime 100.5. This is the
         default.
     min_level_of_detail {Long}:
         The integer representation corresponding to the number of scales used
         to define a cache tiling scheme. This scale value defines the level at
         which the cache tiles begin to be available and generated in the tile
         package. Possible values are from 0 to 23. The default value is 0. The
         minimum level of detail value must be less than or equal to the
         maximum level of detail value.
     area_of_interest {Feature Set}:
         A feature set that constrains where tiles will be created. Use an area
         of interest to create tiles for irregularly shaped areas or multipart
         features. The areas outside the bounding box of area of interest
         features will not be cached. If no value is provided for this
         parameter, the area of interest will be the full extent of the input
         map.
     create_multiple_packages {Boolean}:
         Specifies whether a single large tile package or multiple small tile
         packages will be generated. This parameter is not available when the
         parallelProcessingFactor environment variable is 0 or when the
         package_type parameter is set to tpk.

         * CREATE_MULTIPLE_PACKAGES-Multiple tile packages (each approximately
         1 GB in size) will be generated in the location defined in the
         output_folder parameter.

         * CREATE_SINGLE_PACKAGE-A single tile package will be generated in the
         location defined in the output_file parameter. This is the default.
     output_folder {Folder}:
         The output folder where the multiple tile packages will be generated.
         If the output folder is not empty, a subfolder will be with created in
         the output folder to store the tiles. An automatically generated GUID
         will be used as the folder name.

    OUTPUTS:
     output_file {File}:
         The output path and file name for the map tile package."""
    ...

@gptooldoc("CreateMobileMapPackage_management", None)
def CreateMobileMapPackage(
    in_map=...,
    output_file=...,
    in_locator=...,
    area_of_interest=...,
    extent=...,
    clip_features=...,
    title=...,
    summary=...,
    description=...,
    tags=...,
    credits=...,
    use_limitations=...,
    anonymous_use=...,
    enable_map_expiration=...,
    map_expiration_type=...,
    expiration_date=...,
    expiration_message=...,
    select_related_rows=...,
    reference_online_content=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateMobileMapPackage_management(in_map;in_map..., output_file, {in_locator;in_locator...}, {area_of_interest}, {extent}, {clip_features}, {title}, {summary}, {description}, {tags}, {credits}, {use_limitations}, {anonymous_use}, {enable_map_expiration}, {map_expiration_type}, {expiration_date}, {expiration_message}, {select_related_rows}, {reference_online_content})

       Packages maps and basemaps along with all referenced data sources into
       a single .mmpk file.

    INPUTS:
     in_map (Map):
         One or more maps or basemaps that will be packaged into a single .mmpk
         file.
     in_locator {Address Locator}:
         Locators have the following restrictions:
     area_of_interest {Feature Layer}:
         Polygon layer that defines the area of interest. Only those features
         that intersect the area_of_interest value will be included in the
         mobile map package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     clip_features {Boolean}:
         Specifies whether the output features will be clipped to the given
         area of interest or extent.

         * CLIP-The geometry of the features will be clipped to the given
         area_of_interest value or extent value.

         * SELECT-Features in the map will be selected and their geometry will
         remain unaltered. This is the default.
     title {String}:
         Adds title information to the properties of the package.
     summary {String}:
         Adds summary information to the properties of the package.
     description {String}:
         Adds description information to the properties of the package.
     tags {String}:
         Adds tag information to the properties of the package. Multiple tags
         can be added or separated by a comma or semicolon.
     credits {String}:
         Adds credit information to the properties of the package.
     use_limitations {String}:
         Adds use limitations to the properties of the package.
     anonymous_use {Boolean}:
         Specifies whether the mobile map can be used by anyone.

         * ANONYMOUS_USE-Anyone with access to the package can use the mobile
         map without signing in with an Esri Named User account.

         * STANDARD-Anyone with access to the package must be signed in with a
         Named User account to use the mobile map. This is the default.
         This optional parameter is only available with the Publisher
         extension.
     enable_map_expiration {Boolean}:
         Specifies whether a time-out will be enabled on the mobile map
         package.

         * ENABLE_MAP_EXPIRATION-Time-out will be enabled on the mobile map
         package.

         * DISABLE_MAP_EXPIRATION-Time-out will not be enabled on the mobile
         map package. This is the default.
         This optional parameter is only available with the Publisher
         extension.
     map_expiration_type {String}:
         Specifies the type of access a user will have to the expired mobile
         map package.

         * ALLOW_TO_OPEN-A user of the package will be warned that the map has
         expired, but will be allowed to open it. This is the default.

         * DONOT_ALLOW_TO_OPEN-A user of the package will be warned that the
         map has expired, and will not be allowed to open it.
         This optional parameter is only available with the Publisher
         extension.
     expiration_date {Date}:
         Specifies the date the mobile map package will expire.This optional
         parameter is only available with the Publisher
         extension.
     expiration_message {String}:
         A text message that will display when an expired map is accessed.This
         optional parameter is only available with the Publisher
         extension.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     reference_online_content {Boolean}:
         Specifies whether service layers will be referenced in the package.

         * INCLUDE_SERVICE_LAYERS-Service layers will be referenced in the
         mobile package.

         * EXCLUDE_SERVICE_LAYERS-Service layers will not be referenced in the
         mobile package. This is the default.

    OUTPUTS:
     output_file (File):
         The output mobile map package (.mmpk)."""
    ...

@gptooldoc("CreateMobileScenePackage_management", None)
def CreateMobileScenePackage(
    in_scene=...,
    output_file=...,
    in_locator=...,
    area_of_interest=...,
    extent=...,
    clip_features=...,
    title=...,
    summary=...,
    description=...,
    tags=...,
    credits=...,
    use_limitations=...,
    anonymous_use=...,
    texture_optimization=...,
    enable_scene_expiration=...,
    scene_expiration_type=...,
    expiration_date=...,
    expiration_message=...,
    select_related_rows=...,
    reference_online_content=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateMobileScenePackage_management(in_scene;in_scene..., output_file, {in_locator;in_locator...}, {area_of_interest}, {extent}, {clip_features}, {title}, {summary}, {description}, {tags}, {credits}, {use_limitations}, {anonymous_use}, {texture_optimization}, {enable_scene_expiration}, {scene_expiration_type}, {expiration_date}, {expiration_message}, {select_related_rows}, {reference_online_content})

       Creates a mobile scene package file (.mspk) from one or more scenes
       for use across the ArcGIS platform.

    INPUTS:
     in_scene (Map):
         One or more local or global scenes that will be packaged into a single
         .mspk file. Active scenes and .mapx files can be added as input.
     in_locator {Address Locator}:
         Locators have the following restrictions:
     area_of_interest {Feature Layer}:
         A polygon layer that defines the area of interest. Only those features
         that intersect the area of interest will be included in the mobile
         scene package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     clip_features {Boolean}:
         Specifies whether the output features will be clipped to the given
         area of interest or extent.

         * Checked-The geometry of the features will be clipped to the given
         area of interest or extent.

         * Unchecked-Features in the scene will be selected and their geometry
         will remain unaltered. This is the default.
         Multipatch feature layers, 3D point feature layers, LAS dataset
         layers, service layers, and tile packages cannot be clipped and will
         be completely copied to the mobile scene package.Specifies whether the
         output features will be clipped to the given
         area of interest or extent.

         * CLIP-The geometry of the features will be clipped to the given area
         of interest or extent.

         * SELECT-Features in the map will be selected and their geometry will
         remain unaltered. This is the default.
         Multipatch feature layers, 3D point feature layers, LAS dataset
         layers, and tile packages cannot be clipped and will be completely
         copied to the mobile scene package.
     title {String}:
         Title information that will be added to the properties of the package.
     summary {String}:
         Summary information that will be added to the properties of the
         package.
     description {String}:
         Description information that will be added to the properties of the
         package.
     tags {String}:
         Tag information that will be added to the properties of the package.
         Multiple tags can be added, separated by a comma or semicolon.
     credits {String}:
         Credit information that will be added to the properties of the
         package.
     use_limitations {String}:
         Use limitations that will be added to the properties of the package.
     anonymous_use {Boolean}:
         Specifies whether the mobile scenes can be used by anyone or only
         those with an ArcGIS account.

         * ANONYMOUS_USE-Anyone with access to the package can use the mobile
         scene without signing in with an Esri named user account.

         * STANDARD-Anyone with access to the package must be signed in with a
         named user account to use the mobile scene. This is the default.
         This optional parameter is only available with the Publisher
         extension.
     texture_optimization {String}:
         Specifies the textures that will be optimized according to the
         target platform where the scene layer package is used.
         Optimizations that include KTX2 may take significant time to process.
         For fastest results, use the DESKTOP or NONE options.

         * ALL-All texture formats will be optimized including JPEG, DXT, and
         KTX2 for use in desktop, web, and mobile platforms.

         * DESKTOP-Windows, Linux, and Mac supported textures will be optimized
         including JPEG and DXT for use in ArcGIS Pro clients on Windows and
         ArcGIS Runtime desktop clients on Windows, Linux, and Mac. This is the
         default.

         * MOBILE-Android and iOS supported textures will be optimized
         including JPEG and KTX2 for use in ArcGIS Runtime mobile applications.

         * NONE-JPEG textures will be optimized for use in desktop and web
         platforms.
     enable_scene_expiration {Boolean}:
         Specifies whether the mobile scene package will time out.

         * ENABLE_SCENE_EXPIRATION-Time-out functionality will be enabled on
         the mobile scene package.

         * DISABLE_SCENE_EXPIRATION-Time-out functionality will not be enabled
         on the mobile scene package. This is the default.
         This optional parameter is only available with the Publisher
         extension.
     scene_expiration_type {String}:
         Specifies the type of scene access that will be used for the expired
         mobile scene package.

         * ALLOW_TO_OPEN-The user of the package will be warned that the scene
         has expired and allowed to open the scene. This is the default.

         * DONOT_ALLOW_TO_OPEN-The user of the package will be warned that the
         scene has expired and will not be allowed to open the package.
         This optional parameter is only available with the Publisher
         extension.
     expiration_date {Date}:
         The date the mobile scene package will expire.This optional parameter
         is only available with the Publisher
         extension.
     expiration_message {String}:
         A text message will appear when an expired scene is accessed.This
         optional parameter is only available with the Publisher
         extension.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     reference_online_content {Boolean}:
         Specifies whether service layers will be referenced in the package.

         * INCLUDE_SERVICE_LAYERS-Service layers will be referenced in the
         mobile package.

         * EXCLUDE_SERVICE_LAYERS-Service layers will not be referenced in the
         mobile package. This is the default.

    OUTPUTS:
     output_file (File):
         The output mobile scene package .mspk file."""
    ...

@gptooldoc("CreateSceneLayerPackage_management", None)
def CreateSceneLayerPackage(
    in_layer=...,
    out_slpk=...,
    attributes=...,
    spatial_reference=...,
    point_size_m=...,
    xy_max_error_m=...,
    z_max_error_m=...,
    transform_method=...,
    in_coordinate_system=...,
    scene_layer_version=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateSceneLayerPackage_management(in_layer, out_slpk, {attributes;attributes...}, {spatial_reference}, {point_size_m}, {xy_max_error_m}, {z_max_error_m}, {transform_method;transform_method...}, {in_coordinate_system}, {scene_layer_version})

       Creates a scene layer package (.slpk file) from 3D points, multipatch
       features, or LAS data.

    INPUTS:
     in_layer (Layer File / Feature Layer / LAS Dataset Layer / Folder / File):
         The 3D points, multipatch feature layers, or LAS data (LAS, ZLAS, or
         LAZ) that will be used to create a scene layer package. LAS data can
         also be specified by selecting the parent folder that contains the
         desired files.
     attributes {String}:
         The source data attributes to be included in the scene layer package.
         These values will be accessible when the content is consumed in other
         viewers. Select attributes are required for the desired rendering and
         filtering option (for example, intensity, returns, class codes, and
         RGB). To reduce storage, unneeded attributes should be excluded.

         * INTENSITY-The return strength of the laser pulse for each lidar
         point.

         * RGB-RGB imagery information collected for each lidar point.

         * FLAGS-Classification and scan direction flags.

         * CLASS_CODE-Classification code values.

         * RETURNS-Discrete return number from the lidar pulse.

         * USER_DATA-A customizable attribute that can be any number in the
         range from 0 through 255.

         * POINT_SRC_ID-For aerial lidar, this value typically identifies the
         flight path that collected a given lidar point.

         * GPS_TIME-The GPS time stamp at which the laser point was emitted
         from the aircraft. The time is in GPS seconds of the week.

         * SCAN_ANGLE-The angular direction of the laser scanner for a given
         lidar point. This value can range from -90 through 90.

         * NEAR_INFRARED-Near infrared records collected for each lidar point.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output scene layer package. It
         can be any projected coordinate system or GCS_WGS_1984. If a z-datum
         is defined, the linear unit must match that of the horizontal
         coordinate system. If the horizontal coordinate system is expressed in
         geographic coordinates, the z-datum must use meters. GCS_WGS_1984 is
         the default coordinate system. The spatial reference can be specified
         by any of the following:

         * Specifying the path to a .prj file

         * Referencing a geodataset with the desired spatial reference

         * Using an arcpy.SpatialReference object
     point_size_m {Double}:
         The point size of the lidar data. For airborne lidar data, the default
         of 0 or a value close to the average point spacing is usually best.
         For terrestrial lidar data, the point size should match the desired
         point spacing for the areas of interest. The default of 0 will
         automatically determine the best value for the input dataset.
     xy_max_error_m {Double}:
         The maximum x,y error tolerated. A higher tolerance will result in
         better data compression and more efficient data transfer. The default
         is 0.1.
     z_max_error_m {Double}:
         The maximum z-error tolerated. A higher tolerance will result in
         better data compression and more efficient data transfer. The default
         is 0.1.
     transform_method {String}:
         The datum transformation method that will be used when the input
         layer's spatial reference uses a datum that differs from the output
         coordinate system. All transformations are bidirectional, regardless
         of the direction implied by their names. For example,
         NAD_1927_to_WGS_1984_3 will work correctly even if the datum
         conversion is from WGS 1984 to NAD 1927.
     in_coordinate_system {Coordinate System}:
         The coordinate system of the input LAZ files. This parameter is only
         used for LAZ files that do not contain spatial reference information
         in their header or have a PRJ file.
     scene_layer_version {String}:
         The Indexed 3D Scene Layer (I3S) version of the resulting point cloud
         scene layer package. Specifying a version provides support for
         backward compatibility and allows scene layer packages to be shared
         with earlier versions of ArcGIS.

         * 1.X-Supported in all ArcGIS clients. This is the default.

         * 2.X-Supported in ArcGIS Pro 2.1.2 or later and can be published to
         ArcGIS Online and ArcGIS Enterprise 10.6.1 or later.

    OUTPUTS:
     out_slpk (File):
         The output scene layer package (.slpk file)."""
    ...

@gptooldoc("CreateVectorTileIndex_management", None)
def CreateVectorTileIndex(
    in_map=...,
    out_featureclass=...,
    service_type=...,
    tiling_scheme=...,
    vertex_count=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateVectorTileIndex_management(in_map, out_featureclass, service_type, {tiling_scheme}, {vertex_count})

       Creates a multiscale mesh of polygons that can be used as index
       polygons when creating vector tile packages.

    INPUTS:
     in_map (Map):
         The input map with the feature distribution and vertex density that
         dictate the size and arrangement of output polygons. The input map is
         typically one that you will subsequently use to create vector tiles
         using the Create Vector Tile Package tool.
     service_type (Boolean):
         Specifies whether the tiling scheme will be generated from an existing
         map service or for ArcGIS Online, Bing Maps, and Google Maps.

         * ONLINE-The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be
         used. The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you
         to overlay cache tiles with tiles from these online mapping services.
         ArcGIS Pro includes this tiling scheme as a built-in option when
         loading a tiling scheme. When you choose this tiling scheme, the data
         frame of your source map must use the WGS 1984 Web Mercator (Auxiliary
         Sphere) projected coordinate system. This is the default.

         * EXISTING-The tiling scheme from an existing vector tile service will
         be used. Only tiling schemes with scales that double in progression
         through levels and have 512-by-512 tile size are supported. You must
         specify a vector tile service or tiling scheme file in the
         tiling_scheme parameter.
     tiling_scheme {Map Server / File}:
         The vector tile service or tiling scheme file to be used if the
         service_type parameter is set to EXISTING. The tiling scheme tile size
         must be 512 by 512 and must have consecutive scales in a ratio of two.
     vertex_count {Long}:
         The ideal number of vertices from all visible layers to be enclosed by
         each polygon in the output feature class. The default value is the
         recommended count of 10,000 vertices.

    OUTPUTS:
     out_featureclass (Feature Class):
         The output polygon feature class of indexed tiles at each level of
         detail. Each tile encloses a manageable number of input vertices not
         exceeding the number specified by the vertex_count parameter."""
    ...

@gptooldoc("CreateVectorTilePackage_management", None)
def CreateVectorTilePackage(
    in_map=...,
    output_file=...,
    service_type=...,
    tiling_scheme=...,
    tile_structure=...,
    min_cached_scale=...,
    max_cached_scale=...,
    index_polygons=...,
    summary=...,
    tags=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateVectorTilePackage_management(in_map, output_file, service_type, {tiling_scheme}, {tile_structure}, min_cached_scale, max_cached_scale, {index_polygons}, {summary}, {tags})

       Generates vector tiles from a map or basemap and packages the tiles in
       a single .vtpk file.

    INPUTS:
     in_map (Map):
         The map from which tiles will be generated and packaged. The input map
         must have metadata description and tags.
     service_type (Boolean):
         Specifies whether the tiling scheme will be generated from an existing
         map service or if map tiles will be generated for ArcGIS Online, Bing
         Maps, and Google Maps.

         * ONLINE-The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be
         used. This tiling scheme allows you to overlay cache tiles with tiles
         from these online mapping services. ArcGIS Pro includes this tiling
         scheme as a built-in option when loading a tiling scheme. When you
         choose this tiling scheme, the data frame of the source map must use
         the WGS 1984 Web Mercator (Auxiliary Sphere) projected coordinate
         system. This is the default.

         * EXISTING-A tiling scheme from an existing vector tile service will
         be used. Only tiling schemes with scales that double in progression
         through levels and have 512-by-512 tile size are supported. You must
         specify a vector tile service or tiling scheme file in the
         tiling_scheme parameter.
     tiling_scheme {Map Server / File}:
         A vector tile service or tiling scheme file that will be used if the
         service_type parameter is set to EXISTING. The tiling scheme tile size
         must be 512 by 512 and must have consecutive scales in a ratio of two.
     tile_structure {String}:
         Specifies whether the tile generation structure will be optimized with
         an indexed structure or as a flat array of all tiles at all levels of
         detail. The optimized indexed structure is the default and results in
         a smaller cache.

         * INDEXED-Tiles that are based on an index of feature density that
         optimizes the tile generation and file sizes will be produced. This is
         the default.

         * FLAT-Regular tiles for each level of detail will be produced without
         regard to feature density. This cache is larger than that produced
         with an indexed structure.
     min_cached_scale (Double):
         The minimum (smallest) scale at which tiles will be generated. This
         does not need to be the smallest scale in the tiling scheme. The
         minimum cached scale determines which scales will be used to generate
         cache.
     max_cached_scale (Double):
         The maximum (largest) scale at which tiles will be generated. This
         does not need to be the largest scale in the tiling scheme. The
         maximum cached scale determines which scales will be used to generate
         cache.
     index_polygons {Feature Layer}:
         A pregenerated index of tiles based on feature density, applicable
         only when the tile_structure parameter is set to INDEXED. Use the
         Create Vector Tile Index tool to create index polygons. If no index
         polygons are specified for this parameter, optimized index polygons
         will be generated during processing to aid in tile creation, but they
         will not be saved or output.
     summary {String}:
         Adds summary information to properties of the output vector tile
         package.
     tags {String}:
         Adds tag information to the properties of the output vector tile
         package. Separate multiple tags with commas or semicolons.

    OUTPUTS:
     output_file (File):
         The output vector tile package. The file extension of the package is
         .vtpk."""
    ...

@gptooldoc("ExtractPackage_management", None)
def ExtractPackage(
    in_package=...,
    output_folder=...,
    cache_package=...,
    storage_format_type=...,
    create_ready_to_serve_format=...,
    target_cloud_connection=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExtractPackage_management(in_package, {output_folder}, {cache_package}, {storage_format_type}, {create_ready_to_serve_format}, {target_cloud_connection})

       Extracts the contents of a package to a specified folder. The output
       folder will be updated with the extracted contents of the input
       package.

    INPUTS:
     in_package (File):
         The input package that will be extracted.
     cache_package {Boolean}:
         Specifies whether a copy of the package will be cached to your
         profile.When extracting a package, the output is first extracted to
         your user
         profile and appended with a unique ID before a copy is made to the
         directory specified in the output_folder parameter. Downloading and
         extracting subsequent versions of the same package will only update
         this location. You do not need to manually create a cached version of
         the package in your user profile when using this parameter. This
         parameter is not enabled if the input package is a vector tile package
         (.vtpk) or a tile package (.tpk and .tpkx).

         * CACHE-A copy of the package will be extracted and cached to your
         profile. This is the default.

         * NO_CACHE-A copy of the package will only be extracted to the output
         parameter specified; it will not be cached.
     storage_format_type {String}:
         Specifies the storage format that will be used for the extracted
         cache. This parameter is applicable only when the input package is a
         vector tile package (.vtpk).

         * COMPACT-The tiles will be grouped in bundle files using the Compact
         V2 storage format. This format provides better performance on network
         shares and cloud store directories. This is the default.

         * EXPLODED-Each tile will be stored as an individual file.
     create_ready_to_serve_format {Boolean}:
         Specifies whether a ready-to-serve format for ArcGIS Enterprise will
         be created. This parameter is enabled only when the input package is a
         vector tile package (.vtpk) or a tile package (.tpkx).

         * READY_TO_SERVE_CACHE_DATASET-A folder structure with an extracted
         cache that can be used to create a tile layer in ArcGIS Enterprise
         will be created. The file extension of the folder signifies the
         content it stores: .tiles (cache dataset) for tile layer packages or
         .vtiles (vector cache dataset) for vector tile packages.

         * EXTRACTED_PACKAGE-A folder structure with extracted contents of the
         package will be created. This is the default.
     target_cloud_connection {Folder}:
         The target .acs file to which the package contents will be extracted.
         This parameter is enabled only when the input package is a scene layer
         package (.slpk), a vector tile package (.vtpk), or a tile package
         (.tpkx).

    OUTPUTS:
     output_folder {Folder}:
         The output folder that will contain the contents of the package.If the
         specified folder does not exist, a folder will be created."""
    ...

@gptooldoc("PackageLayer_management", None)
def PackageLayer(
    in_layer=...,
    output_file=...,
    convert_data=...,
    convert_arcsde_data=...,
    extent=...,
    apply_extent_to_arcsde=...,
    schema_only=...,
    version=...,
    additional_files=...,
    summary=...,
    tags=...,
    select_related_rows=...,
    preserve_sqlite=...,
    exclude_network_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PackageLayer_management(in_layer;in_layer..., output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {version;version...}, {additional_files;additional_files...}, {summary}, {tags}, {select_related_rows}, {preserve_sqlite}, {exclude_network_dataset})

       Packages one or more layers and all referenced data sources to create
       a single compressed .lpkx file.

    INPUTS:
     in_layer (Layer / Table View):
         The layers that will be packaged.
     convert_data {Boolean}:
         Specifies whether input layers will be converted to a file geodatabase
         or preserved in their original format.

         * CONVERT-Data will be converted to a file geodatabase. This option
         does not apply to enterprise geodatabase data sources. To convert
         enterprise geodatabase data, set the convert_arcsde_data parameter to
         CONVERT_ARCSDE.

         * PRESERVE-Data formats will be preserved when possible. This is the
         default.
     convert_arcsde_data {Boolean}:
         Specifies whether input enterprise geodatabase layers will be
         converted to a file geodatabase or preserved in their original format.

         * CONVERT_ARCSDE-Enterprise geodatabase data will be converted to a
         file geodatabase and will be included in the consolidated folder or
         package. This is the default.

         * PRESERVE_ARCSDE-Enterprise geodatabase data will be preserved and
         will be referenced in the consolidated folder or package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ARCSDE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     schema_only {Boolean}:
         Specifies whether only the schema of the input layers will be
         consolidated or packaged.

         * ALL-All features and records will be consolidated or packaged. This
         is the default.

         * SCHEMA_ONLY-Only the schema of the input layers will be
         consolidated or packaged.
     version {String}:
         Specifies the version of the geodatabases that will be created
         in the resulting package. Specifying a version allows packages to be
         shared with earlier versions of ArcGIS and supports backward
         compatibility. A package saved to an earlier version may lose
         properties that are
         only available in the later version.

         * ALL-The package will contain a geodatabase and layer file
         compatible with all versions (ArcGIS Pro 1.2 and later).

         * CURRENT-The package will contain a geodatabase and layer file
         compatible with the version of the current ArcGIS Pro release.

         * 1.2-The package will contain a geodatabase and layer file compatible
         with ArcGIS Pro version 1.2 and later.

         * 2.x-The package will contain a geodatabase and layer file compatible
         with ArcGIS Pro version 2.0 and later.

         * 3.x-The package will contain a geodatabase and layer file compatible
         with ArcGIS Pro version 3.0 and later.
     additional_files {File}:
         Adds files to a package. Additional files, such as .doc, .txt, .pdf,
         and so on, are used to provide more information about the contents and
         purpose of the package.
     summary {String}:
         Adds summary information to the properties of the package.
     tags {String}:
         Adds tag information to the properties of the package. Multiple tags
         can be added or separated by a comma or semicolon.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     preserve_sqlite {Boolean}:
         Specifies whether mobile geodatabase data will be preserved in the
         output or written to file geodatabase format. If the input data is a
         mobile geodatabase network dataset, the output will be a mobile
         geodatabase.This parameter overrides the convert_data parameter when
         the input
         data is a mobile geodatabase.

         * CONVERT_SQLITE-Mobile geodatabase data will be converted to file
         geodatabase format. This is the default.

         * PRESERVE_SQLITE-Mobile geodatabase data will be preserved in the
         output. The geodatabase will be included in its entirety.
     exclude_network_dataset {Boolean}:
         For network analysis layers, specifies whether the network dataset
         will also be packaged.

         * INCLUDE_NETWORK_DATASET-The network dataset will be included and
         packaged. This is the default.

         * EXCLUDE_NETWORK_DATASET-The network dataset will not be included.
         Only the network analysis layers will be packaged.

    OUTPUTS:
     output_file (File):
         The location and name of the output package file (.lpkx) that will be
         created."""
    ...

@gptooldoc("PackageLocator_management", None)
def PackageLocator(
    in_locator=...,
    output_file=...,
    copy_arcsde_locator=...,
    additional_files=...,
    summary=...,
    tags=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PackageLocator_management(in_locator, output_file, {copy_arcsde_locator}, {additional_files;additional_files...}, {summary}, {tags})

       Package a locator or composite locator to create a single compressed
       .gcpk file.

    INPUTS:
     in_locator (Address Locator):
         The locator or composite locator that will be packaged.
     copy_arcsde_locator {Boolean}:
         This parameter has no effect in ArcGIS Pro. It remains only to support
         backward compatibility.
     additional_files {File}:
         Adds files to a package. Additional files, such as .doc, .txt, .pdf,
         and so on, are used to provide more information about the contents and
         purpose of the package.
     summary {String}:
         Adds summary information to the properties of the package.
     tags {String}:
         Adds tag information to the properties of the package. Multiple tags
         can be added or separated by a comma or semicolon.

    OUTPUTS:
     output_file (File):
         The name and location of the output locator package (.gcpk)."""
    ...

@gptooldoc("PackageMap_management", None)
def PackageMap(
    in_map=...,
    output_file=...,
    convert_data=...,
    convert_arcsde_data=...,
    extent=...,
    apply_extent_to_arcsde=...,
    arcgisruntime=...,
    reference_all_data=...,
    version=...,
    additional_files=...,
    summary=...,
    tags=...,
    select_related_rows=...,
    preserve_sqlite=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PackageMap_management(in_map;in_map..., output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {arcgisruntime}, {reference_all_data}, {version;version...}, {additional_files;additional_files...}, {summary}, {tags}, {select_related_rows}, {preserve_sqlite})

       Packages a map and all referenced data sources to create a single
       compressed .mpkx file.

    INPUTS:
     in_map (Map):
         The map to be packaged. When running this tool in ArcGIS Pro, the
         input can be a map, scene, or basemap.
     convert_data {Boolean}:
         Specifies whether input layers will be converted to a file geodatabase
         or preserved in their original format.

         * CONVERT-Data will be converted to a file geodatabase. This option
         does not apply to enterprise geodatabase data sources. To convert
         enterprise geodatabase data, set the convert_arcsde_data parameter to
         CONVERT_ARCSDE.

         * PRESERVE-Data formats will be preserved when possible. This is the
         default.
     convert_arcsde_data {Boolean}:
         Specifies whether input enterprise geodatabase layers will be
         converted to a file geodatabase or preserved in their original format.

         * CONVERT_ARCSDE-Enterprise geodatabase data will be converted to a
         file geodatabase and will be included in the consolidated folder or
         package. This is the default.

         * PRESERVE_ARCSDE-Enterprise geodatabase data will be preserved and
         will be referenced in the consolidated folder or package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ARCSDE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     arcgisruntime {Boolean}:
         Specifies whether the package will support ArcGIS Runtime. To support
         ArcGIS Runtime, all data sources will be converted to a file
         geodatabase and an .msd file will be created in the output package.

         * DESKTOP-The output package will not support ArcGIS Runtime. Unless
         otherwise specified, data sources will not be converted to a file
         geodatabase, and an .msd file will not be created.

         * RUNTIME-The output package will support ArcGIS Runtime. All data
         sources will be converted to a file geodatabase, and an .msd file will
         be created in the output package.
     reference_all_data {Boolean}:
         Specifies whether a package that references the necessary data will be
         created rather than copying the data. This is helpful when trying to
         package large datasets that are available from a central location in
         an organization.

         * REFERENCED-A package that references the necessary data will be
         created rather than copying the data.

         * NOT_REFERENCED-A package that contains the necessary data will be
         created. This is the default.
     version {String}:
         Specifies the version of the geodatabases that will be created
         in the resulting package. Specifying a version allows packages to be
         shared with earlier versions of ArcGIS and supports backward
         compatibility. A package saved to an earlier version may lose
         properties that are
         only available in the later version.

         * ALL-The package will contain geodatabases and a map compatible with
         all versions (ArcGIS Pro 1.2 and later).

         * CURRENT-The package will contain geodatabases and a map compatible
         with the version of the current ArcGIS Pro release.

         * 1.2-The package will contain geodatabases and a map compatible with
         ArcGIS Pro version 1.2 and later.

         * 2.x-The package will contain geodatabases and a map compatible with
         ArcGIS Pro version 2.0 and later.

         * 3.x-The package will contain geodatabases and a map compatible with
         ArcGIS Pro version 3.0 and later.
     additional_files {File}:
         Adds files to a package. Additional files, such as .doc, .txt, .pdf,
         and so on, are used to provide more information about the contents and
         purpose of the package.
     summary {String}:
         Adds summary information to the properties of the package.
     tags {String}:
         Adds tag information to the properties of the package. Multiple tags
         can be added or separated by a comma or semicolon.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     preserve_sqlite {Boolean}:
         Specifies whether mobile geodatabase data will be preserved in the
         output or written to file geodatabase format. If the input data is a
         mobile geodatabase network dataset, the output will be a mobile
         geodatabase.

         * CONVERT_SQLITE-Mobile geodatabase data will be converted to file
         geodatabase format. This is the default.

         * PRESERVE_SQLITE-Mobile geodatabase data will be preserved in the
         output. The geodatabase will be included in its entirety.

    OUTPUTS:
     output_file (File):
         The output map package (.mpkx file)."""
    ...

@gptooldoc("PackageProject_management", None)
def PackageProject(
    in_project=...,
    output_file=...,
    sharing_internal=...,
    package_as_template=...,
    extent=...,
    apply_extent_to_arcsde=...,
    additional_files=...,
    summary=...,
    tags=...,
    version=...,
    include_toolboxes=...,
    include_history_items=...,
    read_only=...,
    select_related_rows=...,
    preserve_sqlite=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PackageProject_management(in_project, output_file, {sharing_internal}, {package_as_template}, {extent}, {apply_extent_to_arcsde}, {additional_files;additional_files...}, {summary}, {tags}, {version;version...}, {include_toolboxes}, {include_history_items}, {read_only}, {select_related_rows}, {preserve_sqlite})

       Consolidates and packages a project file (.aprx) of referenced maps
       and data to a packaged project file (.ppkx).

    INPUTS:
     in_project (File):
         The project (.aprx file) to be packaged.
     sharing_internal {Boolean}:
         Specifies whether the project will be consolidated for your internal
         environment or will move all data elements so it can be shared
         externally.

         * INTERNAL-Enterprise data sources, such as enterprise geodatabases
         and data from a UNC path, will not be copied to the local folder. This
         is the default.

         * EXTERNAL-Data formats will be copied and preserved when possible.
     package_as_template {Boolean}:
         Specifies whether a project template or a project package will be
         created. Templates can include maps, layouts, connections to databases
         and servers, and so on. A project template can be used to standardize
         a series of maps for different projects and to ensure that the correct
         layers are immediately available for everyone to use in their maps.

         * PROJECT_PACKAGE-A project package will be created. This is the
         default.

         * PROJECT_TEMPLATE-A project template will be created
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ENTERPRISE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     additional_files {File}:
         Adds files to a package. Additional files, such as .doc, .txt, .pdf,
         and so on, are used to provide more information about the contents and
         purpose of the package.
     summary {String}:
         The summary information that will be added to the properties of the
         package.
     tags {String}:
         The tags that will be added to the properties of the package. Separate
         multiple tags with a comma or semicolon.
     version {String}:
         Specifies the version of the geodatabases that will be created
         in the resulting package. Specifying a version allows packages to be
         shared with earlier versions of ArcGIS and supports backward
         compatibility. A package saved to an earlier version may lose
         properties that are
         only available in the later version.

         * ALL-The package will contain geodatabases and maps compatible with
         all versions (ArcGIS Pro 2.1 and later).

         * CURRENT-The package will contain geodatabases and maps compatible
         with the version of the current release.

         * 2.2-The package will contain geodatabases and maps compatible with
         version 2.2.

         * 2.3-The package will contain geodatabases and maps compatible with
         version 2.3.

         * 2.4-The package will contain geodatabases and maps compatible with
         version 2.4.

         * 2.5-The package will contain geodatabases and maps compatible with
         version 2.5.

         * 2.6-The package will contain geodatabases and maps compatible with
         version 2.6.

         * 2.7-The package will contain geodatabases and maps compatible with
         version 2.7.

         * 2.8-The package will contain geodatabases and maps compatible with
         version 2.8.

         * 2.9-The package will contain geodatabases and maps compatible with
         version 2.9.

         * 3.0-The package will contain geodatabases and maps compatible with
         version 3.0.

         * 3.1-The package will contain geodatabases and maps compatible with
         version 3.1.
     include_toolboxes {Boolean}:
         Specifies whether project toolboxes will be consolidated and included
         in the output package. All projects require a default toolbox, and the
         default toolbox will be included regardless of this setting.

         * TOOLBOXES-Project toolboxes will be included in the output package.
         This is the default.

         * NO_TOOLBOXES-Project toolboxes will not be included in the output
         package.
     include_history_items {String}:
         Specifies whether geoprocessing history items will be consolidated and
         included in the output package. Included history items will
         consolidate the data required to reprocess the history item.

         * HISTORY_ITEMS-History items will be included in the output package.
         This is the default.

         * NO_HISTORY_ITEMS-History items will not be included in the output
         package.

         * VALID_HISTORY_ITEMS_ONLY-Only valid history items will be included
         in the output package. History items are invalid if any of the
         original input layers or tools cannot be found.
     read_only {Boolean}:
         Specifies whether the project will be read-only. Read-only projects
         cannot be modified or saved.

         * READ_ONLY-The project will be read-only.

         * READ_WRITE-The project will be writable. This is the default.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.
     preserve_sqlite {Boolean}:
         Specifies whether mobile geodatabase data will be preserved in the
         output or written to file geodatabase format. If the input data is a
         mobile geodatabase network dataset, the output will be a mobile
         geodatabase.

         * CONVERT_SQLITE-Mobile geodatabase data will be converted to file
         geodatabase format. This is the default.

         * PRESERVE_SQLITE-Mobile geodatabase data will be preserved in the
         output. The geodatabase will be included in its entirety.

    OUTPUTS:
     output_file (File):
         The output project package (.ppkx file)."""
    ...

@gptooldoc("PackageResult_management", None)
def PackageResult(
    in_result=...,
    output_file=...,
    convert_data=...,
    convert_arcsde_data=...,
    extent=...,
    apply_extent_to_arcsde=...,
    schema_only=...,
    arcgisruntime=...,
    additional_files=...,
    summary=...,
    tags=...,
    version=...,
    select_related_rows=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PackageResult_management(in_result;in_result..., output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {arcgisruntime}, {additional_files;additional_files...}, {summary}, {tags}, {version;version...}, {select_related_rows})

       Packages one or more geoprocessing results, including all tools and
       input and output datasets, into a single compressed file (.gpkx).

    INPUTS:
     in_result (String / File):
         The result that will be packaged.The input can be either a result from
         the history of the current
         project or a Result object's resultID property when the tool is being
         used in a Python script.
     convert_data {Boolean}:
         Specifies whether input layers will be converted to a file geodatabase
         or preserved in their original format.

         * CONVERT-Data will be converted to a file geodatabase. This option
         does not apply to enterprise geodatabase data sources. To convert
         enterprise geodatabase data, set the convert_arcsde_data parameter to
         CONVERT_ARCSDE.

         * PRESERVE-Data formats will be preserved when possible. This is the
         default.
     convert_arcsde_data {Boolean}:
         Specifies whether input enterprise geodatabase layers will be
         converted to a file geodatabase or preserved in their original format.

         * CONVERT_ARCSDE-Enterprise geodatabase data will be converted to a
         file geodatabase and will be included in the consolidated folder or
         package. This is the default.

         * PRESERVE_ARCSDE-Enterprise geodatabase data will be preserved and
         will be referenced in the consolidated folder or package.
     extent {Extent}:
         Specifies the extent that will be used to select or clip features.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     apply_extent_to_arcsde {Boolean}:
         Specifies whether the specified extent will be applied to all layers
         or to enterprise geodatabase layers only.

         * ALL-The specified extent will be applied to all layers. This is
         the default.

         * ARCSDE_ONLY-The specified extent will be applied to enterprise
         geodatabase layers only.
     schema_only {Boolean}:
         Specifies whether all records for input and output datasets or only
         the schema of input and output datasets will be consolidated or
         packaged.

         * ALL-All records for input and output datasets will be consolidated
         or packaged. This is the default.

         * SCHEMA_ONLY-Only the schema of input and output datasets will be
         consolidated or packaged.
     arcgisruntime {Boolean}:
         Specifies whether the package will support ArcGIS Runtime. To support
         ArcGIS Runtime, all data sources will be converted to a file
         geodatabase.

         * DESKTOP-The output package will not support ArcGIS Runtime. This is
         the default.

         * RUNTIME-The output package will support ArcGIS Runtime.
     additional_files {File}:
         Adds files to a package. Additional files, such as .doc, .txt, .pdf,
         and so on, are used to provide more information about the contents and
         purpose of the package.
     summary {String}:
         Adds summary information to the properties of the package.
     tags {String}:
         Adds tag information to the properties of the package. Multiple tags
         can be added or separated by a comma or semicolon.
     version {String}:
         Specifies the version of the geodatabases that will be created
         in the resulting package. Specifying a version allows packages to be
         shared with earlier versions of ArcGIS and supports backward
         compatibility. A package saved to an earlier version may lose
         properties that are
         only available in the later version.

         * ALL-The package will contain geodatabases and maps compatible with
         all versions (ArcGIS Pro 2.1 and later).

         * CURRENT-The package will contain geodatabases and maps compatible
         with the version of the current release.

         * 2.2-The package will contain geodatabases and maps compatible with
         version 2.2.

         * 2.3-The package will contain geodatabases and maps compatible with
         version 2.3.

         * 2.4-The package will contain geodatabases and maps compatible with
         version 2.4.

         * 2.5-The package will contain geodatabases and maps compatible with
         version 2.5.

         * 2.6-The package will contain geodatabases and maps compatible with
         version 2.6.

         * 2.7-The package will contain geodatabases and maps compatible with
         version 2.7.

         * 2.8-The package will contain geodatabases and maps compatible with
         version 2.8.

         * 2.9-The package will contain geodatabases and maps compatible with
         version 2.9.

         * 3.0-The package will contain geodatabases and maps compatible with
         version 3.0.

         * 3.1-The package will contain geodatabases and maps compatible with
         version 3.1.
     select_related_rows {Boolean}:
         Specifies whether the specified extent will be applied to related data
         sources.

         * KEEP_ONLY_RELATED_ROWS-Only related data corresponding to records
         within the specified extent will be consolidated.

         * KEEP_ALL_RELATED_ROWS-Related data sources will be consolidated in
         their entirety. This is the default.

    OUTPUTS:
     output_file (File):
         The name and location of the output package file (.gpkx)."""
    ...

@gptooldoc("SharePackage_management", None)
def SharePackage(
    in_package=...,
    username=...,
    password=...,
    summary=...,
    tags=...,
    credits=...,
    public=...,
    groups=...,
    organization=...,
    publish_web_layer=...,
    portal_folder=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SharePackage_management(in_package, username, password, {summary}, {tags}, {credits}, {public}, {groups;groups...}, {organization}, {publish_web_layer}, {portal_folder})

       Shares a package by uploading it to ArcGIS Online or ArcGIS
       Enterprise.

    INPUTS:
     in_package (File):
         The input layer (.lpk or .lpkx), scene layer (.slpk), map (.mpk or
         .mpkx), geoprocessing (.gpk, .gpkx), tile (.tpk or .tpkx), mobile map
         (.mmpk), vector tile (.vtpk), address locator (.gcpk), or project
         (.ppkx or .aptx) package file.
     username (String):
         The ArcGIS Online or ArcGIS Enterprise username. This parameter has
         been deprecated and should consist of an empty string. Before running
         the Python script, you must sign in to the active portal from the
         application. Alternatively, you can sign in using the SignInToPortal
         function.
     password (Encrypted String):
         The ArcGIS Online or ArcGIS Enterprise password. This parameter has
         been deprecated and should consist of an empty string. Before running
         the Python script, you must sign in to the active portal from the
         application. Alternatively, you can sign in using the SignInToPortal
         function.
     summary {String}:
         The summary of the package. The summary is displayed in the item
         information of the package on ArcGIS Online or ArcGIS Enterprise.
     tags {String}:
         The tags used to describe and identify the package. Individual tags
         are separated using either a comma or a semicolon.
     credits {String}:
         The credits for the package. This is generally the name of the
         organization that is given credit for authoring and providing the
         content for the package.
     public {Boolean}:
         Specifies whether the input package will be shared with and available
         to everybody.

         * EVERYBODY-The input package will be shared with everybody.

         * MYGROUPS-The input package will be shared with the package owner
         and any selected groups. This is the default.
     groups {String}:
         The groups the package will be shared with.
     organization {Boolean}:
         Specifies whether the input package will be available within your
         organization only or shared publicly with everyone.

         * EVERYBODY-The package will be shared with everybody. This is the
         default.

         * MYORGANIZATION-The package will be shared within your organization
         only.
     publish_web_layer {Boolean}:
         Specifies whether the package will be published as a web layer to your
         portal. Only tile packages, vector tile packages, and scene layer
         packages are supported.

         * FALSE-The package will be uploaded without publishing. This is the
         default.

         * TRUE-The package will be uploaded and published as a web layer with
         the same name.
     portal_folder {String}:
         An existing folder or the name of a new folder on the portal for the
         package. If a web layer is published, it is stored in the same folder."""
    ...

@gptooldoc("Create3DObjectSceneLayerPackage_management", None)
def Create3DObjectSceneLayerPackage(
    in_dataset=...,
    out_slpk=...,
    out_coor_system=...,
    transform_method=...,
    texture_optimization=...,
    target_cloud_connection=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Create3DObjectSceneLayerPackage_management(in_dataset, {out_slpk}, {out_coor_system}, {transform_method;transform_method...}, {texture_optimization}, {target_cloud_connection})

       Creates a scene layer package (.slpk) or scene layer content
       (.i3sREST) from a multipatch or 3D object feature layer input.

    INPUTS:
     in_dataset (Layer File / Feature Layer):
         The input multipatch or 3D object feature layer.
     out_coor_system {Spatial Reference}:
         The coordinate system of the output scene layer package. It
         can be any projected or custom coordinate system. Supported geographic
         coordinate systems include WGS84 and China Geodetic Coordinate System
         2000. WGS84 and EGM96 Geoid are the default horizontal and vertical
         coordinate systems, respectively. The coordinate system can be
         specified in any of the following ways:

         * Specify the path to a .prj file.

         * Reference a dataset with the correct coordinate system.

         * Use an arcpy.SpatialReference object.
     transform_method {String}:
         The datum transformation method that will be used when the input
         layer's coordinate system uses a datum that differs from the output
         coordinate system. All transformations are bidirectional, regardless
         of the direction implied by their names. For example,
         NAD_1927_to_WGS84_3 will work correctly even if the datum conversion
         is from WGS84 to NAD 1927.The ArcGIS coordinate system data is
         required for vertical datum
         transformations between ellipsoidal and gravity-related and two
         gravity-related datums.
     texture_optimization {String}:
         Specifies the textures that will be optimized according to the
         target platform where the scene layer package is used.
         Optimizations that include KTX2 may take significant time to process.
         For fastest results, use the DESKTOP or NONE options.

         * ALL-All texture formats will be optimized including JPEG, DXT, and
         KTX2 for use in desktop, web, and mobile platforms.

         * DESKTOP-Windows, Linux, and Mac supported textures will be optimized
         including JPEG and DXT for use in ArcGIS Pro clients on Windows and
         ArcGIS Runtime desktop clients on Windows, Linux, and Mac. This is the
         default.

         * MOBILE-Android and iOS supported textures will be optimized
         including JPEG and KTX2 for use in ArcGIS Runtime mobile applications.

         * NONE-JPEG textures will be optimized for use in desktop and web
         platforms.
     target_cloud_connection {Folder}:
         The target cloud connection file (.acs) where the scene layer content
         (.i3sREST) will be output.

    OUTPUTS:
     out_slpk {File}:
         The output scene layer package (.slpk)."""
    ...

@gptooldoc("CreateBuildingSceneLayerPackage_management", None)
def CreateBuildingSceneLayerPackage(
    in_dataset=...,
    out_slpk=...,
    out_coor_system=...,
    transform_method=...,
    texture_optimization=...,
    target_cloud_connection=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateBuildingSceneLayerPackage_management(in_dataset, {out_slpk}, {out_coor_system}, {transform_method;transform_method...}, {texture_optimization}, {target_cloud_connection})

       Creates a scene layer package (.slpk) or scene layer content
       (.i3sREST) from a building layer input.

    INPUTS:
     in_dataset (Layer File / Building Layer):
         The input building layer or layer file (.lyrx).
     out_coor_system {Spatial Reference}:
         The coordinate system of the output scene layer package. It
         can be any projected or custom coordinate system. Supported geographic
         coordinate systems include WGS84 and China Geodetic Coordinate System
         2000. WGS84 and EGM96 Geoid are the default horizontal and vertical
         coordinate systems, respectively. The coordinate system can be
         specified in any of the following ways:

         * Specify the path to a .prj file.

         * Reference a dataset with the correct coordinate system.

         * Use an arcpy.SpatialReference object.
     transform_method {String}:
         The datum transformation method that will be used when the input
         layer's coordinate system uses a datum that differs from the output
         coordinate system. All transformations are bidirectional, regardless
         of the direction implied by their names. For example,
         NAD_1927_to_WGS84_3 will work correctly even if the datum conversion
         is from WGS84 to NAD 1927.The ArcGIS coordinate system data is
         required for vertical datum
         transformations between ellipsoidal and gravity-related and two
         gravity-related datums.
     texture_optimization {String}:
         Specifies the textures that will be optimized according to the
         target platform where the scene layer package is used.
         Optimizations that include KTX2 may take significant time to process.
         For fastest results, use the DESKTOP or NONE options.

         * ALL-All texture formats will be optimized including JPEG, DXT, and
         KTX2 for use in desktop, web, and mobile platforms.

         * DESKTOP-Windows, Linux, and Mac supported textures will be optimized
         including JPEG and DXT for use in ArcGIS Pro clients on Windows and
         ArcGIS Runtime desktop clients on Windows, Linux, and Mac. This is the
         default.

         * MOBILE-Android and iOS supported textures will be optimized
         including JPEG and KTX2 for use in ArcGIS Runtime mobile applications.

         * NONE-JPEG textures will be optimized for use in desktop and web
         platforms.
     target_cloud_connection {Folder}:
         The target cloud connection file (.acs) where the scene layer content
         (.i3sREST) will be output.

    OUTPUTS:
     out_slpk {File}:
         The output scene layer package (.slpk)."""
    ...

@gptooldoc("CreateIntegratedMeshSceneLayerPackage_management", None)
def CreateIntegratedMeshSceneLayerPackage(
    in_dataset=...,
    out_slpk=...,
    anchor_point=...,
    file_suffix=...,
    out_coor_system=...,
    max_texture_size=...,
    texture_optimization=...,
    target_cloud_connection=...,
    out_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateIntegratedMeshSceneLayerPackage_management(in_dataset;in_dataset..., {out_slpk}, {anchor_point}, {file_suffix}, {out_coor_system}, {max_texture_size}, {texture_optimization}, {target_cloud_connection}, {out_name})

       Creates scene layer content (.slpk or .i3sREST) from OpenSceneGraph
       binary (OSGB) data.

    INPUTS:
     in_dataset (Folder / File):
         The OSGB format files, or folders containing OSGB format files, that
         will be imported into the integrated mesh scene layer package. This
         parameter allows a selection of multiple OSGB format files or a
         selection of multiple folders containing OSGB format files.
     anchor_point {File / Feature Layer}:
         The point feature or .3mx, .xml, or .wld3 file that will be used to
         position the center of the OSGB model. If there are multiple points in
         the feature class, only the first point will be used to georeference
         the data.
     file_suffix {String}:
         Specifies the files that will be processed for the input dataset.

         * *-All binary files, regardless of their extension, will be processed
         to determine if they are in the OSGB format.

         * osgb-Only files with the .osgb extension will be processed.
     out_coor_system {Spatial Reference}:
         The coordinate system of the output scene layer package. It
         can be any projected or custom coordinate system. Supported geographic
         coordinate systems include WGS84 and China Geodetic Coordinate System
         2000. WGS84 and EGM96 Geoid are the default horizontal and vertical
         coordinate systems, respectively. The coordinate system can be
         specified in any of the following ways:

         * Specify the path to a .prj file.

         * Reference a dataset with the correct coordinate system.

         * Use an arcpy.SpatialReference object.
     max_texture_size {Long}:
         The maximum texture size in pixels for each scene layer node.
     texture_optimization {String}:
         Specifies the textures that will be optimized according to the
         target platform where the scene layer package is used.
         Optimizations that include KTX2 may take significant time to process.
         For fastest results, use the Desktop or None options.

         * All-All texture formats will be optimized including JPEG, DXT, and
         KTX2 for use in desktop, web, and mobile platforms.

         * Desktop-Windows, Linux, and Mac supported textures will be optimized
         including JPEG and DXT for use in ArcGIS Pro clients on Windows and
         ArcGIS Runtime desktop clients on Windows, Linux, and Mac. This is the
         default.

         * Mobile-Android and iOS supported textures will be optimized
         including JPEG and KTX2 for use in ArcGIS Runtime mobile applications.

         * None-JPEG textures will be optimized for use in desktop and web
         platforms.
     target_cloud_connection {Folder}:
         The target cloud connection file (.acs) where the scene layer content
         (.i3sREST) will be output.
     out_name {String}:
         The output name of the scene layer content when output to a cloud
         store. This parameter is only available when a target_cloud_connection
         parameter value is specified.

    OUTPUTS:
     out_slpk {File}:
         The integrated mesh scene layer package that will be created. This
         parameter is required if a Target Cloud Connection parameter value is
         not specified."""
    ...

@gptooldoc("CreatePointCloudSceneLayerPackage_management", None)
def CreatePointCloudSceneLayerPackage(
    in_dataset=...,
    out_slpk=...,
    out_coor_system=...,
    transform_method=...,
    attributes=...,
    point_size_m=...,
    xy_max_error_m=...,
    z_max_error_m=...,
    in_coor_system=...,
    scene_layer_version=...,
    target_cloud_connection=...,
    out_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreatePointCloudSceneLayerPackage_management(in_dataset, {out_slpk}, {out_coor_system}, {transform_method;transform_method...}, {attributes;attributes...}, {point_size_m}, {xy_max_error_m}, {z_max_error_m}, {in_coor_system}, {scene_layer_version}, {target_cloud_connection}, {out_name})

       Creates a point cloud scene layer package (.slpk) or scene layer
       content (.i3sREST) in the cloud from LAS, zLAS, LAZ, or LAS dataset
       input.

    INPUTS:
     in_dataset (Layer File / LAS Dataset Layer / Folder / File):
         The lidar data (LAS, zLAS, LAZ, or LAS dataset) that will be used to
         create a scene layer package. The lidar data can also be specified by
         selecting the parent folder that contains the files.
     out_coor_system {Spatial Reference}:
         The coordinate system of the output scene layer package. It
         can be any projected or custom coordinate system. Supported geographic
         coordinate systems include WGS84 and China Geodetic Coordinate System
         2000. WGS84 and EGM96 Geoid are the default horizontal and vertical
         coordinate systems, respectively. The coordinate system can be
         specified in any of the following ways:

         * Specify the path to a .prj file.

         * Reference a dataset with the correct coordinate system.

         * Use an arcpy.SpatialReference object.
     transform_method {String}:
         The datum transformation method that will be used when the
         input layer's coordinate system uses a datum that differs from the
         output coordinate system. All transformations are bidirectional,
         regardless of the direction implied by their names. For example,
         NAD_1927_to_WGS_1984_3 will work correctly even if the datum
         conversion is from WGS 1984 to NAD 1927. ArcGIS coordinate
         system data is required for vertical datum
         transformations between ellipsoidal and gravity-related and two
         gravity-related datums.
     attributes {String}:
         Specifies the source data attributes that will be included in the
         scene layer package. These values will be accessible when the content
         is consumed in other viewers. Select attributes that are required for
         the desired rendering and filtering options (for example, intensity,
         returns, class codes, RGB). To reduce storage, exclude unneeded
         attributes.

         * INTENSITY-The return strength of the laser pulse for each lidar
         point will be included.

         * RGB-RGB imagery information collected for each lidar point will be
         included.

         * FLAGS-Classification and scan direction flags will be included.

         * CLASS_CODE-Classification code values will be included.

         * RETURNS-Discrete return numbers from the lidar pulse will be
         included

         * USER_DATA-A customizable attribute that can be any number in the
         range of 0 through 255 will be included.

         * POINT_SRC_ID-For aerial lidar, this value typically identifies the
         flight path that collected a given lidar point, which will be
         included.

         * GPS_TIME-The GPS time stamp at which the laser point was emitted
         from the aircraft will be included. The time is in GPS seconds of the
         week in which the time stamp is between 0 and 604800 and resets at
         midnight on a Sunday.

         * SCAN_ANGLE-The angular direction of the laser scanner for a given
         lidar point will be included. The value range is from -90 through 90.

         * NEAR_INFRARED-Near infrared records collected for each lidar point
         will be included.
     point_size_m {Double}:
         The point size of the lidar data. For airborne lidar data, the default
         of 0 or a value close to the average point spacing is usually best.
         For terrestrial lidar data, the point size should match the desired
         point spacing for the areas of interest. Values are expressed in
         meters. The default of 0 will automatically determine the best value
         for the input dataset.
     xy_max_error_m {Double}:
         The maximum x,y error tolerated. A higher tolerance will result in
         better data compression and more efficient data transfer. Values are
         expressed in meters. The default is 0.001.
     z_max_error_m {Double}:
         The maximum z-error tolerated. A higher tolerance will result in
         better data compression and more efficient data transfer. Values are
         expressed in meters. The default is 0.001.
     in_coor_system {Coordinate System}:
         The coordinate system of the input .laz files. This parameter is only
         used for .laz files that do not contain spatial reference information
         in their header or have a .prj file in the same location.
     scene_layer_version {String}:
         The Indexed 3D Scene Layer (I3S) version of the resulting point cloud
         scene layer package. Specifying a version supports backward
         compatibility and allows scene layer packages to be shared with
         earlier versions of ArcGIS.

         * 1.X-The point cloud scene layer package will be supported in all
         ArcGIS clients.

         * 2.X-The point cloud scene layer package will be supported in ArcGIS
         Pro 2.1.2 or later and can be published to ArcGIS Online and ArcGIS
         10.6.1 or later. This is the default.
     target_cloud_connection {Folder}:
         The target cloud connection file (.acs) where the scene layer content
         (.i3sREST) will be output.
     out_name {String}:
         The output name of the scene layer content when output to a cloud
         store. This parameter is only available when a target_cloud_connection
         parameter value is specified.

    OUTPUTS:
     out_slpk {File}:
         The output scene layer package (.slpk)."""
    ...

@gptooldoc("CreatePointSceneLayerPackage_management", None)
def CreatePointSceneLayerPackage(
    in_dataset=...,
    out_slpk=...,
    out_coor_system=...,
    transform_method=...,
    target_cloud_connection=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreatePointSceneLayerPackage_management(in_dataset, {out_slpk}, {out_coor_system}, {transform_method;transform_method...}, {target_cloud_connection})

       Creates a point scene layer package (.slpk) or scene layer content
       (.i3sREST) from a point feature layer.

    INPUTS:
     in_dataset (Layer File / Feature Layer):
         The input point feature layer.
     out_coor_system {Spatial Reference}:
         The coordinate system of the output scene layer package. It
         can be any projected or custom coordinate system. Supported geographic
         coordinate systems include WGS84 and China Geodetic Coordinate System
         2000. WGS84 and EGM96 Geoid are the default horizontal and vertical
         coordinate systems, respectively. The coordinate system can be
         specified in any of the following ways:

         * Specify the path to a .prj file.

         * Reference a dataset with the correct coordinate system.

         * Use an arcpy.SpatialReference object.
     transform_method {String}:
         The datum transformation method that will be used when the input
         layer's coordinate system uses a datum that differs from the output
         coordinate system. All transformations are bidirectional, regardless
         of the direction implied by their names. For example,
         NAD_1927_to_WGS84_3 will work correctly even if the datum conversion
         is from WGS84 to NAD 1927.The ArcGIS coordinate system data is
         required for vertical datum
         transformations between ellipsoidal and gravity-related and two
         gravity-related datums.
     target_cloud_connection {Folder}:
         The target cloud connection file (.acs) where the scene layer content
         (.i3sREST) will be output.

    OUTPUTS:
     out_slpk {File}:
         The output scene layer package (.slpk)."""
    ...

@gptooldoc("CreateVoxelSceneLayerContent_management", None)
def CreateVoxelSceneLayerContent(
    in_dataset=..., out_slpk=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateVoxelSceneLayerContent_management(in_dataset, out_slpk)

       Creates a scene layer package (.slpk file) from a voxel layer input.

    INPUTS:
     in_dataset (Layer File / Voxel Layer):
         The input voxel layer or layer file.

    OUTPUTS:
     out_slpk (File):
         The output scene layer package (.slpk file)."""
    ...

@gptooldoc("UpgradeSceneLayer_management", None)
def UpgradeSceneLayer(
    in_dataset=...,
    out_folder_path=...,
    out_name=...,
    out_log=...,
    texture_optimization=...,
    date_format=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpgradeSceneLayer_management(in_dataset, out_folder_path, out_name, {out_log}, {texture_optimization}, {date_format})

       Upgrades a scene layer package to the current I3S version in SLPK
       format or output to i3sREST format for use in ArcGIS Enterprise.

    INPUTS:
     in_dataset (File):
         The input scene layer package.
     out_folder_path (Folder):
         The location where the output scene layer package will be created or
         the cloud connection file (.acs) to output to i3sREST format.
     out_name (String):
         The name of the output scene layer.
     texture_optimization {String}:
         Specifies the textures that will be optimized according to the
         target platform where the scene layer package is used.
         Optimizations that include KTX2 may take significant time to process.
         For fastest results, use the DESKTOP or NONE options.

         * ALL-All texture formats will be optimized including JPEG, DXT, and
         KTX2 for use in desktop, web, and mobile platforms.

         * DESKTOP-Windows, Linux, and Mac supported textures will be optimized
         including JPEG and DXT for use in ArcGIS Pro clients on Windows and
         ArcGIS Runtime desktop clients on Windows, Linux, and Mac. This is the
         default.

         * MOBILE-Android and iOS supported textures will be optimized
         including JPEG and KTX2 for use in ArcGIS Runtime mobile applications.

         * NONE-JPEG textures will be optimized for use in desktop and web
         platforms.
     date_format {String}:
         The format of the date values in the scene layers date fields. This
         parameter is hidden if no date fields are encountered.

    OUTPUTS:
     out_log {File}:
         The output log file that will summarize the results of the evaluation."""
    ...

@gptooldoc("ValidateSceneLayerPackage_management", None)
def ValidateSceneLayerPackage(
    in_slpk=..., out_report=..., in_folder=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ValidateSceneLayerPackage_management({in_slpk}, out_report, {in_folder})

       Evaluates a scene layer package (*.slpk or *.i3sREST) in a cloud store
       to determine its conformity to I3S specifications.

    INPUTS:
     in_slpk {File}:
         The scene layer package (*.slpk) that will be evaluated.
     in_folder {Folder}:
         The scene layer content (*.i3sREST) in a cloud store that will be
         evaluated.

    OUTPUTS:
     out_report (File):
         The output log file that will summarize the results of the evaluation."""
    ...

@gptooldoc("GeoTaggedPhotosToPoints_management", None)
def GeoTaggedPhotosToPoints(
    Input_Folder=...,
    Output_Feature_Class=...,
    Invalid_Photos_Table=...,
    Include_Non_GeoTagged_Photos=...,
    Add_Photos_As_Attachments=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeoTaggedPhotosToPoints_management(Input_Folder, Output_Feature_Class, {Invalid_Photos_Table}, {Include_Non_GeoTagged_Photos}, {Add_Photos_As_Attachments})

       Creates points from the x-, y-, and z-coordinates stored in the
       metadata of geotagged photo files (.jpg or .tif). You can add the
       photo files to the output features as geodatabase attachments.

    INPUTS:
     Input_Folder (Folder):
         The folder where photo files (.jpg or .tif) are located. This folder
         is scanned recursively for photo files; any photos in the base level
         of the folder, as well as in any subfolders, will be added to the
         output.
     Include_Non-GeoTagged_Photos {Boolean}:
         Specifies whether all photo files will be included in the output
         feature class or only those with valid coordinates.

         * ALL_PHOTOS-All photos will be included as records in the output
         feature class. If a photo file does not have coordinate information,
         it will be included as a feature with null geometry. This is the
         default.

         * ONLY_GEOTAGGED-Only photos with valid coordinate information will
         be included in the output feature class.
     Add_Photos_As_Attachments {Boolean}:
         Specifies whether the input photos will be added to the output
         features as geodatabase attachments.Adding attachments requires an
         ArcGIS Desktop Standard or ArcGIS
         Desktop Advanced license, and the output feature class must be in a
         version 10 or later geodatabase.

         * ADD_ATTACHMENTS-Photos will be added to the output features as
         geodatabase attachments copied internally to the geodatabase. This is
         the default.

         * NO_ATTACHMENTS-Photos will not be added to the output features as
         geodatabase attachments.

    OUTPUTS:
     Output_Feature_Class (Feature Class):
         The output point feature class.
     Invalid_Photos_Table {Table}:
         The path to an optional output table that will list any photo files in
         the input folder with invalid Exif metadata or empty or invalid
         coordinates.If no path is specified, the table will not be created."""
    ...

@gptooldoc("MatchPhotosToRowsByTime_management", None)
def MatchPhotosToRowsByTime(
    Input_Folder=...,
    Input_Table=...,
    Time_Field=...,
    Output_Table=...,
    Unmatched_Photos_Table=...,
    Add_Photos_As_Attachments=...,
    Time_Tolerance=...,
    Clock_Offset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MatchPhotosToRowsByTime_management(Input_Folder, Input_Table, Time_Field, Output_Table, {Unmatched_Photos_Table}, {Add_Photos_As_Attachments}, {Time_Tolerance}, {Clock_Offset})

       Matches photo files to table or feature class rows according to the
       photo and row time stamps. The row with the time stamp closest to the
       capture time of a photo will be matched to that photo. Creates a new
       table containing the ObjectIDs from the input rows and their matching
       photo paths. Optionally adds matching photo files to the rows of the
       input table as geodatabase attachments.

    INPUTS:
     Input_Folder (Folder):
         The folder where photo files (.jpg or .tif) are located. This folder
         is scanned recursively for photo files; any photos in the base level
         of the folder, as well as in any subfolders, will be added to the
         output.
     Input_Table (Table View):
         The table or feature class whose rows will be matched with photo
         files. The input table will typically be a point feature class
         representing GPS recordings.
     Time_Field (Field):
         The date/time field from the input table that indicates when each row
         was captured or created. Must be a date field; cannot be a string or
         numeric field.
     Add_Photos_As_Attachments {Boolean}:
         Specifies if photo files will be added to the rows of the
         input table as geodatabase attachments. Adding attachments
         requires at minimum an ArcGIS Desktop Standard
         license, and the output feature class must be in a version 10 or
         higher geodatabase.

         * ADD_ATTACHMENTS-Photo files will be added to the rows of the input
         table as geodatabase attachments. Geodatabase attachments are copied
         internally to the geodatabase. This is the default.

         * NO_ATTACHMENTS-Photo files will not be added to the rows of the
         input table as geodatabase attachments.
     Time_Tolerance {Double}:
         The maximum difference (in seconds) between the date/time of an input
         row and a photo file that will be matched. If an input row and a photo
         file have time stamps that are different by more than this tolerance,
         no match will occur. To match a photo file to a row with the closest
         time stamp, regardless of how large the date/time difference might be,
         set the tolerance to 0. The sign of this value (- or +) is irrelevant;
         the absolute value of the number specified will be used.Do not use
         this parameter to adjust for consistent shifts or offsets
         between the times recorded by the GPS and the digital camera. Use the
         Clock Offset parameter, or the Convert Time Zone tool to shift the
         time stamps of the input rows to match those of the photos.
     Clock_Offset {Double}:
         The difference (in seconds) between the internal clock of the digital
         camera used to capture the photos and the GPS unit. If the clock of
         the digital camera is behind the clock of the GPS unit, use a positive
         value; if the clock of the digital camera is ahead of the clock of the
         GPS unit, use a negative value.For example, if a photo with a time
         stamp of 11:35:17 should match a
         row with a time stamp of 11:35:32, use a Clock Offset of 15.

    OUTPUTS:
     Output_Table (Table):
         The output table containing the OBJECTIDs from the input table that
         match a photo, and the matching photo path. Only OBJECTIDs from the
         input table that are found to match a photo are included in the output
         table.
     Unmatched_Photos_Table {Table}:
         The optional output table that will list any photo files in the input
         folder with an invalid time stamp or any photos that cannot be matched
         because there is no input row within the time tolerance.If no path is
         specified, this table will not be created."""
    ...

@gptooldoc("BatchProject_management", None)
def BatchProject(
    Input_Feature_Class_or_Dataset=...,
    Output_Workspace=...,
    Output_Coordinate_System=...,
    Template_dataset=...,
    Transformation=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BatchProject_management(Input_Feature_Class_or_Dataset;Input_Feature_Class_or_Dataset..., Output_Workspace, {Output_Coordinate_System}, {Template_dataset}, {Transformation})

       Changes the coordinate system of a set of input feature classes or
       feature datasets to a common coordinate system. To change the
       coordinate system of a single feature class or dataset use the Project
       tool.

    INPUTS:
     Input_Feature_Class_or_Dataset (Feature Layer / Feature Dataset):
         The input feature classes or feature datasets whose coordinates are to
         be converted.
     Output_Workspace (Workspace / Feature Dataset):
         The location of each new output feature class or feature dataset.
     Output_Coordinate_System {Coordinate System}:
         The coordinate system to be used to project the inputs.Valid values
         are a SpatialReference object, a file with a .prj
         extension, or a string representation of a coordinate system.
     Template_dataset {Geodataset}:
         The feature class or the feature dataset used to specify the output
         coordinate system used for projection.
     Transformation {String}:
         The name of the geographic transformation to be applied to convert
         data between two geographic coordinate systems (datums)."""
    ...

@gptooldoc("ConvertCoordinateNotation_management", None)
def ConvertCoordinateNotation(
    in_table=...,
    out_featureclass=...,
    x_field=...,
    y_field=...,
    input_coordinate_format=...,
    output_coordinate_format=...,
    id_field=...,
    spatial_reference=...,
    in_coor_system=...,
    exclude_invalid_records=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConvertCoordinateNotation_management(in_table, out_featureclass, x_field, y_field, input_coordinate_format, output_coordinate_format, {id_field}, {spatial_reference}, {in_coor_system}, {exclude_invalid_records})

       Converts coordinate notations contained in one or two fields from one
       notation format to another.

    INPUTS:
     in_table (Table View):
         The input table or text file. Point features are also valid.
     x_field (Field):
         A field from the input table containing the longitude value.For the
         input_coordinate_format parameter's DD_2, DD_NUMERIC, DDM_2,
         and DMS_2 options, this is the longitude field.For the DD_1, DDM_1,
         and DMS_1 options, this field contains both
         latitude and longitude values in a single string.For the GARS, GEOREF,
         GEOREF16, UTM_ZONES, UTM_BANDS, USNG, USNG16,
         MGRS, and MGRS16 options, this field contains an alphanumeric system
         of notation in a single text field.
     y_field (Field):
         A field from the input table containing the latitude value.For the
         input_coordinate_format parameter's DD_2, DD_NUMERIC, DDM_2,
         and DMS_2 options, this is the longitude field.This parameter is
         ignored when one of the single-string formats is
         chosen.
     input_coordinate_format (String):
         Specifies the coordinate format of the input fields.

         * DD_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DD_2-Longitude and latitude values are in two separate fields.This
         is the default.

         * DDM_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DDM_2-Longitude and latitude values are in two separate fields.

         * DMS_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DMS_2-Longitude and latitude values are in two separate fields.

         * GARS-Global Area Reference System. Based on latitude and longitude,
         it divides and subdivides the world into cells.

         * GEOREF-World Geographic Reference System. A grid-based system that
         divides the world into 15-degree quadrangles and then subdivides into
         smaller quadrangles.

         * GEOREF16-World Geographic Reference System in 16-digit precision.

         * UTM_ZONES-The letter N or S after the UTM zone number designates
         only North or South hemisphere.

         * UTM_BANDS-The letter after the UTM zone number designates one of the
         20 latitude bands. N or S does not designate a hemisphere.

         * USNG-United States National Grid. Almost exactly the same as MGRS
         but uses North American Datum 1983 (NAD83) as its datum.

         * USNG16-United States National Grid in 16-digit higher precision.

         * MGRS-Military Grid Reference System. Follows the UTM coordinates and
         divides the world into 6-degree longitude and 20 latitude bands, but
         MGRS then further subdivides the grid zones into smaller 100,000-meter
         grids. These 100,000-meter grids are then divided into 10,000-meter,
         1,000-meter, 100-meter, 10-meter, and 1-meter grids.

         * MGRS16-Military Grid Reference System in 16-digit precision.

         * SHAPE-Only available when a point feature layer is selected as
         input. The coordinates of each point are used to define the output
         format.
         DD, DDM, DMS, and UTM are also valid keywords; they can be used just
         by typing in (on dialog) or passing the value in scripting. However,
         keywords with underscore and a qualifier tell more about the field
         values.
     output_coordinate_format (String):
         Specifies the coordinate format to which the input notations will be
         converted.

         * DD_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DD_2-Longitude and latitude values are in two separate fields.

         * DD_NUMERIC-Longitude and latitude values are in two separate fields
         of type Double. Values in the West and South are denoted by a minus
         sign.

         * DDM_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DDM_2-Longitude and latitude values are in two separate fields.

         * DMS_1-Both longitude and latitude values are in a single field. Two
         values are separated by a space, a comma, or a slash.

         * DMS_2-Longitude and latitude values are in two separate fields.

         * GARS-Global Area Reference System. Based on latitude and longitude,
         it divides and subdivides the world into cells.

         * GEOREF-World Geographic Reference System. A grid-based system that
         divides the world into 15-degree quadrangles and then subdivides into
         smaller quadrangles.

         * GEOREF16-World Geographic Reference System in 16-digit precision.

         * UTM_ZONES-The letter N or S after the UTM zone number designates
         only North or South hemisphere.

         * UTM_BANDS-The letter after the UTM zone number designates one of the
         20 latitude bands. N or S does not designate a hemisphere.

         * USNG-United States National Grid. Almost exactly the same as MGRS
         but uses North American Datum 1983 (NAD83) as its datum.

         * USNG16-United States National Grid in 16-digit higher precision.

         * MGRS-Military Grid Reference System. Follows the UTM coordinates and
         divides the world into 6-degree longitude and 20 latitude bands, but
         MGRS then further subdivides the grid zones into smaller 100,000-meter
         grids. These 100,000-meter grids are then divided into 10,000-meter,
         1,000-meter, 100-meter, 10-meter, and 1-meter grids.

         * MGRS16-Military Grid Reference System in 16-digit precision.
         DD, DDM, DMS, and UTM are also valid keywords; they can be used just
         by typing in (on dialog) or passing the value in scripting. However,
         keywords with underscore and a qualifier tell more about the field
         values.
     id_field {Field}:
         This parameter is ignored, as all fields are transferred to output
         table.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature class. The default is
         GCS_WGS_1984.The tool projects the output to the spatial reference
         specified. If
         the input and output coordinate systems are in a different datum, a
         default transformation will be used based on the coordinate systems of
         the input and the output and the extent of the data.
     in_coor_system {Coordinate System}:
         The spatial reference of the input data. If the input spatial
         reference cannot be obtained from the input table, a default of
         GCS_WGS_1984 will be used.
     exclude_invalid_records {Boolean}:
         Specifies whether to exclude records with invalid notation.

         * EXCLUDE_INVALID-Invalid records will be excluded and only valid
         records will be converted to points in the output. This is the
         default.

         * INCLUDE_INVALID-Valid records will be converted to points in the
         output and invalid records will be included as null geometry.

    OUTPUTS:
     out_featureclass (Feature Class):
         The output point feature class. The attribute table will contain all
         fields of the input table along with the fields containing converted
         values in the output format."""
    ...

@gptooldoc("CreateCustomGeoTransformation_management", None)
def CreateCustomGeoTransformation(
    geot_name=..., in_coor_system=..., out_coor_system=..., custom_geot=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateCustomGeoTransformation_management(geot_name, in_coor_system, out_coor_system, custom_geot)

       Creates a transformation method for converting data between two
       geographic coordinate systems or datums. The output of this tool can
       be used as a transformation method for any tool with a parameter that
       requires a geographic transformation.

    INPUTS:
     geot_name (String):
         Name of the custom transformation method. All custom geographic
         transformation files are saved with a
         .gtf extension and stored in the ESRI\\<ArcGIS
         product>\\ArcToolbox\\CustomTransformations folder in the user's
         Application Data folder. The CustomTransformations folder is created
         by the tool if it does not exist. If the Application Data folder is
         read-only or hidden, the output is created in
         ArcToolbox\\CustomTransformations in the user's temp folder. The
         location or name of the Application Data and temp folders is dependent
         on the operating system.

         * In any Windows operating system the Application Data folder is
         located in %appdata% and a user's Temp folder is located in %temp%.
         Entering %appdata% in a command window returns the Application Data
         folder location. Entering %temp% returns the temp folder location.

         * In Unix systems, the tmp and Application Data folders are located in
         the user's home directory, under $HOME and $TMP, respectively. Typing
         /tmp in a terminal returns the location.
     in_coor_system (Coordinate System):
         The starting geographic coordinate system.
     custom_geot (String):
         Set the METHOD and PARAMETER values wrapped in a string for custom
         transformation GEOGTRAN. Set the name of the method from the available
         methods of Geocentric_Translation, Molodensky, Molodensky_Abridged,
         Position_Vector, Coordinate_Frame, Molodensky_Badekas, NADCON, HARN,
         NTV2, Longitude_Rotation, Unit_Change, and Geographic_2D_Offset. Each
         method has its own sets of parameters-you can edit the values of the
         parameters by entering text next to the name of the parameter within
         the whole string representation of the custom geographic
         transformation. See examples in the Python sample below.

    OUTPUTS:
     out_coor_system (Coordinate System):
         The final geographic coordinate system."""
    ...

@gptooldoc("CreateSpatialReference_management", None)
def CreateSpatialReference(
    spatial_reference=...,
    spatial_reference_template=...,
    xy_domain=...,
    z_domain=...,
    m_domain=...,
    template=...,
    expand_ratio=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateSpatialReference_management({spatial_reference}, {spatial_reference_template}, {xy_domain}, {z_domain}, {m_domain}, {template;template...}, {expand_ratio})

       Creates a spatial reference for use in ModelBuilder.

    INPUTS:
     spatial_reference {Spatial Reference}:
         The name of the spatial reference to be created.
     spatial_reference_template {Feature Layer / Raster Dataset}:
         The feature class or layer to be used as a template to set the value
         for the spatial reference.
     xy_domain {Envelope}:
         The allowable coordinate range for x,y coordinates.
     z_domain {String}:
         The allowable coordinate range for z-values.
     m_domain {String}:
         The allowable coordinate range for m-values.
     template {Feature Layer}:
         The feature classes or layers that can be used to define the XY
         Domain.
     expand_ratio {Double}:
         The percentage by which the XY Domain will be expanded."""
    ...

@gptooldoc("DefineProjection_management", None)
def DefineProjection(
    in_dataset=..., coor_system=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DefineProjection_management(in_dataset, coor_system)

       Overwrites the coordinate system information (map projection and
       datum) stored with a dataset. This tool is intended for datasets that
       have an unknown or incorrect coordinate system defined.

    INPUTS:
     in_dataset (Feature Layer / Geodataset):
         The dataset or feature class whose projection will be defined.
     coor_system (Coordinate System):
         The coordinate system that will be applied to the input.Valid values
         are a SpatialReference object, a file with a .prj
         extension, or a string representation of a coordinate system."""
    ...

@gptooldoc("Project_management", None)
def Project(
    in_dataset=...,
    out_dataset=...,
    out_coor_system=...,
    transform_method=...,
    in_coor_system=...,
    preserve_shape=...,
    max_deviation=...,
    vertical=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Project_management(in_dataset, out_dataset, out_coor_system, {transform_method;transform_method...}, {in_coor_system}, {preserve_shape}, {max_deviation}, {vertical})

       Projects spatial data from one coordinate system to another.

    INPUTS:
     in_dataset (Feature Layer / Feature Dataset / Scene Layer / Building Scene Layer / File):
         The feature class, feature layer, feature dataset, scene layer, or
         scene layer package to be projected.
     out_coor_system (Coordinate System):
         Valid values are a SpatialReference object, a file with a .prj
         extension, or a string representation of a coordinate system.
     transform_method {String}:
         This method can be used to convert data between two geographic
         coordinate systems or datums. This optional parameter may be required
         if the input and output coordinate systems have different datums.To
         get a list of valid transformations, use the
         arcpy.ListTransformations method. The most appropriate transformation
         is usually the first one in the returned list. The list is sorted by
         amount of overlap of the data versus the areas of use of the
         transformations. If two or more transformations have the same amount
         of overlap with the data, the transformation accuracy values are used
         as a secondary sort parameter.Transformations are bidirectional. For
         example, if you're converting
         data from WGS 1984 to NAD 1927, you can choose a transformation called
         NAD_1927_to_WGS_1984_3, and the tool will apply it correctly. If no
         transformation is provided, a default transformation is used. This
         default transformation is suitable for general mapping applications
         but may not be suitable for applications that require precise
         locational accuracy.
     in_coor_system {Coordinate System}:
         The coordinate system of the input feature class or dataset. When the
         input has an unknown or unspecified coordinate system, you can specify
         the data's coordinate system without having to modify the input data
         (which may not be possible if the input is in read-only format).
     preserve_shape {Boolean}:
         Specifies whether vertices will be added to the output lines or
         polygons so their projected shape is more accurate.

         * NO_PRESERVE_SHAPE-Extra vertices will not be added to the output
         lines or polygons. This is the default.

         * PRESERVE_SHAPE-Extra vertices will be added to the output lines or
         polygons as needed, so their projected shape is more accurate.
     max_deviation {Linear Unit}:
         The distance a projected line or polygon can deviate from its exact
         projected location when the preserve_shape parameter is set to
         PRESERVE_SHAPE. The default is 100 times the x,y tolerance of the
         spatial reference of the output dataset.
     vertical {Boolean}:
         Specifies whether a vertical transformation will be applied.This
         parameter is only enabled when the input and output coordinate
         systems have a vertical coordinate system and the input feature class
         coordinates have z-values. Also, many vertical transformations require
         additional data files that must be installed using the ArcGIS
         Coordinate Systems Data installation package.This parameter is not
         compatible with the preserve_shape parameter.

         * NO_VERTICAL-A vertical transformation will not be applied. The
         z-values of geometry coordinates will be ignored and the z-values will
         not be modified. This is the default.

         * VERTICAL-The transformation specified in the transform_method
         parameter will be applied. The Project tool transforms x-, y-, and
         z-values of geometry coordinates.

    OUTPUTS:
     out_dataset (Feature Class / Feature Dataset / File):
         The output dataset to which the results will be written."""
    ...

@gptooldoc("Flip_management", None)
def Flip(
    in_raster=..., out_raster=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Flip_management(in_raster, out_raster)

       Reorients the raster by turning it over, from top to bottom, along the
       horizontal axis through the center of the raster. This may be useful
       to correct raster datasets that are upside down.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The input raster dataset.

    OUTPUTS:
     out_raster (Raster Dataset):
         The output raster dataset.When storing the raster dataset in a file
         format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("Mirror_management", None)
def Mirror(
    in_raster=..., out_raster=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Mirror_management(in_raster, out_raster)

       Reorients the raster by turning it over, from left to right, along the
       vertical axis through the center of the raster.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The input raster dataset.

    OUTPUTS:
     out_raster (Raster Dataset):
         The output raster dataset.When storing the raster dataset in a file
         format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("ProjectRaster_management", None)
def ProjectRaster(
    in_raster=...,
    out_raster=...,
    out_coor_system=...,
    resampling_type=...,
    cell_size=...,
    geographic_transform=...,
    Registration_Point=...,
    in_coor_system=...,
    vertical=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ProjectRaster_management(in_raster, out_raster, out_coor_system, {resampling_type}, {cell_size}, {geographic_transform;geographic_transform...}, {Registration_Point}, {in_coor_system}, {vertical})

       Transforms a raster dataset from one coordinate system to another.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The raster dataset that will be transformed into a new projection.
     out_coor_system (Coordinate System):
         The coordinate system of the new raster dataset. Valid values
         for this parameter are the following:

         * An existing feature class, feature dataset, raster dataset
         (basically anything with a coordinate system)

         * An ArcPy SpatialReference object
     resampling_type {String}:
         Specifies the resampling technique that will be used. The default is
         Nearest.

         * NEAREST-The nearest neighbor technique will be used. It minimizes
         changes to pixel values since no new values are created and is the
         fastest resampling technique. It is suitable for discrete data, such
         as land cover.

         * BILINEAR-The bilinear interpolation technique will be used. It
         calculates the value of each pixel by averaging (weighted for
         distance) the values of the surrounding four pixels. It is suitable
         for continuous data.

         * CUBIC-The cubic convolution technique will be used. It calculates
         the value of each pixel by fitting a smooth curve based on the
         surrounding 16 pixels. This produces the smoothest image but can
         create values outside of the range found in the source data. It is
         suitable for continuous data.

         * MAJORITY-The majority resampling technique will be used. It
         determines the value of each pixel based on the most popular value in
         a 4 by 4 window. It is suitable for discrete data.
         The Nearest and Majority options are used for categorical data, such
         as a land-use classification. The Nearest option is the default. It is
         the quickest and does not change the pixel values. Do not use either
         of these options for continuous data, such as elevation surfaces.The
         Bilinear and Cubic options are most appropriate for continuous
         data. It is recommended that you do not use either of these options
         with categorical data because the pixel values may be altered.
     cell_size {Cell Size XY}:
         The cell size of the new raster using an existing raster dataset or by
         specifying its width (x) and height (y).
     geographic_transform {String}:
         The geographic transformation when projecting from one geographic
         system or datum to another. A transformation is required when the
         input and output coordinate systems have different datums.
     Registration_Point {Point}:
         The lower left point for anchoring the output cells. This point does
         not have to be a corner coordinate or fall within the raster
         dataset.The Snap Raster Environment setting will take priority over
         the
         Registration Point parameter. To set the registration point, make sure
         Snap Raster is not set.
     in_coor_system {Coordinate System}:
         The coordinate system of the input raster dataset.
     vertical {Boolean}:
         Specifies whether a vertical transformation will be performed.This
         parameter is only enabled when the input and output coordinate
         systems have a vertical coordinate system and the input feature class
         coordinates have z-values.When the VERTICAL keyword is used, the
         geographic_transform parameter
         can include ellipsoidal transformations and transformations between
         vertical datums. For example,
         “~NAD_1983_To_NAVD88_CONUS_GEOID12B_Height + NAD_1983_To_WGS_1984_1”
         transforms geometry vertices that are defined on NAD 1983 datum with
         NAVD 1988 heights into vertices on the WGS 1984 ellipsoid (with
         z-values representing ellipsoidal heights). The tilde (~) indicates
         reversed direction of transformation.

         * NO_VERTICAL-No vertical transformation is applied. The z-values of
         geometry coordinates will be ignored and the z-values will not be
         modified. This is the default.

         * VERTICAL-The transformation specified in the geographic_transform
         parameter is applied. The Project Raster tool transforms x-, y-, and
         z-values of geometry coordinates.
         Many vertical transformations require additional data files that must
         be installed using the ArcGIS Coordinate Systems Data installation
         package.

    OUTPUTS:
     out_raster (Raster Dataset):
         The raster dataset with the new projection that will be created.When
         storing the raster dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("RegisterRaster_management", None)
def RegisterRaster(
    in_raster=...,
    register_mode=...,
    reference_raster=...,
    input_link_file=...,
    transformation_type=...,
    output_cpt_link_file=...,
    maximum_rms_value=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RegisterRaster_management(in_raster, register_mode, {reference_raster}, {input_link_file}, {transformation_type}, {output_cpt_link_file}, {maximum_rms_value})

       Automatically aligns a raster to a reference image or uses a control
       point file for georegistration. If the input dataset is a mosaic
       dataset, the tool will operate on each mosaic dataset item. To
       automatically register the image, the input raster and the reference
       raster must be in a relatively close geographic area. The tool will
       run faster if the raster datasets are in close alignment. You may need
       to create a link file, also known as a control point file, with a few
       links to get your input raster into the same map space.

    INPUTS:
     in_raster (Raster Dataset / Raster Layer / Mosaic Layer):
         The raster that you want to realign. Registering a mosaic dataset item
         will update that particular item within the mosaic dataset.A mosaic
         dataset item will have the path to the mosaic dataset
         followed by the Object ID of the item. For example, the first item in
         the mosaic dataset would have the following path:
         .\\mosaicDataset\\objectid=1.
     register_mode (String):
         Specifies the registration mode. You can either register the raster
         with a transformation or reset the transformation.

         * REGISTER-Apply a geometric transformation to the input raster.

         * REGISTER_MS-Register the multispectral input to the panchromatic
         input. This is only used for mosaic datasets that have a misalignment
         between the two.

         * RESET-Remove the geometric transformation previously added by this
         tool.

         * CREATE_LINKS-Create a link file with automatically generated links.
     reference_raster {Raster Dataset / Raster Layer / Image Service / Map Server / WMS Map / Mosaic Layer / Internet Tiled Layer / Map Server Layer}:
         The raster dataset that will align the input raster dataset. Leave
         this parameter empty if you want to register your multispectral mosaic
         dataset items to their associated panchromatic raster datasets.
     input_link_file {Feature Class / Text File}:
         The file that has the coordinates to link the input raster dataset
         with the reference. The input link table works with one mosaic item in
         the mosaic layer. The input must specify which item is being
         processed, either selecting the item or specifying the ObjectID in the
         input. Leave this parameter empty to register multispectral mosaic
         dataset items with the associated panchromatic raster datasets.
     transformation_type {String}:
         Specifies the method for shifting the raster dataset.

         * POLYORDER0-This method uses a zero-order polynomial to shift your
         data. This is commonly used when your data is already georeferenced,
         but a small shift will better line up your data. Only one link is
         required to perform a zero-order polynomial shift.

         * POLYSIMILARITY-This is a first-order transformation that attempts
         to preserve the shape of the original raster. The RMS error tends to
         be higher than other polynomial transformations because the
         preservation of shape is more important than the best fit.

         * POLYORDER1-A first-order polynomial (affine) fits a flat plane to
         the input points.

         * POLYORDER2-A second-order polynomial fits a somewhat more
         complicated surface to the input points.

         * POLYORDER3-A third-order polynomial fits a more complicated surface
         to the input points.

         * ADJUST-This method combines a polynomial transformation and uses a
         triangulated irregular network (TIN) interpolation technique to
         optimize for both global and local accuracy.

         * SPLINE-This method transforms the source control points precisely
         to the target control points. In the output, the control points will
         be accurate, but the raster pixels between the control points are not.

         * PROJECTIVE-This method warps lines so they remain straight. In
         doing so, lines that were once parallel may no longer remain parallel.
         The projective transformation is especially useful for oblique
         imagery, scanned maps, and for some imagery products.

         * FRAME-This method uses an image resection algorithm on aerial
         images. The image resection algorithm refines the exterior orientation
         (perspective, omega, phi, and kappa) of the image from known ground
         control points, using a least-square fitting method. Each image must
         have at least three noncollinear points. When the input is a mosaic
         dataset, it will register the selected images one at a time.
     maximum_rms_value {Double}:
         The amount of modeled error (in pixels) that you want in the output.
         The default is 0.5, and values below 0.3 are not recommended as this
         leads to overfitting.

    OUTPUTS:
     output_cpt_link_file {Text File}:
         If specified, a text file will be written containing the links created
         by this tool. This file can be used in the Warp From File tool. The
         output link table works with one mosaic dataset item in the mosaic
         layer. The input must specify which item is being processed, either
         selecting the item or specifying the ObjectID in the input."""
    ...

@gptooldoc("Rescale_management", None)
def Rescale(
    in_raster=..., out_raster=..., x_scale=..., y_scale=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Rescale_management(in_raster, out_raster, x_scale, y_scale)

       Resizes a raster by the specified x and y scale factors.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The input raster.
     x_scale (Double):
         The factor by which to scale the cell size in the x direction.The
         factor must be greater than zero.
     y_scale (Double):
         The factor by which to scale the cell size in the y direction.The
         factor must be greater than zero.

    OUTPUTS:
     out_raster (Raster Dataset):
         The output raster dataset.When storing the raster dataset in a file
         format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("Rotate_management", None)
def Rotate(
    in_raster=...,
    out_raster=...,
    angle=...,
    pivot_point=...,
    resampling_type=...,
    clipping_extent=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Rotate_management(in_raster, out_raster, angle, {pivot_point}, {resampling_type}, {clipping_extent})

       Turns a raster dataset around a specified pivot point.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The raster dataset to rotate.
     angle (Double):
         Specify a value between 0 and 360 degrees the raster will be rotated
         in the clockwise direction. To rotate the raster in the
         counterclockwise direction, specify the angle as a negative value. The
         angle can be specified as an integer or a floating-point value.
     pivot_point {Point}:
         The point the raster will rotate around. If left blank, the lower left
         corner of the input raster dataset will serve as the pivot.
     resampling_type {String}:
         Specifies the resampling technique that will be used. The default is
         Nearest.

         * NEAREST-The nearest neighbor technique will be used. It minimizes
         changes to pixel values since no new values are created and is the
         fastest resampling technique. It is suitable for discrete data, such
         as land cover.

         * BILINEAR-The bilinear interpolation technique will be used. It
         calculates the value of each pixel by averaging (weighted for
         distance) the values of the surrounding four pixels. It is suitable
         for continuous data.

         * CUBIC-The cubic convolution technique will be used. It calculates
         the value of each pixel by fitting a smooth curve based on the
         surrounding 16 pixels. This produces the smoothest image but can
         create values outside of the range found in the source data. It is
         suitable for continuous data.

         * MAJORITY-The majority resampling technique will be used. It
         determines the value of each pixel based on the most popular value in
         a 4 by 4 window. It is suitable for discrete data.
         The Nearest and Majority options are used for categorical data, such
         as a land-use classification. The Nearest option is the default. It is
         the quickest and does not change the pixel values. Do not use either
         of these options for continuous data, such as elevation surfaces.The
         Bilinear and Cubic options are most appropriate for continuous
         data. It is recommended that you do not use either of these options
         with categorical data because the pixel values may be altered.
     clipping_extent {Extent}:
         The processing extent of the raster dataset. The source data will be
         clipped to the specified extent before rotation.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format for the dataset you are creating. When
         storing a raster dataset in a geodatabase, do not add a file extension
         to the name of the raster dataset. When storing your raster dataset to
         a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can
         specify a compression type and compression quality.When storing the
         raster dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid"""
    ...

@gptooldoc("Shift_management", None)
def Shift(
    in_raster=..., out_raster=..., x_value=..., y_value=..., in_snap_raster=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Shift_management(in_raster, out_raster, x_value, y_value, {in_snap_raster})

       Moves (slides) the raster to a new geographic location based on x and
       y shift values. This tool is helpful if your raster dataset needs to
       be shifted to align with another data file.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The input raster dataset.
     x_value (Double):
         The value used to shift the x-coordinates.
     y_value (Double):
         The value used to shift the y-coordinates.
     in_snap_raster {Raster Layer}:
         The raster dataset used to align the cells of the output raster
         dataset.

    OUTPUTS:
     out_raster (Raster Dataset):
         The output raster dataset.When storing the raster dataset in a file
         format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("Warp_management", None)
def Warp(
    in_raster=...,
    source_control_points=...,
    target_control_points=...,
    out_raster=...,
    transformation_type=...,
    resampling_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Warp_management(in_raster, source_control_points;source_control_points..., target_control_points;target_control_points..., out_raster, {transformation_type}, {resampling_type})

       Transforms a raster dataset using source and target control points.
       This is similar to georeferencing.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The raster to be transformed.
     source_control_points (Point):
         The coordinates of the raster to be warped.
     target_control_points (Point):
         The coordinates to which the source raster will be warped.
     transformation_type {String}:
         Specifies the transformation method for shifting the raster dataset.

         * POLYORDER0-A zero-order polynomial will be used to shift the data.
         This is commonly used when the data is georeferenced, but a small
         shift will better line it up. Only one link is required to perform a
         zero-order polynomial shift.

         * POLYSIMILARITY-A first order transformation will be used that
         attempts to preserve the shape of the original raster. The RMS error
         tends to be higher than other polynomial transformations because the
         preservation of shape is more important than the best fit.

         * POLYORDER1-A first-order polynomial (affine) will be used to fit a
         flat plane to the input points.

         * POLYORDER2-A second-order polynomial will be used to fit a somewhat
         more complicated surface to the input points.

         * POLYORDER3-A third-order polynomial will be used to fit a more
         complicated surface to the input points.

         * ADJUST-A polynomial transformation is combined with a triangulated
         irregular network (TIN) interpolation technique that will optimize for
         both global and local accuracy.

         * SPLINE-The source control points will be transformed precisely to
         the target control points. In the output, the control points will be
         accurate, but the raster pixels between the control points will not.

         * PROJECTIVE-Lines will be warped so that they remain straight. Lines
         that were once parallel may no longer remain parallel. The projective
         transformation is especially useful for oblique imagery, scanned maps,
         and for some imagery products.
     resampling_type {String}:
         Specifies the resampling technique that will be used. The default is
         Nearest.

         * NEAREST-The nearest neighbor technique will be used. It minimizes
         changes to pixel values since no new values are created and is the
         fastest resampling technique. It is suitable for discrete data, such
         as land cover.

         * BILINEAR-The bilinear interpolation technique will be used. It
         calculates the value of each pixel by averaging (weighted for
         distance) the values of the surrounding four pixels. It is suitable
         for continuous data.

         * CUBIC-The cubic convolution technique will be used. It calculates
         the value of each pixel by fitting a smooth curve based on the
         surrounding 16 pixels. This produces the smoothest image but can
         create values outside of the range found in the source data. It is
         suitable for continuous data.

         * MAJORITY-The majority resampling technique will be used. It
         determines the value of each pixel based on the most popular value in
         a 4 by 4 window. It is suitable for discrete data.
         The Nearest and Majority options are used for categorical data, such
         as a land-use classification. The Nearest option is the default. It is
         the quickest and does not change the pixel values. Do not use either
         of these options for continuous data, such as elevation surfaces.The
         Bilinear and Cubic options are most appropriate for continuous
         data. It is recommended that you do not use either of these options
         with categorical data because the pixel values may be altered.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format for the dataset you are creating. When
         storing a raster dataset in a geodatabase, do not add a file extension
         to the name of the raster dataset. When storing your raster dataset to
         a JPEG file, JPEG 2000 file, TIFF file, or geodatabase, you can
         specify a compression type and compression quality.When storing the
         raster dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid"""
    ...

@gptooldoc("WarpFromFile_management", None)
def WarpFromFile(
    in_raster=...,
    out_raster=...,
    link_file=...,
    transformation_type=...,
    resampling_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """WarpFromFile_management(in_raster, out_raster, link_file, {transformation_type}, {resampling_type})

       Transforms a raster dataset using an existing text file containing
       source and target control points.

    INPUTS:
     in_raster (Raster Layer / Mosaic Layer):
         The raster to be transformed.
     link_file (Text File):
         The text, CSV file, or TAB file containing the coordinates to warp the
         input raster. This can be generated from the Register Raster tool or
         from the Georeferencing tab.
     transformation_type {String}:
         Specifies the transformation method for shifting the raster dataset.

         * POLYORDER0-A zero-order polynomial will be used to shift the data.
         This is commonly used when the data is georeferenced, but a small
         shift will better line it up. Only one link is required to perform a
         zero-order polynomial shift.

         * POLYSIMILARITY-A first order transformation will be used that
         attempts to preserve the shape of the original raster. The RMS error
         tends to be higher than other polynomial transformations because the
         preservation of shape is more important than the best fit.

         * POLYORDER1-A first-order polynomial (affine) will be used to fit a
         flat plane to the input points.

         * POLYORDER2-A second-order polynomial will be used to fit a somewhat
         more complicated surface to the input points.

         * POLYORDER3-A third-order polynomial will be used to fit a more
         complicated surface to the input points.

         * ADJUST-A polynomial transformation is combined with a triangulated
         irregular network (TIN) interpolation technique that will optimize for
         both global and local accuracy.

         * SPLINE-The source control points will be transformed precisely to
         the target control points. In the output, the control points will be
         accurate, but the raster pixels between the control points will not.

         * PROJECTIVE-Lines will be warped so that they remain straight. Lines
         that were once parallel may no longer remain parallel. The projective
         transformation is especially useful for oblique imagery, scanned maps,
         and for some imagery products.
     resampling_type {String}:
         The resampling algorithm to be used.

         * NEAREST-The nearest neighbor technique will be used. It minimizes
         changes to pixel values since no new values are created and is the
         fastest resampling technique. It is suitable for discrete data, such
         as land cover.

         * BILINEAR-The bilinear interpolation technique will be used. It
         calculates the value of each pixel by averaging (weighted for
         distance) the values of the surrounding four pixels. It is suitable
         for continuous data.

         * CUBIC-The cubic convolution technique will be used. It calculates
         the value of each pixel by fitting a smooth curve based on the
         surrounding 16 pixels. This produces the smoothest image but can
         create values outside of the range found in the source data. It is
         suitable for continuous data.

         * MAJORITY-The majority resampling technique will be used. It
         determines the value of each pixel based on the most popular value in
         a 4 by 4 window. It is suitable for discrete data.
         The Nearest and Majority options are used for categorical data, such
         as a land-use classification. The Nearest option is the default. It is
         the quickest and does not change the pixel values. Do not use either
         of these options for continuous data, such as elevation surfaces.The
         Bilinear and Cubic options are most appropriate for continuous
         data. It is recommended that you do not use either of these options
         with categorical data because the pixel values may be altered.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format for the dataset you are creating. When
         storing a raster dataset in a geodatabase, do not add a file extension
         to the name of the raster dataset. When storing your raster dataset to
         a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can
         specify a compression type and compression quality using environment
         settings.

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid"""
    ...

@gptooldoc("AddRastersToMosaicDataset_management", None)
def AddRastersToMosaicDataset(
    in_mosaic_dataset=...,
    raster_type=...,
    input_path=...,
    update_cellsize_ranges=...,
    update_boundary=...,
    update_overviews=...,
    maximum_pyramid_levels=...,
    maximum_cell_size=...,
    minimum_dimension=...,
    spatial_reference=...,
    filter=...,
    sub_folder=...,
    duplicate_items_action=...,
    build_pyramids=...,
    calculate_statistics=...,
    build_thumbnails=...,
    operation_description=...,
    force_spatial_reference=...,
    estimate_statistics=...,
    aux_inputs=...,
    enable_pixel_cache=...,
    cache_location=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddRastersToMosaicDataset_management(in_mosaic_dataset, raster_type, input_path;input_path..., {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {maximum_pyramid_levels}, {maximum_cell_size}, {minimum_dimension}, {spatial_reference}, {filter}, {sub_folder}, {duplicate_items_action}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {operation_description}, {force_spatial_reference}, {estimate_statistics}, {aux_inputs;aux_inputs...}, {enable_pixel_cache}, {cache_location})

       Adds raster datasets to a mosaic dataset from various sources,
       including a file, folder, table, or web service.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The path and name of the mosaic dataset to which the raster data will
         be added.
     raster_type (Raster Type):
         The type of raster that will be added. The raster type is specific to
         imagery products. It identifies metadata-such as georeferencing,
         acquisition date, and sensor type-along with a raster format.If you
         are using a LAS, LAS Dataset, or Terrain raster type, an .art
         file must be used when the cell size is specified.For a list of
         supported sensors and raster types, see the list of
         supported sensors.
     input_path (Workspace / File / WCS Coverage / Image Service / Map Server / WMS Map / Raster Catalog / Table View / Raster Layer / Mosaic Layer / Terrain Layer / LAS Dataset Layer / Layer File / WMTS Layer):
         Specifies the path and name of the input file, folder, raster dataset,
         mosaic dataset, table, or service.Not all input options will be
         available. The selected raster type
         determines the available options.

         * Dataset-An ArcGIS geographic dataset, such as a raster or mosaic
         dataset in a geodatabase or table, will be used as input.

         * Folder-A folder containing multiple raster datasets will be used as
         input. The folder can contain subfolders.This option is affected by
         the Include Sub Folders and Input Data Filter parameters.

         * File-One or more raster datasets stored in a folder on disk, an
         image service definition file (.ISDef), or a raster process definition
         file (.RPDef) will be used as input. Files that do not correspond to
         the raster type being added will be ignored. Do not use this option
         with file formats that are raster datasets, such as TIFF or MrSID
         files; use the Dataset option instead.

         * Service-A WCS, a map, an image service, or a web service layer file
         will be used as input.
     update_cellsize_ranges {Boolean}:
         Specifies whether the cell size ranges of each raster in the mosaic
         dataset will be calculated. These values will be written to the
         attribute table in the minPS and maxPS fields.

         * UPDATE_CELL_SIZES-The cell size ranges will be calculated for all
         the rasters in the mosaic dataset. This is the default.

         * NO_CELL_SIZES-The cell size ranges will not be calculated.
     update_boundary {Boolean}:
         Specifies whether the boundary polygon of a mosaic dataset will be
         generated or updated. By default, the boundary merges all the
         footprint polygons to create a single boundary representing the extent
         of the valid pixels.

         * UPDATE_BOUNDARY-The boundary will be generated or updated. This is
         the default.

         * NO_BOUNDARY-The boundary will not be generated or updated.
     update_overviews {Boolean}:
         Specifies whether overviews for a mosaic dataset will be defined and
         generated.

         * UPDATE_OVERVIEWS-Overviews will be defined and generated.

         * NO_OVERVIEWS-Overviews will not be defined or generated. This is the
         default.
     maximum_pyramid_levels {Long}:
         The maximum number of pyramid levels that will be used in the mosaic
         dataset. For example, a value of 2 will use only the first two pyramid
         levels from the source raster. Leaving this parameter blank or typing
         a value of -1 will build pyramids for all levels.This value can affect
         the display and number of overviews that will be
         generated.
     maximum_cell_size {Double}:
         The maximum pyramid cell size that will be used in the mosaic dataset.
     minimum_dimension {Long}:
         The minimum dimensions of a raster pyramid that will be used in the
         mosaic dataset.
     spatial_reference {Spatial Reference}:
         The spatial reference system of the input data.This should be
         specified if the data does not have a coordinate
         system; otherwise, the coordinate system of the mosaic dataset will be
         used. This can also be used to override the coordinate system of the
         input data.
     filter {String}:
         A filter for the data being added to the mosaic dataset. You can use
         SQL expressions to create the data filter. The wildcards for the
         filter work on the full path to the input data. The following
         SQL statement will select the rows in which the
         following object IDs match:

         * OBJECTID IN (19745, 19680, 19681, 19744, 5932, 5931, 5889, 5890,
         14551, 14552, 14590, 14591)
                 To add only a TIFF image, add an asterisk before a file
         extension.

         * *.TIF
                 To add an image with the word sensor in the file path or file
         name, add an asterisk before and after the word sensor.

         * *sensor2009*
                 You can also use PERL syntax to create a data filter.

         * REGEX:.*1923.*|.*1922.*

         * REGEX:.*192[34567].*|.*194.*|.*195.*
                 The following PERL syntax with multiple lexical groupings as
         part of the expression is not supported:

         * REGEX:.*
         map_mean_.*(?:(?:[a-z0-9]*)_pptPct_(?:[0-9]|1[0-2]*?)_2[0-9]_*\\w*).img
                 Alternatively, you can use the following syntax:

         * REGEX:.*map_mean_*[a-z0-9]*_pptPct_([0-9]|1[0-2])_2[0-9]*_\\w*.img
     sub_folder {Boolean}:
         Specifies whether subfolders will be recursively explored.

         * SUBFOLDERS-All subfolders will be explored for data. This is the
         default.

         * NO_SUBFOLDERS-Only the top-level folder will be explored for data.
     duplicate_items_action {String}:
         Specifies how duplicate rasters will be handled. A check will be
         performed to determine whether each raster has already been added,
         using the original path and file name. Choose the action to be
         performed when a duplicate path and file name are found.

         * ALLOW_DUPLICATES-All rasters will be added even if they already
         exist in the mosaic dataset. This is the default.

         * EXCLUDE_DUPLICATES-Duplicate rasters will not be added.

         * OVERWRITE_DUPLICATES-Duplicate rasters will overwrite existing
         rasters.
     build_pyramids {Boolean}:
         Specifies whether pyramids will be built for each source raster.

         * NO_PYRAMIDS-Pyramids will not be built. This is the default.

         * BUILD_PYRAMIDS-Pyramids will be built.
     calculate_statistics {Boolean}:
         Specifies whether statistics will be calculated for each source
         raster.

         * NO_STATISTICS-Statistics will not be calculated. This is the
         default.

         * CALCULATE_STATISTICS-Statistics will be calculated.
     build_thumbnails {Boolean}:
         Specifies whether thumbnails will be built for each source raster.

         * NO_THUMBNAILS-Thumbnails will not be built. This is the default.

         * BUILD_THUMBNAILS-Thumbnails will be built.
     operation_description {String}:
         The description used to represent the operation of adding raster data.
         It will be added to the raster type table, which can be used as part
         of a search or as a reference at another time.
     force_spatial_reference {Boolean}:
         Specifies the coordinate system that will be used. Use the coordinate
         system specified in the spatial_reference parameter for all the
         rasters when loading data into the mosaic dataset.

         * NO_FORCE_SPATIAL_REFERENCE-The coordinate system of each raster data
         will be used when loading data. This is the default.

         * FORCE_SPATIAL_REFERENCE-The coordinate system specified in the
         spatial_reference parameter will be used for each raster when loading
         data.
     estimate_statistics {Boolean}:
         Specifies whether statistics will be estimated on the mosaic dataset
         for faster rendering and processing at the mosaic dataset level.

         * NO_STATISTICS-Statistics will not be estimated. Statistics generated
         from each item in the mosaic dataset will be used for display and
         processing. This is the default.

         * ESTIMATE_STATISTICS-Statistics will be estimated at the mosaic
         dataset level. This will use the distribution of pixels to display the
         mosaic dataset rather than the distribution of the source item in the
         mosaic dataset.
     aux_inputs {Value Table}:
         The raster type settings that will be defined on the Raster
         Type Properties page. This parameter will override the settings
         defined on the Raster Type Properties page. Auxiliary input options
         include the following:

         * CameraFile-A .cam or .csv file that stores the camera model
         information, such as the focal length, sensor pixel size, columns,
         rows and similar attributes.

         * CameraProperties-A JSON string that defines the camera model.

         * ConstantZ-A value, in meters, that is used to provide an initial
         estimate of the flight height for each image.

         * CorrectGeoid-A value of 0 represents false, and a value of 1
         represents true. This option is only available when a DEM is used.

         * DEM-A DEM used to provide an initial estimate of the flight height
         for each image.

         * EstimateFlightHeight-An estimated flight height which can be
         directly used to compute footprint and estimate photo scale.

         * GPSAltRef-Define the altitude used as the reference altitude. 0
         indicates above sea level, and 1 is below sea level.

         * GPSFile-A GPS text file, such as a comma-separated value file
         (.csv), which includes values for the Image Name, Latitude, Longitude,
         and Altitude fields, and optionally, the Omega, Phi, and Kappa fields.
         The geolocation file is provided by the supplier along with the drone
         imagery.

         * GPSLatRef-Define whether the latitude is north or south latitude. N
         indicates north latitude, and S is south latitude.

         * GPSLonRef-Define whether the longitude is east or west longitude. E
         indicates east longitude, and W is west longitude.

         * GPSSRS-Define the spatial reference of the GPS.

         * GPSVCS-Define the vertical coordinate system of the GPS.

         * IsAltitudeFlightHeight-A Boolean value defines whether the drone
         reports heights relative to the takeoff point, or altitude for heights
         relative to a vertical datum. Use a value of 0 when the altitude value
         is altitude above a datum, and a value of 1 when the altitude value is
         flight height from a takeoff point.

         * MinimumFlightHeight-Define the minimum flight height. If the
         estimated flight height of an image is smaller than this number, the
         image will be considered invalid and will not be added to the image
         collection for future processing.

         * ZFactor-The scaling factor used to convert the elevation values.
         This option is only available when using a DEM.

         * ZOffset-The base value to be added to the elevation value in the
         DEM. This can be used to offset elevation values that do not start at
         sea level. This option is only available when using a DEM.
     enable_pixel_cache {Boolean}:
         Specifies whether the pixel cache will be generated for faster display
         and processing of the mosaic dataset.

         * NO_PIXEL_CACHE-The pixel cache will not be generated. This is the
         default.

         * USE_PIXEL_CACHE-The pixel cache will be generated.
     cache_location {Folder / String}:
         The location of the pixel cache. If no location is defined, the cache
         will be written to
         C:\\Users\\<Username>\\AppData\\Local\\ESRI\\rasterproxies\\.Once the
         location is defined, you do not need to redefine the path
         when adding new rasters to the mosaic dataset. You do need to check
         the Enable Pixel Cache parameter (enable_pixel_cache =
         "USE_PIXEL_CACHE" in Python) when adding the new data."""
    ...

@gptooldoc("AlterMosaicDatasetSchema_management", None)
def AlterMosaicDatasetSchema(
    in_mosaic_dataset=..., side_tables=..., raster_type_names=..., editor_tracking=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterMosaicDatasetSchema_management(in_mosaic_dataset, {side_tables;side_tables...}, {raster_type_names;raster_type_names...}, {editor_tracking})

       Defines the editing operations that nonowners have when editing a
       mosaic dataset in an enterprise geodatabase.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset on which the permitted operations will be changed.
     side_tables {String}:
         Specifies the operations that will be permissible for the mosaic
         dataset.

         * ANALYSIS-A nonowner will be allowed to run the Analyze Mosaic
         Dataset tool on the mosaic dataset.

         * BOUNDARY-A nonowner will be allowed to create or edit the boundary
         of the mosaic dataset. This is also required if a nonowner will add
         rasters outside of the existing boundary.

         * CACHE-A nonowner will be allowed to create a cache for the mosaic
         dataset.

         * COLOR_CORRECTION-A nonowner will be allowed to color correct the
         mosaic dataset.

         * DEFINITION-A nonowner will be allowed to add multidimensional data
         or a processing template to the mosaic dataset.

         * LEVELS-A nonowner will be allowed to calculate cell size ranges or
         create seamlines for the mosaic dataset.

         * LOG-A nonowner will be allowed to create a log table for the mosaic
         dataset.

         * OVERVIEW-A nonowner will be allowed to create overviews for the
         mosaic dataset.

         * SEAMLINE-A nonowner will be allowed to create seamlines for the
         mosaic dataset.

         * STEREO-A nonowner will be allowed to define stereo pairs for the
         mosaic dataset.

         * VIEW-A nonowner will be allowed to edit the image service. The
         editor_tracking parameter will be automatically enabled when this
         option is specified, since the View table must have editor tracking
         turned on.
     raster_type_names {String}:
         Specifies the raster types that nonowners can add to the mosaic
         dataset.To select a custom raster type, provide the location of the
         custom
         raster type file.

         * ADS-The Leica ADS raster type can be added.

         * Altum-The Altum raster type can be added.

         * ASTER-The ASTER raster type can be added.

         * BlackSky-The BlackSky raster type can be added.

         * CADRG/ECRG-The CADRG/ECRG raster type can be added.

         * CIB-The CIB raster type can be added.

         * Capella-The Capella raster type can be added.

         * DEIMOS-2-The Deimos-2 raster type can be added.

         * DTED-The DTED raster type can be added.

         * DMCii-The DMCii raster type can be added.

         * DubaiSat-2-The DubaiSat-2 raster type can be added.

         * FORMOSAT-2-The FORMOSAT-2 raster type can be added.

         * Frame Camera-The Frame Camera raster type can be added.

         * GeoEye-1-The GeoEye-1 raster type can be added.

         * GF-1 PMS-The GF-1 PMS raster type can be added.

         * GF-1 WFV-The GF-1 WFV raster type can be added.

         * GF-2 PMS-The GF-2 PMS raster type can be added.

         * GF-4 PMI-The GF-4 PMI raster type can be added.

         * GRIB-The GRIB raster type can be added.

         * HDF-The HDF raster type can be added.

         * HJ 1A/1B CCD-The HJ 1A/HJ 1B CCD raster type can be added.

         * HRE-The HRE raster type can be added.

         * ICEYE-The ICEYE raster type can be added.

         * IKONOS-The IKONOS raster type can be added.

         * Jilin-1-The Jilin-1 raster type can be added.

         * KOMPSAT-2-The KOMPSAT-2 raster type can be added.

         * KOMPSAT-3-The KOMPSAT-3 raster type can be added.

         * LAS-The LAS raster type can be added.

         * Landsat 1-5 MSS-The Landsat 1-5 MSS raster type can be added.

         * Landsat 4-5 TM-The Landsat 4-5 TM raster type can be added.

         * Landsat 7 ETM+-The Landsat 7 ETM+ raster type can be added.

         * Landsat 8-The Landsat 8 raster type can be added.

         * Landsat 9-The Landsat 9 raster type can be added. can be added.

         * MAXAR-The Maxar raster type can be added.

         * NCDRD-The NCDRD raster type can be added.

         * NITF-The NITF raster type can be added.

         * NetCDF-The NetCDF raster type can be added.

         * PlanetScope-The PlanetScope raster type can be added.

         * Pleiades Neo-The Pleiades Neo raster type can be added.

         * Pleiades-1-The Pleiades-1 raster type can be added.

         * QuickBird-The Quickbird raster type can be added.

         * RADARSAT-2-The RADARSAT-2 raster type can be added.

         * RCM-The RCM raster type can be added.

         * RapidEye-The RapidEye raster type can be added.

         * Raster Process Definition-The Raster Process Definition raster type
         can be added.

         * RedEdge-The RedEdge raster type can be added.

         * Scanned Aerial Imagery-The Scanned Aerial Imagery raster type can be
         added.

         * Sentinel-1-The Sentinel-1 raster type can be added.

         * Sentinel-2-The Sentinel-2 raster type can be added.

         * Sentinel-3-The Sentinel-3 raster type can be added.

         * SkySat-The SkySat-C raster type can be added.

         * SPOT 5-The SPOT 5 raster type can be added.

         * SPOT 6-The SPOT 6 raster type can be added.

         * SPOT 7-The SPOT 7 raster type can be added.

         * SuperView-1-The SuperView-1 raster type can be added.

         * TeLEOS-1-The TelEOS-1 raster type can be added.

         * TH-01-The TH-01 raster type can be added.

         * UAV/UAS-The UAV/UAS raster type can be added.

         * WorldView-1-The WorldView-1 raster type can be added.

         * WorldView-2-The WorldView-2 raster type can be added.

         * WorldView-3-The WorldView-3 raster type can be added.

         * WorldView-4-The WorldView-4 raster type can be added.

         * ZY1-02C HRC-The ZY1-02C HRC raster type can be added.

         * ZY1-02C PMS-The ZY1-02C PMS raster type can be added.

         * ZY3-CRESDA-The ZY3-CRESDA raster type can be added.

         * ZY3-SASMAC-The ZY3-SASMAC raster type can be added.
     editor_tracking {Boolean}:
         Specifies whether editor tracking will be enabled.Editor tracking can
         help you maintain accountability and enforce
         quality-control standards.

         * NO_EDITOR_TRACKING-Editor tracking will not be enabled. This is the
         default.

         * EDITOR_TRACKING-Editor tracking will be enabled.
         If the VIEW keyword is specified for the side_tables parameter, editor
         tracking will be automatically enabled."""
    ...

@gptooldoc("AnalyzeMosaicDataset_management", None)
def AnalyzeMosaicDataset(
    in_mosaic_dataset=..., where_clause=..., checker_keywords=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AnalyzeMosaicDataset_management(in_mosaic_dataset, {where_clause}, {checker_keywords;checker_keywords...})

       Performs checks on a mosaic dataset for errors and possible
       improvements.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset you want to analyze.
     where_clause {SQL Expression}:
         An SQL statement that confines your analysis to specific raster
         datasets within this mosaic dataset.
     checker_keywords {String}:
         Choose which parts of the mosaic dataset you want to analyze for known
         issues.

         * FOOTPRINT-Analyze the footprint geometry of each selected mosaic
         dataset item. This is checked on by default.

         * FUNCTION-Analyze the function chains for each selected mosaic
         dataset item.

         * RASTER-Analyze the original raster datasets. This is checked on by
         default.

         * PATHS-Check for broken paths. This is checked on by default.

         * SOURCE_VALIDITY-Analyze potential problems with the source data
         associated with each mosaic dataset item in the selected mosaic
         dataset. This is a good way to detect issues that may arise during
         synchronization workflows.

         * STALE-Overviews are stale when the underlying source data has
         changed. Once the mosaic dataset is analyzed, you can select which
         items are stale by right-clicking on the error and clicking Select
         Associated Items on the context menu.

         * PYRAMIDS-Analyze the raster pyramids associated with each mosaic
         dataset item in the selected mosaic dataset. Test for disconnected
         auxiliary files, which can occur when they are stored in a raster
         proxy location.

         * STATISTICS-Test for disconnected auxiliary statistics files if they
         are stored in the raster proxy location. Analyze the covariance matrix
         associated with the raster, when the Gram-Schmidt pan-sharpening
         method is enabled. Analyze the radiometric pixel depth of a mosaic
         dataset item against the pixel depth of the mosaic dataset.

         * PERFORMANCE-Factors that increase performance include compression
         during transmission and caching items with many raster functions.

         * INFORMATION-Generate general information about the mosaic dataset."""
    ...

@gptooldoc("BuildBoundary_management", None)
def BuildBoundary(
    in_mosaic_dataset=...,
    where_clause=...,
    append_to_existing=...,
    simplification_method=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildBoundary_management(in_mosaic_dataset, {where_clause}, {append_to_existing}, {simplification_method})

       Updates the extent of the boundary when adding new raster datasets to
       a mosaic dataset that extend beyond its previous coverage.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         Select the mosaic dataset where you want to recompute the boundary.
     where_clause {SQL Expression}:
         An SQL query to compute a boundary for select raster datasets. Use
         this option in conjunction with setting the append_to_existing
         parameter to APPEND to save time when adding new raster datasets.
     append_to_existing {Boolean}:
         Set this to APPEND when adding new raster datasets to an existing
         mosaic dataset. Instead of calculating the entire boundary, APPEND
         will merge the boundary of the new raster datasets with the existing
         boundary.

         * OVERWRITE-Recompute the boundary in its entirety.

         * APPEND-Append the perimeter of footprints to the existing boundary.
         This can save time when adding additional raster data to the mosaic
         dataset, as the entire boundary will not be recalculated. If there are
         rasters selected, the boundary will be recalculated to include only
         the selected footprints. This is the default.
     simplification_method {String}:
         Specifies the simplification method that will be used to reduce the
         number of vertices, since a dense boundary can affect
         performance.Choose the simplification method to use to simplify the
         boundary.

         * NONE-No simplification method will be implemented. This is the
         default.

         * CONVEX_HULL-The minimum bounding geometry of the mosaic dataset will
         be used to simplify the boundary. If there are disconnected
         footprints, a minimum bounding geometry for each continuous group of
         footprints will be used to simplify the boundary.

         * ENVELOPE-The envelope of the mosaic dataset will provide a
         simplified boundary. If there are disconnected footprints, an envelope
         for each continuous group of footprints will be used to simplify the
         boundary."""
    ...

@gptooldoc("BuildFootprints_management", None)
def BuildFootprints(
    in_mosaic_dataset=...,
    where_clause=...,
    reset_footprint=...,
    min_data_value=...,
    max_data_value=...,
    approx_num_vertices=...,
    shrink_distance=...,
    maintain_edges=...,
    skip_derived_images=...,
    update_boundary=...,
    request_size=...,
    min_region_size=...,
    simplification_method=...,
    edge_tolerance=...,
    max_sliver_size=...,
    min_thinness_ratio=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildFootprints_management(in_mosaic_dataset, {where_clause}, {reset_footprint}, {min_data_value}, {max_data_value}, {approx_num_vertices}, {shrink_distance}, {maintain_edges}, {skip_derived_images}, {update_boundary}, {request_size}, {min_region_size}, {simplification_method}, {edge_tolerance}, {max_sliver_size}, {min_thinness_ratio})

       Computes the extent of every raster in a mosaic dataset. This tool is
       used when you have added or removed raster datasets from a mosaic
       dataset and want to recompute the footprints.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that contains the raster datasets whose footprints
         you want to compute.
     where_clause {SQL Expression}:
         An SQL expression to select specific raster datasets within the mosaic
         dataset.
     reset_footprint {String / Boolean}:
         Refine the footprints using one of the following methods:

         * RADIOMETRY-Exclude pixels with a value outside of a defined range.
         This option is generally used to exclude border areas, which do not
         contain valid data. This is the default.

         * GEOMETRY-Restore the footprint to its original geometry.

         * COPY_TO_SIBLING-Replace the panchromatic footprint with the
         multispectral footprint when using a pan-sharpened raster type. This
         can occur when the panchromatic and multispectral images do not have
         identical geometries.

         * NONE-Do not redefine the footprints.
     min_data_value {Double}:
         Exclude pixels with a value less than this number.
     max_data_value {Double}:
         Exclude pixels with a value greater than this number.
     approx_num_vertices {Long}:
         Choose between 4 and 10,000. More vertices will improve accuracy but
         can extend processing time. A value of -1 will calculate all vertices.
         More vertices will increase accuracy but also the processing time.
     shrink_distance {Double}:
         Clip the footprint by this distance. This can eliminate artifacts from
         using lossy compression, which causes the edges of the image to
         overlap into NoData areas.Shrinking of the polygon is used to
         counteract effects of lossy
         compression, which causes edges of the image to overlap into NoData
         areas.
     maintain_edges {Boolean}:
         Use this parameter when using raster datasets that have been tiled and
         are adjacent (line up along the seams with little to no overlap).

         * NO_MAINTAIN_EDGES-Remove the sheet edges from all the footprints.
         This is the default.

         * MAINTAIN_EDGES-Maintain the footprints in their original state.
     skip_derived_images {Boolean}:
         Adjust the footprints of overviews.

         * SKIP_DERIVED_IMAGES-Do not adjust the footprints of overviews. This
         is the default.

         * NO_SKIP_DERIVED_IMAGES-Adjust the footprints of overviews and
         associated raster datasets.
     update_boundary {Boolean}:
         Update the boundary of the mosaic dataset if you have added or removed
         imagery that changes the extent.

         * UPDATE_BOUNDARY-Update the boundary. This is the default.

         * NO_BOUNDARY-Do not update the boundary.
     request_size {Long}:
         Set the resampled extent (in columns and rows) for the raster when
         building footprints. Greater image resolution provides more detail in
         the raster dataset but increases the processing time. A value of -1
         will compute the footprint at the original resolution.
     min_region_size {Long}:
         Avoid small holes in your imagery when using pixel values to create a
         mask. For example, your imagery may have a range of values from 0 to
         255, and to mask clouds, you've excluded values from 245 to 255, which
         may cause other, noncloud pixels to be masked as well. If those areas
         are smaller than the number of pixels specified here, they will not be
         masked out.
     simplification_method {String}:
         Reduce the number of vertices in the footprint to improve performance.

         * NONE-Do not limit the number of vertices. This is the default.

         * CONVEX_HULL-Use the minimum bounding box to simplify the footprint.

         * ENVELOPE-Use the envelope of each mosaic dataset item to simplify
         the footprint.
     edge_tolerance {Double}:
         Snap the footprint to the sheet edge if it is within this tolerance.
         Units are the same as those in the mosaic dataset coordinate system.
         This is used when maintain_edges is set to MAINTAIN_EDGES.By default,
         the value is empty for which the tolerance is computed
         based on the pixel size corresponding to the requested resampled
         raster.A value of -1 will compute the tolerance using the average
         pixel size
         of the mosaic dataset.
     max_sliver_size {Long}:
         Identify all polygons that are smaller than the square of this value.
         The value is specified in pixels and is based on the request_size, not
         the spatial resolution of the source raster.Regions less than the
         (max_sliver_size)2 and less than the
         min_thinness_ratio are considered slivers and will be removed.
     min_thinness_ratio {Double}:
         Define the thinness of slivers on a scale from 0 to 1.0, where 1.0
         represents a circle and 0.0 represents a polygon that approaches a
         straight line.Polygons that are below both the max_sliver_size and
         min_thinness_ratio will be removed from the footprint."""
    ...

@gptooldoc("BuildMosaicDatasetItemCache_management", None)
def BuildMosaicDatasetItemCache(
    in_mosaic_dataset=...,
    where_clause=...,
    define_cache=...,
    generate_cache=...,
    item_cache_folder=...,
    compression_method=...,
    compression_quality=...,
    max_allowed_rows=...,
    max_allowed_columns=...,
    request_size_type=...,
    request_size=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildMosaicDatasetItemCache_management(in_mosaic_dataset, {where_clause}, {define_cache}, {generate_cache}, {item_cache_folder}, {compression_method}, {compression_quality}, {max_allowed_rows}, {max_allowed_columns}, {request_size_type}, {request_size})

       Inserts the Cached Raster function as the final step in all function
       chains within a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset where you want to apply the cache function.
     where_clause {SQL Expression}:
         An SQL expression to select specific raster datasets within the mosaic
         dataset on which you want the item cache built.
     define_cache {Boolean}:
         Choose to define the mosaic dataset cache. A Cached Raster function
         will be inserted to the selected items. If an item already has a
         Cached Raster function, it will not add another one.

         * DEFINE_CACHE-The Cached Raster function will be added to the
         selected items. If an item already has this function, it will not add
         another one. This is the default.

         * NO_DEFINE_CACHE-No raster cache will be defined.
     generate_cache {Boolean}:
         Choose to generate the cache files based on the properties defined in
         the Cached Raster function, such as the location and the compression
         of the cache.

         * GENERATE_CACHE-Cache will be generated. This is the default.

         * NO_GENERATE_CACHE-Cache will not be generated.
     item_cache_folder {Workspace}:
         Choose to overwrite the default location to save your cache. If the
         mosaic dataset is inside of a file geodatabase, by default, the cache
         is saved in a folder with the same name as the geodatabase and a
         .cache extension. If the mosaic dataset is inside of an enterprise
         geodatabase, by default, the cache will be saved inside of that
         geodatabase. Once created, the cache will always save to the same
         location. To save the cache to a different location, you need to first
         use the Repair Mosaic Dataset tool to specify a new location and run
         this tool again.Once an item cache is created, regenerating an item
         cache to a
         different location is not possible by specifying a different cache
         path and rerunning this tool. It will still generate the item cache in
         the location where it was generated the first time. However, you can
         remove this function and insert a new one with the new path or use the
         Repair Mosaic Dataset tool to modify the cache path and run this tool
         to generate the item cache in a different location.
     compression_method {String}:
         Choose how you want to compress your data for faster transmission.

         * LOSSLESS-Retain the values of each pixel when generating cache.
         Lossless has a compression ratio of approximately 2:1.

         * LOSSY-Appropriate when your imagery is only used as a backdrop.
         Lossy has the highest compression ratio (20:1) but groups similar
         pixel values to achieve higher compression.

         * NONE-Do not compress imagery. This will make your imagery slower to
         transmit but faster to draw because it will not need to be
         decompressed when viewed.
     compression_quality {Long}:
         Set a compression quality when using the lossy method. The compression
         quality value is between 1 and 100 percent, with 100 compressing the
         least.
     max_allowed_rows {Long}:
         Limit the size of the cache dataset by number of rows. If value is
         more than the number of rows in the dataset, the cache will not
         generate.
     max_allowed_columns {Long}:
         Limit the size of the cache dataset by number of columns. If value is
         more than the number of columns in the dataset, the cache will not
         generate.
     request_size_type {String}:
         Resample the cache using one of these two methods:

         * PIXEL_SIZE_FACTOR-Set a scaling factor relative to the pixel size.
         To not resample the cache, choose PIXEL_SIZE_FACTOR and set the
         request_size parameter to 1.

         * PIXEL_SIZE-Specify a pixel size for the cached raster.
     request_size {Double}:
         Set a value to apply to the request_size_type."""
    ...

@gptooldoc("BuildOverviews_management", None)
def BuildOverviews(
    in_mosaic_dataset=...,
    where_clause=...,
    define_missing_tiles=...,
    generate_overviews=...,
    generate_missing_images=...,
    regenerate_stale_images=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildOverviews_management(in_mosaic_dataset, {where_clause}, {define_missing_tiles}, {generate_overviews}, {generate_missing_images}, {regenerate_stale_images})

       Defines and generates overviews on a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset where you want to build overviews.
     where_clause {SQL Expression}:
         An SQL statement to select specific rasters within the mosaic dataset.
         The selected rasters will have their overview built.
     define_missing_tiles {Boolean}:
         Identify where overviews are needed and define them.

         * DEFINE_MISSING_TILES-Automatically identify where overviews are
         needed and define them. This is the default.

         * NO_DEFINE_MISSING_TILES-Do not define new overviews.
     generate_overviews {Boolean}:
         Generate all overviews that need to be created or re-created. This
         includes missing overviews and stale overviews.

         * GENERATE_OVERVIEWS-Generate all overviews, including those that
         already exist. This is the default.

         * NO_GENERATE_OVERVIEWS-Generate only the overviews that have been
         defined but not yet generated.
     generate_missing_images {Boolean}:
         Use if overviews have been defined but not generated.

         * GENERATE_MISSING_IMAGES-Generate overviews that have been defined
         but not generated. This is the default.

         * IGNORE_MISSING_IMAGES-Do not generate overviews that have been
         defined but not generated.
     regenerate_stale_images {Boolean}:
         Overviews become stale when you change the underlying raster datasets
         or modify their properties.

         * REGENERATE_STALE_IMAGES-Identify and regenerate stale overviews.
         This is the default.

         * IGNORE_STALE_IMAGES-Do not regenerate stale overviews."""
    ...

@gptooldoc("BuildSeamlines_management", None)
def BuildSeamlines(
    in_mosaic_dataset=...,
    cell_size=...,
    sort_method=...,
    sort_order=...,
    order_by_attribute=...,
    order_by_base_value=...,
    view_point=...,
    computation_method=...,
    blend_width=...,
    blend_type=...,
    request_size=...,
    request_size_type=...,
    blend_width_units=...,
    area_of_interest=...,
    where_clause=...,
    update_existing=...,
    min_region_size=...,
    min_thinness_ratio=...,
    max_sliver_size=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildSeamlines_management(in_mosaic_dataset, {cell_size;cell_size...}, {sort_method}, {sort_order}, {order_by_attribute}, {order_by_base_value}, {view_point}, {computation_method}, {blend_width}, {blend_type}, {request_size}, {request_size_type}, {blend_width_units}, {area_of_interest}, {where_clause}, {update_existing}, {min_region_size}, {min_thinness_ratio}, {max_sliver_size})

       Generate or update seamlines for your mosaic dataset. Seamlines are
       used to sort overlapping imagery and produce a smoother-looking
       mosaic.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         Select the mosaic dataset on which to build seamlines.
     cell_size {Double}:
         Generate seamlines for raster datasets that fall within the following
         range of spatial resolutions.You can leave this parameter empty and
         the tool will automatically
         create seamlines at the appropriate levels.The units for this
         parameter are the same as the spatial reference of
         the input mosaic dataset.
     sort_method {String}:
         Set a rule to determine which raster will be used to generate
         seamlines when images overlap.

         * NORTH_WEST-Select the raster datasets that have center points
         closest to the northwest corner of the boundary. This is the default.

         * CLOSEST_TO_VIEWPOINT-Select raster datasets based on a user-defined
         location and a nadir location for the raster datasets using the
         Viewpoint tool.

         * BY_ATTRIBUTE-Select raster datasets based on an attribute from the
         footprint attribute table. Commonly used attributes include
         acquisition date, cloud cover, or viewing angle.
     sort_order {Boolean}:
         Choose whether to sort the rasters in ascending order or descending
         order.

         * ASCENDING-Sort the rasters in ascending order. This is the default.

         * DESCENDING-Sort the rasters in descending order.
     order_by_attribute {Field}:
         Order the raster datasets based on this field when the sort method is
         BY_ATTRIBUTE. The default attribute is ObjectID.
     order_by_base_value {Variant}:
         Sort the rasters by their difference between this value and their
         value in the order_by_attribute parameter.
     view_point {Point}:
         Set the coordinate location to use when sort_method is
         CLOSEST_TO_VIEWPOINT.
     computation_method {String}:
         Choose how to build seamlines.

         * GEOMETRY-Generate seamlines for overlapping areas based on the
         intersection of footprints. Areas with no overlapping imagery will
         merge the footprints. This is the default.

         * RADIOMETRY-Generate seamlines based on the spectral patterns of
         features within the imagery.

         * COPY_FOOTPRINT-Generate seamlines directly from the footprints.

         * COPY_TO_SIBLING-Apply the seamlines from another mosaic dataset. The
         mosaic datasets have to be in the same group. For example, the extent
         of the panchromatic band does not always match the extent of the
         multispectral band. This option makes sure they share the same
         seamline.

         * EDGE_DETECTION-Generate seamlines over intersecting areas based on
         the edges of features in the area.

         * VORONOI-Generate seamlines using the area Voronoi diagram.

         * DISPARITY-Generate seamlines based on the disparity images of stereo
         pairs. This method can avoid seamlines cutting through buildings.
         The Sort Method parameter applies to each computation method.
     blend_width {Double}:
         Blending (feathering) occurs along a seamline between pixels where
         there are overlapping rasters. The blend width defines how many pixels
         will be blended.If the blend width value is 10, and you use BOTH as
         the blend type,
         then 5 pixels will be blended on the inside and outside of the
         seamline. If the value is 10, and the blend type is INSIDE, then 10
         pixels will be blended on the inside of the seamline.
     blend_type {String}:
         Determine how to blend one image into another, over the seamlines.
         Options are to blend inside the seamlines, outside the seamlines, or
         both inside and outside.

         * BOTH-Blend using pixels on either side of the seamlines. For
         example, if the Blend Width is 10 pixels, then five pixels will be
         blended on the inside and outside of the seamline. This is the
         default.

         * INSIDE-Blend inside of the seamline.

         * OUTSIDE-Blend outside of the seamline.
     request_size {Long}:
         Specify the number of columns and rows for resampling. The maximum
         value is 5,000. Increase or decrease this value based on the
         complexity of your raster data. Greater image resolution provides more
         detail in the raster dataset but also increases the processing time.
     request_size_type {String}:
         Set the units for the Request Size.

         * PIXELS-Modify the request size based on the pixel size.This is the
         default option and resamples the closest image based on the raster
         pixel size.

         * PIXELSIZE_FACTOR-Modify the request size by specifying a scaling
         factor. This option resamples the closest image by multiplying the
         raster pixel size (from cell size level table) with the pixel size
         factor.
     blend_width_units {String}:
         Specify the unit of measurement for blend width.

         * PIXELS-Measure using the number of pixels. This is the default.

         * GROUND_UNITS-Measure using the same units as the mosaic dataset.
     area_of_interest {Feature Set}:
         Build seamlines on all the rasters that intersect this polygon. To
         select an area of interest, use an input feature class.
     where_clause {SQL Expression}:
         SQL expression to build seamlines on specific raster datasets within
         the mosaic dataset.
     update_existing {Boolean}:
         Update seamlines that are affected by the addition or deletion of the
         mosaic dataset items.

         * IGNORE_EXISTING-Regenerates seamlines for all items and ignores
         existing seamlines, if any. This is the default.

         * UPDATE_EXISTING-Only update items without seamlines. If any new
         items overlap with the previously created seamlines, the existing
         seamlines may be affected.
         This parameter is ignored if seamlines do not exist.
     min_region_size {Long}:
         Specify the minimum region size, in pixel units. Any polygons smaller
         than this specified threshold will be removed in the seamline result.
         The default is 100 pixels.This parameter value should be smaller than
         the sliver area, which is
         (max_sliver_size) * (max_sliver_size).
     min_thinness_ratio {Double}:
         Define how thin a polygon can be, before it is considered a sliver.
         This is based on a scale from 0 to 1.0, where a value of 0.0
         represents a polygon that is almost a straight line, and a value of
         1.0 represents a polygon that is a circle.Slivers are removed when
         building seamlines.
     max_sliver_size {Long}:
         Specify the maximum size a polygon can be to still be considered a
         sliver. This parameter is specified in pixels and is based on the
         request_size, not the spatial resolution of the source raster. Any
         polygon that is less than the square of this value is considered a
         sliver. Any regions that are less than (max_sliver_size)2 are
         considered slivers.Slivers are removed when building seamlines."""
    ...

@gptooldoc("CalculateCellSizeRanges_management", None)
def CalculateCellSizeRanges(
    in_mosaic_dataset=...,
    where_clause=...,
    do_compute_min=...,
    do_compute_max=...,
    max_range_factor=...,
    cell_size_tolerance_factor=...,
    update_missing_only=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateCellSizeRanges_management(in_mosaic_dataset, {where_clause}, {do_compute_min}, {do_compute_max}, {max_range_factor}, {cell_size_tolerance_factor}, {update_missing_only})

       Computes the visibility levels of raster datasets in a mosaic dataset
       based on the spatial resolution.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset to calculate the visibility levels for.
     where_clause {SQL Expression}:
         An SQL expression to select specific rasters in the mosaic dataset on
         which to calculate visibility levels. If no query is specified, all
         the mosaic dataset items will have their cell size ranges calculated.
     do_compute_min {Boolean}:
         Compute the minimum pixel size for each selected raster in the mosaic
         dataset.

         * MIN_CELL_SIZES-Compute the minimum pixel size. This is the default.

         * NO_MIN_CELL_SIZES-Do not compute the minimum pixel size.
     do_compute_max {Boolean}:
         Compute the maximum pixel size for each selected raster in the mosaic
         dataset.

         * MAX_CELL_SIZES-Compute the maximum pixel size. This is the default.

         * NO_MAX_CELL_SIZES-Do not compute the maximum pixel size.
     max_range_factor {Double}:
         Set a multiplication factor to apply to the native resolution. The
         default is 10, meaning that an image with a resolution of 30 meters
         will be visible at a scale appropriate for 300 meters. The
         relationship between cell size and scale is as follows:Cell Size =
         Scale * 0.0254 / 96Scale = Cell Size * 96 / 0.0254
     cell_size_tolerance_factor {Double}:
         Use this to group images with similar resolutions as having the same
         nominal resolution. For example 1 m imagery and 0.9 m imagery can be
         grouped together by setting this factor to 0.1, because they are
         within 10% of each other.
     update_missing_only {Boolean}:
         Calculate only the missing cell size range values.

         * UPDATE_ALL-Calculate cell size minimum and maximum values for
         selected rasters within the mosaic dataset. This is the default.

         * UPDATE_MISSING_ONLY-Calculate cell size minimum and maximum values
         only if they do not exist."""
    ...

@gptooldoc("ClearPixelCache_management", None)
def ClearPixelCache(
    in_mosaic_dataset=..., generated_before=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ClearPixelCache_management(in_mosaic_dataset, {generated_before})

       Clears the pixel cache associated with a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The input mosaic dataset with the pixel cache to be deleted.
     generated_before {Date}:
         All cache generated before this date will be deleted."""
    ...

@gptooldoc("ColorBalanceMosaicDataset_management", None)
def ColorBalanceMosaicDataset(
    in_mosaic_dataset=...,
    balancing_method=...,
    color_surface_type=...,
    target_raster=...,
    exclude_raster=...,
    stretch_type=...,
    gamma=...,
    block_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ColorBalanceMosaicDataset_management(in_mosaic_dataset, {balancing_method}, {color_surface_type}, {target_raster}, {exclude_raster}, {stretch_type}, {gamma}, {block_field})

       Makes transitions from one image to an adjoining image appear
       seamless.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset you want to color balance.
     balancing_method {String}:
         The balancing algorithm to use.

         * DODGING-Change each pixel's value toward a target color. With this
         technique, you must also choose the type of target color surface,
         which affects the target color. Dodging tends to give the best result
         in most cases.

         * HISTOGRAM-Change each pixel's value according to its relationship
         with a target histogram. The target histogram can be derived from all
         of the rasters, or you can specify a raster. This technique works well
         when all of the rasters have a similar histogram.

         * STANDARD_DEVIATION-Change each of the pixel's values according to
         its relationship with the histogram of the target raster, within one
         standard deviation. The standard deviation can be calculated from all
         of the rasters in the mosaic dataset, or you can specify a target
         raster. This technique works best when all of the rasters have normal
         distributions.
     color_surface_type {String}:
         When using the Dodging balance method, each pixel needs a target
         color, which is determined by the surface type.

         * SINGLE_COLOR-Use when there are only a small number of raster
         datasets and a few different types of ground objects. If there are too
         many raster datasets or too many types of ground surfaces, the output
         color may become blurred. All the pixels are altered toward a single
         color point-the average of all pixels.

         * COLOR_GRID-Use when you have a large number of raster datasets, or
         areas with a large number of diverse ground objects. Pixels are
         altered toward multiple target colors, which are distributed across
         the mosaic dataset.

         * FIRST_ORDER-This technique tends to create a smoother color change
         and uses less storage in the auxiliary table, but it may take longer
         to process compared to the color grid surface. All pixels are altered
         toward many points obtained from the two-dimensional polynomial
         slanted plane.

         * SECOND_ORDER-This technique tends to create a smoother color change
         and uses less storage in the auxiliary table, but it may take longer
         to process compared to the color grid surface. All input pixels are
         altered toward a set of multiple points obtained from the two-
         dimensional polynomial parabolic surface.

         * THIRD_ORDER-This technique tends to create a smoother color change
         and uses less storage in the auxiliary table, but it may take longer
         to process compared to the color grid surface. All input pixels are
         altered toward multiple points obtained from the cubic surface.
     target_raster {Raster Dataset / Raster Layer / Internet Tiled Layer / Map Server Layer}:
         The raster you want to use to color balance the other images. The
         balance method and color surface type, if applicable, will be derived
         from this image.
     exclude_raster {Raster Layer}:
         Apply a mask before color balancing the mosaic dataset. Create the
         mask using the Generate Exclude Area tool.
     stretch_type {String}:
         Stretch the range of values before color balancing. Choose from one of
         the following options:

         * NONE-Use the original pixel values. This is the default.

         * ADAPTIVE-An adaptive prestretch will be applied before any
         processing takes place.

         * MINIMUM_MAXIMUM-Stretch the values between their actual minimum and
         maximum values.

         * STANDARD_DEVIATION-Stretch the values between the default number of
         standard deviations.
     gamma {Double}:
         Adjust the overall brightness of an image. A low value will minimize
         the contrast between moderate values by making them appear darker.
         Higher values increase the contrast by making them appear brighter.
     block_field {String}:
         The name of the field in a mosaic dataset's attribute table used to
         identify items that should be considered one item when performing some
         calculations and operations."""
    ...

@gptooldoc("ComputeDirtyArea_management", None)
def ComputeDirtyArea(
    in_mosaic_dataset=..., where_clause=..., timestamp=..., out_feature_class=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeDirtyArea_management(in_mosaic_dataset, {where_clause}, timestamp, out_feature_class)

       Identifies areas within a mosaic dataset that have changed since a
       specified point in time. This is used commonly when a mosaic dataset
       is updated or synchronized, or when derived products, such as cache,
       need to be updated. This tool will enable you to limit such processes
       to only the areas that have changed.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that you want to analyze for changes.
     where_clause {SQL Expression}:
         SQL expression to select specific rasters within the mosaic dataset on
         which to compute dirty areas.
     timestamp (String):
         Compute the areas that have changed since the input time. XML
         time syntax:

         * YYYY-MM-DDThh:mm:ss

         * YYYY-MM-DDThh:mm:ss.ssssZ

         * 2002-10-10T12:00:00.ssss-00:00

         * 2002-10-10T12:00:00+00:00
         Non-XML time syntax:

         * 2002/12/25 23:59:58.123

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class containing the areas that have changed."""
    ...

@gptooldoc("ComputeMosaicCandidates_management", None)
def ComputeMosaicCandidates(
    in_mosaic_dataset=...,
    maximum_overlap=...,
    maximum_area_loss=...,
    maximum_obliqueness_angle=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeMosaicCandidates_management(in_mosaic_dataset, {maximum_overlap}, {maximum_area_loss}, {maximum_obliqueness_angle})

       Finds the image candidates in a mosaic dataset that best represent the
       mosaic area.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset with densely overlapped images.
     maximum_overlap {Double}:
         The maximum amount of overlap between the mosaic dataset and the
         footprint of each image in the mosaic dataset. If the percentage of
         overlap is greater than this threshold, the image is excluded since it
         will have too much redundant information.The percentage is expressed
         as a decimal. For example, a maximum
         overlap of 60 percent is expressed as 0.6.
     maximum_area_loss {Double}:
         The maximum percentage of area that can be excluded by the candidate
         images. After the tool finds the best candidate images based on the
         maximum_overlap parameter value, it checks whether the maximum
         excluded area is below the threshold specified. If the excluded area
         is greater than the specified threshold, the tool will add candidate
         images to fill in some of the voids that were missing. These excluded
         areas will typically be along the border of the mosaic dataset.The
         percentage is expressed as a double. For example, a maximum
         excluded area of 5 percent is expressed as 0.05.
     maximum_obliqueness_angle {Double}:
         The maximum image obliqueness angle that will be used to filter
         images. Any image with an obliqueness angle larger than this value
         will not be used as a candidate. This parameter is measured in
         degrees. The default value is 15."""
    ...

@gptooldoc("CreateMosaicDataset_management", None)
def CreateMosaicDataset(
    in_workspace=...,
    in_mosaicdataset_name=...,
    coordinate_system=...,
    num_bands=...,
    pixel_type=...,
    product_definition=...,
    product_band_definitions=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateMosaicDataset_management(in_workspace, in_mosaicdataset_name, coordinate_system, {num_bands}, {pixel_type}, {product_definition}, {product_band_definitions;product_band_definitions...})

       Creates an empty mosaic dataset in a geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The path to the geodatabase.Starting at ArcGIS Pro 1.4, mosaic
         datasets created in Oracle,
         PostgreSQL, and SQL Server geodatabases will be created with the
         RASTERBLOB keyword. The RASTERBLOB keyword implements a transfer of
         the mosaic dataset catalog items to the DBMS. Mosaic datasets
         created with RASTERBLOB cannot be opened with
         earlier versions of the software. To create mosaic datasets that are
         backward compatible with earlier versions, alter the configuration
         keyword for RASTER_STORAGE to one of the following compatible
         keywords:

         * BINARY for PostgreSQL and SQL Server

         * BLOB for Oracle
     in_mosaicdataset_name (String):
         The name of the new mosaic dataset.
     coordinate_system (Coordinate System):
         The coordinate system that will be used for all of the items in the
         mosaic dataset.
     num_bands {Long}:
         The number of bands the raster datasets will have in the mosaic
         dataset.
     pixel_type {String}:
         Specifies the bit depth, or radiometric resolution, that will be used
         for the mosaic dataset. If not defined, the pixel type of the first
         raster dataset will be used.

         * 1_BIT-The pixel type will be a 1-bit unsigned integer. The values
         can be 0 or 1.

         * 2_BIT-The pixel type will be a 2-bit unsigned integer. The values
         supported can range from 0 to 3.

         * 4_BIT-The pixel type will be a 4-bit unsigned integer. The values
         supported can range from 0 to 15.

         * 8_BIT_UNSIGNED-The pixel type will be an unsigned 8-bit data type.
         The values supported can range from 0 to 255.

         * 8_BIT_SIGNED-The pixel type will be a signed 8-bit data type. The
         values supported can range from -128 to 127.

         * 16_BIT_UNSIGNED-The pixel type will be a 16-bit unsigned data type.
         The values can range from 0 to 65,535.

         * 16_BIT_SIGNED-The pixel type will be a 16-bit signed data type. The
         values can range from -32,768 to 32,767.

         * 32_BIT_UNSIGNED-The pixel type will be a 32-bit unsigned data type.
         The values can range from 0 to 4,294,967,295.

         * 32_BIT_SIGNED-The pixel type will be a 32-bit signed data type. The
         values can range from -2,147,483,648 to 2,147,483,647.

         * 32_BIT_FLOAT-The pixel type will be a 32-bit data type supporting
         decimals.

         * 64_BIT-The pixel type will be a 64-bit data type supporting
         decimals.
     product_definition {String}:
         Specifies whether a template is specific to the type of imagery you
         are working with or is generic. The generic options include the
         following standard raster data types:

         * NONE-No band ordering is specified for the mosaic dataset. This is
         the default.

         * NATURAL_COLOR_RGB-A 3-band mosaic dataset, with red, green, and blue
         wavelength ranges will be created. This is designed for natural color
         imagery.

         * NATURAL_COLOR_RGBI-A 4-band mosaic dataset, with red, green, blue,
         and near infrared wavelength ranges will be created.

         * VECTOR_FIELD_UV-A mosaic dataset displaying two variables will be
         created.

         * VECTOR_FIELD_MAGNITUDE_DIRECTION-A mosaic dataset displaying
         magnitude and direction will be created.

         * FALSE_COLOR_IRG-A 3-band mosaic dataset, with near infrared, red,
         and green wavelength ranges will be created.

         * BLACKSKY-A 3-band mosaic dataset using the BlackSky wavelength
         ranges will be created

         * DMCII_3BANDS-A 3-band mosaic dataset using the DMCii wavelength
         ranges will be created.

         * DEIMOS2_4BANDS-A 4-band mosaic dataset using the Deimos-2 wavelength
         ranges will be created.

         * DUBAISAT-2_4BANDS-A 4-band mosaic dataset using the DubaiSat-2
         wavelength ranges will be created.

         * FORMOSAT-2_4BANDS-A 4-band mosaic dataset using the FORMOSAT-2
         wavelength ranges will be created.

         * GEOEYE-1_4BANDS-A 4-band mosaic dataset using the GeoEye-1
         wavelength ranges will be created.

         * GF-1 PMS_4BANDS-A 4-band mosaic dataset using the Gaofen-1
         Panchromatic Multispectral Sensor wavelength ranges will be created.

         * GF-1 WFV_4BANDS-A 4-band mosaic dataset using the Gaofen-1 Wide
         Field of View Sensor wavelength ranges will be created.

         * GF-2 PMS_4BANDS-A 4-band mosaic dataset using the Gaofen-2
         Panchromatic Multispectral Sensor wavelength ranges will be created.

         * GF-4 PMI_4BANDS-A 4-band mosaic dataset using the Gaofen-4
         panchromatic and multispectral wavelength ranges will be created.

         * HJ 1A/1B CCD_4BANDS-A 4-band mosaic dataset using the Huan Jing-1
         CCD Multispectral or Hyperspectral Sensor wavelength ranges will be
         created.

         * IKONOS_4BANDS-A 4-band mosaic dataset using the IKONOS wavelength
         ranges will be created.

         * JILIN-1_3BANDS-A 3-band mosaic dataset using the Jilin-1 wavelength
         ranges will be created.

         * KOMPSAT-2_4BANDS-A 4-band mosaic dataset using the KOMPSAT-2
         wavelength ranges will be created.

         * KOMPSAT-3_4BANDS-A 4-band mosaic dataset using the KOMPSAT-3
         wavelength ranges will be created.

         * LANDSAT_6BANDS-A 6-band mosaic dataset using the Landsat 5 and 7
         wavelength ranges from the TM and ETM+ sensors will be created.

         * LANDSAT_8BANDS-An 8-band mosaic dataset using the LANDSAT 8
         wavelength ranges will be created.

         * LANDSAT_9BANDS-An 8-band mosaic dataset using the LANDSAT 9
         wavelength ranges will be created.

         * LANDSAT_MSS_4BANDS-A 4-band mosaic dataset using the Landsat
         wavelength ranges from the MSS sensor will be created.

         * PLANETSCOPE-A 5-band mosaic dataset using the PlanetScope wavelength
         ranges will be created.

         * PLEIADES-1_4BANDS-A 4-band mosaic dataset using the PLEIADES-1
         wavelength ranges will be created.

         * PLEIADES_NEO_6BANDS-A 6-band mosaic dataset using the Pleiades Neo
         wavelength ranges will be created.

         * QUICKBIRD_4BANDS-A 4-band mosaic dataset using the QuickBird
         wavelength ranges will be created.

         * RAPIDEYE_5BANDS-A 5-band mosaic dataset using the RapidEye
         wavelength ranges will be created.

         * SENTINEL2_13BANDS-A 13-band mosaic dataset using the Sentinel 2 MSI
         wavelength ranges will be created.

         * SKYSAT_4BANDS-A 4-band mosaic dataset using the SkySat-C MSI
         wavelength ranges will be created.

         * SPOT-5_4BANDS-A 4-band mosaic dataset using the SPOT-5 wavelength
         ranges will be created.

         * SPOT-6_4BANDS-A 4-band mosaic dataset using the SPOT-6 wavelength
         ranges will be created.

         * SPOT-7_4BANDS-A 4-band mosaic dataset using the SPOT-7 wavelength
         ranges will be created.

         * SUPERVIEW-1_4BANDS-A 4-band mosaic dataset using the SuperView-1
         wavelength ranges will be created.

         * TH-01_4BANDS-A 4-band mosaic dataset using the Tian Hui-1 wavelength
         ranges will be created.

         * WORLDVIEW-2_8BANDS-An 8-band mosaic dataset using the WorldView-2
         wavelength ranges will be created.

         * WORLDVIEW-3_8BANDS-An 8-band mosaic dataset using the WorldView-3
         wavelength ranges will be created.

         * WORLDVIEW-4_4BANDS-A 4-band mosaic dataset using the WorldView-4
         wavelength ranges will be created.

         * ZY1-02C PMS_3BANDS-A 3-band mosaic dataset using the ZiYuan-1
         panchromatic/multispectral wavelength ranges will be created.

         * ZY3-CRESDA_4BANDS-A 4-band mosaic dataset using the ZiYuan-3 CRESDA
         wavelength ranges will be created.

         * ZY3-SASMAC_4BANDS-A 4-band mosaic dataset using the ZiYuan-3 SASMAC
         wavelength ranges will be created.

         * CUSTOM-The number of bands and the average wavelength for each band
         are defined using the Product Band Definitions parameter
         (product_band_definitions in Python).
     product_band_definitions {Value Table}:
         The definitions of the bands. Edit product_definition when using the
         CUSTOM keyword by adjusting the wavelength ranges, changing the band
         order, and adding new bands."""
    ...

@gptooldoc("CreateReferencedMosaicDataset_management", None)
def CreateReferencedMosaicDataset(
    in_dataset=...,
    out_mosaic_dataset=...,
    coordinate_system=...,
    number_of_bands=...,
    pixel_type=...,
    where_clause=...,
    in_template_dataset=...,
    extent=...,
    select_using_features=...,
    lod_field=...,
    minPS_field=...,
    maxPS_field=...,
    pixelSize=...,
    build_boundary=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateReferencedMosaicDataset_management(in_dataset, out_mosaic_dataset, {coordinate_system}, {number_of_bands}, {pixel_type}, {where_clause}, {in_template_dataset}, {extent}, {select_using_features}, {lod_field}, {minPS_field}, {maxPS_field}, {pixelSize}, {build_boundary})

       Creates a separate mosaic dataset from items in an existing mosaic
       dataset.

    INPUTS:
     in_dataset (Mosaic Dataset / Mosaic Layer):
         The mosaic dataset from which items will be selected.
     coordinate_system {Coordinate System}:
         The projection for the output mosaic dataset.
     number_of_bands {Long}:
         The number of bands that the referenced mosaic dataset will have.
     pixel_type {String}:
         The bit depth, or radiometric resolution, of the mosaic dataset. If
         this is not defined, it will be taken from the first raster dataset.

         * 1_BIT-The pixel type will be a 1-bit unsigned integer. The values
         can be 0 or 1.

         * 2_BIT-The pixel type will be a 2-bit unsigned integer. The values
         supported can range from 0 to 3.

         * 4_BIT-The pixel type will be a 4-bit unsigned integer. The values
         supported can range from 0 to 15.

         * 8_BIT_UNSIGNED-The pixel type will be an unsigned 8-bit data type.
         The values supported can range from 0 to 255.

         * 8_BIT_SIGNED-The pixel type will be a signed 8-bit data type. The
         values supported can range from -128 to 127.

         * 16_BIT_UNSIGNED-The pixel type will be a 16-bit unsigned data type.
         The values can range from 0 to 65,535.

         * 16_BIT_SIGNED-The pixel type will be a 16-bit signed data type. The
         values can range from -32,768 to 32,767.

         * 32_BIT_UNSIGNED-The pixel type will be a 32-bit unsigned data type.
         The values can range from 0 to 4,294,967,295.

         * 32_BIT_SIGNED-The pixel type will be a 32-bit signed data type. The
         values can range from -2,147,483,648 to 2,147,483,647.

         * 32_BIT_FLOAT-The pixel type will be a 32-bit data type supporting
         decimals.

         * 64_BIT-The pixel type will be a 64-bit data type supporting
         decimals.
     where_clause {SQL Expression}:
         An SQL expression to select raster datasets that will be included in
         the output mosaic dataset.
     in_template_dataset {Raster Layer / Feature Layer}:
         Select raster datasets based on the extent of another image or feature
         class. Raster datasets that lay along the defined extent will be
         included in the mosaic dataset. To manually input the minimum and
         maximum coordinates for the extent, use the Extent parameter.
     extent {Envelope}:
         The minimum and maximum coordinates for the extent.
     select_using_features {Boolean}:
         Limit the extent to the shape or envelope when a feature class is
         specified in the in_template_dataset parameter.

         * SELECT_USING_FEATURES-Select using the shape of the feature. This is
         the default.

         * NO_SELECT_USING_FEATURES-Select using the extent of the data in the
         feature class.
     lod_field {Field}:
         This parameter has been deprecated and is ignored in tool execution.
         It remains for backward compatibility reasons.
     minPS_field {Field}:
         Specify a field from the footprint attribute table that defines the
         minimum cell size for displaying the mosaic dataset; otherwise, only a
         footprint will be displayed.
     maxPS_field {Field}:
         Specify a field from the footprint attribute table that defines the
         maximum cell size for displaying the mosaic dataset; otherwise, only a
         footprint will be displayed.
     pixelSize {Double}:
         Set a maximum cell size to display the mosaic instead of specifying a
         field. If you zoom out beyond this cell size, only the footprint will
         be displayed.
     build_boundary {Boolean}:
         Rebuild the boundary. If the selection covers a smaller area than the
         source mosaic dataset, this is recommended.This is only available if
         the mosaic dataset is created in a
         geodatabase.

         * BUILD_BOUNDARY-The boundary will be generated or updated. This is
         the default.

         * NO_BOUNDARY-The boundary will not be generated.

    OUTPUTS:
     out_mosaic_dataset (Mosaic Dataset):
         The referenced mosaic dataset to be created."""
    ...

@gptooldoc("DefineMosaicDatasetNoData_management", None)
def DefineMosaicDatasetNoData(
    in_mosaic_dataset=...,
    num_bands=...,
    bands_for_nodata_value=...,
    bands_for_valid_data_range=...,
    where_clause=...,
    Composite_nodata_value=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DefineMosaicDatasetNoData_management(in_mosaic_dataset, num_bands, {bands_for_nodata_value;bands_for_nodata_value...}, {bands_for_valid_data_range;bands_for_valid_data_range...}, {where_clause}, {Composite_nodata_value})

       Specifies one or more values to be represented as NoData.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset where you want to update the NoData values.
     num_bands (Long):
         The number of bands in the mosaic dataset.
     bands_for_nodata_value {Value Table}:
         Define values for each or all bands. Each band can have a unique
         NoData value defined, or the same value can be specified for all
         bands. If you want to define multiple NoData values for each band
         selection, use a space delimiter between each NoData value within the
         bands_for_nodata_value parameter.The Mask function inserted by this
         tool is inserted before the
         Composite Bands function in the function chain. Therefore, if the
         function chain for each raster within the mosaic dataset contains the
         Composite Bands function, or if your raster data was added with a
         raster type that adds the Composite Bands function to each raster's
         function chain, then any value you specify will apply to all bands.
     bands_for_valid_data_range {Value Table}:
         Specify a range of values to display for each band. Values outside of
         this range will be classified as NoData. When working with composite
         bands, the range will apply to all bands.
     where_clause {SQL Expression}:
         An SQL statement to select specific raster in the mosaic dataset. Only
         the selected rasters will have their NoData values changed.
     Composite_nodata_value {Boolean}:
         Choose whether all bands must be NoData in order for the pixel to be
         classified as NoData.

         * NO_COMPOSITE_NODATA-If any of the bands have pixels of NoData, then
         the pixel is classified as NoData. This is the default.

         * COMPOSITE_NODATA-All of the bands must have pixels of NoData in
         order for the pixel to be classified as NoData."""
    ...

@gptooldoc("DefineOverviews_management", None)
def DefineOverviews(
    in_mosaic_dataset=...,
    overview_image_folder=...,
    in_template_dataset=...,
    extent=...,
    pixel_size=...,
    number_of_levels=...,
    tile_rows=...,
    tile_cols=...,
    overview_factor=...,
    force_overview_tiles=...,
    resampling_method=...,
    compression_method=...,
    compression_quality=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DefineOverviews_management(in_mosaic_dataset, {overview_image_folder}, {in_template_dataset}, {extent}, {pixel_size}, {number_of_levels}, {tile_rows}, {tile_cols}, {overview_factor}, {force_overview_tiles}, {resampling_method}, {compression_method}, {compression_quality})

       Lets you set how mosaic dataset overviews are generated. The settings
       made with this tool are used by the Build Overviews tool.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that you want to build overviews on.
     overview_image_folder {Workspace}:
         The folder or geodatabase to store the overviews.
     in_template_dataset {Raster Layer / Feature Layer}:
         A raster dataset or feature class to define the extent of the
         overviews.
     extent {Envelope}:
         Set the extent using minimum and maximum x and y coordinates.This is
         specified as space delimited in the following order: X-minimum
         X-maximum Y-minimum Y-maximum.The mosaic dataset boundary will
         determine the extent of the overviews
         if you do not define an extent.
     pixel_size {Double}:
         If you prefer not to use all the raster's pyramids, specify a base
         pixel size at which your overviews will be generated.The units for
         this parameter are the same as the spatial reference of
         the mosaic dataset.
     number_of_levels {Long}:
         Specify the number of levels of overviews that you want to generate
         overviews. A value of -1 will determine an optimal value for you.
     tile_rows {Long}:
         Set the number of rows (in pixels) for each tile.Larger values will
         result in fewer, larger individual overviews, and
         increase the likelihood that you will need to regenerate lower level
         overviews. A smaller value will result in more, smaller files.
     tile_cols {Long}:
         Set the number of columns (in pixels) for each tile.Larger values will
         result in fewer, larger individual overviews, and
         increase the likelihood that you will need to regenerate lower level
         overviews. A smaller value will result in more, smaller files.
     overview_factor {Long}:
         Set a ratio to determine the size of the next overview. For example,
         if the cell size of the first level is 10, and the overview factor is
         3, then the next overview pixel size will be 30.
     force_overview_tiles {Boolean}:
         Generate overviews at all levels, or only above existing pyramid
         levels.

         * NO_FORCE_OVERVIEW_TILES-Create overviews above the raster pyramid
         levels. This is the default.

         * FORCE_OVERVIEW_TILES-Create overviews at all levels.
     resampling_method {String}:
         Choose an algorithm for aggregating pixel values in the overviews.

         * NEAREST-The fastest resampling method because it minimizes changes
         to pixel values. Suitable for discrete data, such as land cover. If
         the Raster MetadataData Type is thematic, then nearest neighbor will
         be the default.

         * BILINEAR-Calculates the value of each pixel by averaging (weighted
         for distance) the values of the surrounding 4 pixels. Suitable for
         continuous data.This is the default, unless the Raster Metadata Data
         Type is thematic.

         * CUBIC-Calculates the value of each pixel by fitting a smooth curve
         based on the surrounding 16 pixels. Produces the smoothest image, but
         can create values outside of the range found in the source data.
         Suitable for continuous data.
     compression_method {String}:
         Define the type of data compression to store the overview images.

         * JPEG-A lossy compression. This is the default, unless the Raster
         Metadata Data Type is thematic. This compression method is only valid
         if the mosaic dataset items adhere to JPEG specifications.

         * JPEG_YCbCr-A lossy compression using the luma (Y) and chroma (Cb and
         Cr) color space components.

         * None-No data compression.

         * LZW-A lossless compression. If the Raster Metadata Data Type is
         thematic, then nearest neighbor will be the default.
     compression_quality {Long}:
         Choose a value from 1 - 100. Higher values generate better quality
         outputs, but they create larger files."""
    ...

@gptooldoc("DeleteMosaicDataset_management", None)
def DeleteMosaicDataset(
    in_mosaic_dataset=..., delete_overview_images=..., delete_item_cache=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteMosaicDataset_management(in_mosaic_dataset, {delete_overview_images}, {delete_item_cache})

       Deletes a mosaic dataset, its overviews, and its item cache from disk.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that you want to delete.
     delete_overview_images {Boolean}:
         Delete all overviews associated with the mosaic dataset.

         * DELETE_OVERVIEW_IMAGES-Delete the overviews associated with the
         mosaic dataset. This is the default.

         * NO_DELETE_OVERVIEW_IMAGES-Do not delete the overviews.
     delete_item_cache {Boolean}:
         Delete the item cache associated with the mosaic dataset.

         * DELETE_ITEM_CACHE-Delete the item cache associated with the mosaic
         dataset. This is the default.

         * NO_DELETE_ITEM_CACHE-Do not delete the item cache."""
    ...

@gptooldoc("EditRasterFunction_management", None)
def EditRasterFunction(
    in_mosaic_dataset=...,
    edit_mosaic_dataset_item=...,
    edit_options=...,
    function_chain_definition=...,
    location_function_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """EditRasterFunction_management(in_mosaic_dataset, {edit_mosaic_dataset_item}, {edit_options}, {function_chain_definition}, {location_function_name})

       Adds, replaces, or removes a function chain in a mosaic dataset or a
       raster layer that contains a raster function.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer / Raster Layer):
         The mosaic dataset or a raster layer. If you use a raster layer, it
         must have a function applied.
     edit_mosaic_dataset_item {Boolean}:
         Determines if edits affect functions or the entire mosaic dataset.

         * EDIT_MOSAIC_DATASET-Edits affect the functions associated with the
         mosaic dataset. This is the default.

         * EDIT_MOSAIC_DATASET_ITEM-Edits affect the functions associated with
         all of the items within the mosaic dataset.
     edit_options {String}:
         Insert, replace, or remove a function chain.

         * INSERT-Insert the function chain above the Function Name of the
         existing chain. Specify the function chain in the
         location_function_name parameter. This is the default.

         * REPLACE-Replace the existing function chain with the function chain
         specified in this tool. Specify the function chain below in the
         location_function_name parameter.

         * REMOVE-Remove the function chain starting from the function
         specified in the location_function_name parameter.
     function_chain_definition {File}:
         Choose the function chain (rft.xml file) that you want to insert or
         replace.
     location_function_name {String}:
         Choose where to insert, replace, or remove the function chain within
         the existing function chain."""
    ...

@gptooldoc("ExportMosaicDatasetGeometry_management", None)
def ExportMosaicDatasetGeometry(
    in_mosaic_dataset=..., out_feature_class=..., where_clause=..., geometry_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportMosaicDatasetGeometry_management(in_mosaic_dataset, out_feature_class, {where_clause}, {geometry_type})

       Creates a feature class showing the footprints, boundary, seamlines or
       spatial resolutions of a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that you want to export the geometry from.
     where_clause {SQL Expression}:
         An SQL expression to export specific rasters in the mosaic dataset.
     geometry_type {String}:
         The type of geometry to export.

         * FOOTPRINT-Create a feature class showing the footprints of each
         image.

         * BOUNDARY-Create a feature class showing the boundary of the mosaic
         dataset.

         * SEAMLINE-Create a feature class showing the seamlines.

         * LEVEL-Create a feature class based on cell size level of features
         in your mosaic dataset.

    OUTPUTS:
     out_feature_class (Feature Class):
         Name the feature class you are creating."""
    ...

@gptooldoc("ExportMosaicDatasetItems_management", None)
def ExportMosaicDatasetItems(
    in_mosaic_dataset=...,
    out_folder=...,
    out_base_name=...,
    where_clause=...,
    format=...,
    nodata_value=...,
    clip_type=...,
    template_dataset=...,
    cell_size=...,
    image_space=...,
    remove_distortion=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportMosaicDatasetItems_management(in_mosaic_dataset, out_folder, {out_base_name}, {where_clause}, {format}, {nodata_value}, {clip_type}, {template_dataset}, {cell_size}, {image_space}, {remove_distortion})

       Saves a copy of processed images in a mosaic dataset to a specified
       folder and raster file format.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that contains the images to be exported.
     out_folder (Folder):
         The folder where the images will be saved.
     out_base_name {String}:
         A prefix to add to the name of each item after it is copied. The
         prefix will be followed by the Object ID value from the mosaic dataset
         footprints table.If no base name is set, the text in the Name field of
         the mosaic
         dataset item will be used.
     where_clause {SQL Expression}:
         An SQL expression that will be used to save selected images in the
         mosaic dataset. For more information about SQL syntax, see SQL
         reference for query expressions used in ArcGIS.
     format {String}:
         Specifies the format that will be used for the output raster datasets.

         * TIFF-TIFF format will be used. This is the default.

         * Cloud Optimized GeoTIFF-Cloud Optimized GeoTIFF format will be used.

         * BMP-BMP format will be used.

         * ENVI-ENVI DAT format will be used.

         * Esri BIL-Esri BIL format will be used.

         * Esri BIP-Esri BIP format will be used.

         * Esri BSQ-Esri BSQ format will be used.

         * GIF-GIF format will be used.

         * GRID-Esri Grid format will be used.

         * IMAGINE IMAGE-ERDAS IMAGINE format will be used.

         * JP2-JPEG 2000 format will be used.

         * JPEG-JPEG format will be used.

         * PNG-PNG format will be used.

         * CRF-Cloud raster format will be used.

         * MRF-Meta raster format will be used.
     nodata_value {String}:
         All the pixels with the specified value will be set to NoData in the
         output raster dataset.It is recommended that you specify a NoData
         value if the output images
         will be clipped.
     clip_type {String}:
         Specifies the output extent of the raster datasets. If you choose an
         extent or feature class that covers an area larger than the raster
         data, the output will have the larger extent.

         * NONE-The output will not be clipped. This is the default.

         * EXTENT-An extent will be used to clip the output.

         * FEATURE_CLASS-A feature class extent will be used to clip the
         output.
     template_dataset {Extent}:
         The feature class or bounding box that will be used to limit the
         extent.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     cell_size {Point}:
         The horizontal (x) and vertical (y) dimensions of the output cells.If
         the cell size is not specified, the spatial resolution of the input
         will be used.
     image_space {Boolean}:
         Specifies whether raster items will be exported in map space or image
         space.

         * MAPSPACE-Raster items will be exported in map space. This is the
         default.

         * IMAGESPACE-Raster items will be exported in image space.
     remove_distortion {Boolean}:
         Specifies whether lens distortion will be removed from the exported
         raster in image space.

         * REMOVED-Lens distortion will be removed from the exported raster in
         image space.

         * NOTREMOVE-Lens distortion will not be removed from the exported
         raster in image space. This is the default."""
    ...

@gptooldoc("ExportMosaicDatasetPaths_management", None)
def ExportMosaicDatasetPaths(
    in_mosaic_dataset=...,
    out_table=...,
    where_clause=...,
    export_mode=...,
    types_of_paths=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportMosaicDatasetPaths_management(in_mosaic_dataset, out_table, {where_clause}, {export_mode}, {types_of_paths;types_of_paths...})

       Creates a table of the file path for each item in a mosaic dataset.
       You can specify whether the table contains all the file paths or just
       the ones that are broken.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset containing the file paths to export.
     where_clause {SQL Expression}:
         An SQL expression to select specific rasters for export.
     export_mode {String}:
         Populate the table with either all of the paths, or only the broken
         paths.

         * ALL-Export all paths to the table. This is the default.

         * BROKEN-Export only broken paths to the table.
     types_of_paths {String}:
         Choose to export file paths from only the source raster, only the
         cache, or both. The default is to export all path types.

         * RASTER-Export file paths from rasters.

         * ITEM_CACHE-Export file paths from item cache.

    OUTPUTS:
     out_table (Table):
         The table to create. The table can be a geodatabase table or a .dbf
         file.The SourceOID field in the output table is derived from the OID
         of the
         row in the original mosaic dataset table."""
    ...

@gptooldoc("GenerateExcludeArea_management", None)
def GenerateExcludeArea(
    in_raster=...,
    out_raster=...,
    pixel_type=...,
    generate_method=...,
    max_red=...,
    max_green=...,
    max_blue=...,
    max_white=...,
    max_black=...,
    max_magenta=...,
    max_cyan=...,
    max_yellow=...,
    percentage_low=...,
    percentage_high=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateExcludeArea_management(in_raster, out_raster, pixel_type, generate_method, {max_red}, {max_green}, {max_blue}, {max_white}, {max_black}, {max_magenta}, {max_cyan}, {max_yellow}, {percentage_low}, {percentage_high})

       Masks pixels based on their color or by clipping a range of values.
       The output of this tool is used as an input to the Color Balance
       Mosaic Dataset tool to eliminate areas such as clouds and water that
       can skew the statistics used to color balance multiple images.

    INPUTS:
     in_raster (Mosaic Dataset / Raster Dataset / Raster Layer):
         The raster or mosaic dataset layer that you want to mask.
     pixel_type (String):
         Choose the pixel depth of your input raster dataset. 8-bit is the
         default value; however, raster datasets with a greater bit-depth will
         need to have the color mask and histogram values scaled accordingly.

         * 8_BIT-The input raster dataset has values from 0 to 255. This is the
         default.

         * 11_BIT-The input raster dataset has values from 0 to 2047.

         * 12_BIT-The input raster dataset has values from 0 to 4095.

         * 16_BIT-The input raster dataset has values from 0 to 65535.
     generate_method (String):
         Create your mask based on the color of the pixels or by clipping high
         and low values.

         * COLOR_MASK-Set the maximum color values to include in the output.
         This is the default.

         * HISTOGRAM_PERCENTAGE-Remove a percentage of high and low pixel
         values.
     max_red {Double}:
         The maximum red value to exclude. The default is 255.
     max_green {Double}:
         The maximum green value to exclude. The default is 255.
     max_blue {Double}:
         The maximum blue value to exclude. The default is 255.
     max_white {Double}:
         The maximum white value to exclude. The default is 255.
     max_black {Double}:
         The maximum black value to exclude. The default is 0.
     max_magenta {Double}:
         The maximum magenta value to exclude. The default is 255.
     max_cyan {Double}:
         The maximum cyan value to exclude. The default is 255.
     max_yellow {Double}:
         The maximum yellow value to exclude. The default is 255.
     percentage_low {Double}:
         Exclude this percentage of the lowest pixel values. The default is 0.
     percentage_high {Double}:
         Exclude this percentage of the highest pixel values. The default is
         100.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location and format for the dataset you are creating. When
         storing a raster dataset in a geodatabase, do not add a file extension
         to the name of the raster dataset. When storing your raster dataset to
         a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a
         Compression type and Compression Quality within the Environment
         Settings."""
    ...

@gptooldoc("GenerateRasterCollection_management", None)
def GenerateRasterCollection(
    out_raster_collection=...,
    collection_builder=...,
    collection_builder_arguments=...,
    raster_function=...,
    raster_function_arguments=...,
    collection_properties=...,
    generate_rasters=...,
    out_workspace=...,
    format=...,
    out_base_name=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateRasterCollection_management(out_raster_collection, collection_builder, collection_builder_arguments;collection_builder_arguments..., {raster_function}, {raster_function_arguments;raster_function_arguments...}, {collection_properties;collection_properties...}, {generate_rasters}, {out_workspace}, {format}, {out_base_name})

       Performs batch analysis or processing on image collections contained
       in a mosaic dataset. The images in the input mosaic dataset can be
       processed individually or as groups.

    INPUTS:
     collection_builder (String):
         The input image collection. It can be seen as a template that contains
         arguments such as the source mosaic dataset path, filters to extract a
         subset from the input data source, and so on.Currently, this tool only
         supports SIMPLE_COLLECTION, which allows you
         to define a single data source and a query filter for the data source.

         * SIMPLE_COLLECTION-Allows you to define a data source and a query
         filter.
     collection_builder_arguments (Value Table):
         The list of arguments to create a subset collection of the mosaic
         dataset.This tool only supports the data source and filter to subset
         the
         mosaic dataset. The DataSource and WhereClause values must be
         completed, otherwise the tool cannot be executed.

         * DataSource-The path of the data source.

         * WhereClause-The filter used to subset the mosaic dataset.
     raster_function {File / String}:
         The path to a raster function template file (.rft.xml or .rft.json).
         The raster function template will be applied to every item in the
         input mosaic dataset. The Function Editor can be used to create the
         template. If no RFT is defined, this tool will create the output
         mosaic based on the collection_builder_arguments parameter.
     raster_function_arguments {Value Table}:
         The parameters associated with the function chain.For example, if the
         function chain applies the NDVI function, set the
         visible and infrared IDs. The raster variable name of the RFT should
         be the Tag field value in the input data source.
     collection_properties {Value Table}:
         The output mosaic dataset key properties. The key metadata
         properties that are available is based on the
         type of sensor that captured the imagery. Some examples of key
         metadata properties include the following:

         * SensorName

         * ProductName

         * AcquisitionDate

         * CloudCover

         * SunAzimuth

         * SunElevation

         * SensorAzimuth

         * SensorElevation

         * Off-nadirAngle

         * BandName

         * MinimumWavelength

         * MaximumWavelength

         * RadianceGain

         * RadianceBias

         * SolarIrradiance

         * ReflectanceGain

         * ReflectanceBias
     generate_rasters {Boolean}:
         Choose to generate raster dataset files of the mosaic dataset items,
         after the application of the RFT.

         * NO_GENERATE_RASTERS-The processing defined by the raster function
         template will be appended to the image items from the input data
         source to produce an image item in the output mosaic dataset. This is
         the default.

         * GENERATE_RASTERS-Create raster datasets on disk. You will also need
         to specify the out_workspace and format.
     out_workspace {Folder / String}:
         Defines the output location for the persisted raster datasets, if the
         generate_rasters parameter is set to GENERATE_RASTERS.The naming
         convention for the output raster files is
         oid_<oid#>_<Unique_GUID>.
     format {String}:
         The format type of the raster to be generated.

         * TIFF-Tagged Image File Format (TIFF)

         * IMAGINE Image-ERDAS IMAGINE file

         * CRF-Cloud Raster Format. This is the default.

         * MRF-Meta Raster Format
     out_base_name {String}:
         Defines the output base name for the persisted raster datasets, if the
         generate_rasters parameter is set to GENERATE_RASTERS.

    OUTPUTS:
     out_raster_collection (Mosaic Dataset):
         The full path of the mosaic dataset to be created. The mosaic dataset
         must be stored in a geodatabase."""
    ...

@gptooldoc("ImportMosaicDatasetGeometry_management", None)
def ImportMosaicDatasetGeometry(
    in_mosaic_dataset=...,
    target_featureclass_type=...,
    target_join_field=...,
    input_featureclass=...,
    input_join_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportMosaicDatasetGeometry_management(in_mosaic_dataset, target_featureclass_type, target_join_field, input_featureclass, input_join_field)

       Modifies the geometry for the footprints, boundary, or seamlines in a
       mosaic dataset to match those in a feature class.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset whose geometries you want to edit.
     target_featureclass_type (String):
         The geometry that you want to change.

         * FOOTPRINT-The footprint polygons in the mosaic dataset

         * SEAMLINE-The seamline polygons in the mosaic dataset

         * BOUNDARY-The boundary polygon in the mosaic dataset
     target_join_field (Field):
         The field in the mosaic dataset to use as a basis for the join.
     input_featureclass (Feature Layer):
         The feature class with the new geometry.
     input_join_field (Field):
         The field in the input_featureclass to use as a basis for the join.If
         the input_featureclass has more than 1,000 records, add an index on
         this field by running the Add_Attribute_Index tool. If your mosaic
         dataset is very large and the join field is not indexed, the tool will
         take much longer to complete."""
    ...

@gptooldoc("MergeMosaicDatasetItems_management", None)
def MergeMosaicDatasetItems(
    in_mosaic_dataset=...,
    where_clause=...,
    block_field=...,
    max_rows_per_merged_items=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MergeMosaicDatasetItems_management(in_mosaic_dataset, {where_clause}, {block_field}, {max_rows_per_merged_items})

       Groups multiple items in a mosaic dataset together as one item.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that has the items that you want to merge.
     where_clause {SQL Expression}:
         An SQL expression to select specific rasters to merge in the mosaic
         dataset.
     block_field {Field}:
         The field in the attribute table that you want to use to group images.
         Only date, numeric, and string fields can be specified as Block
         fields.
     max_rows_per_merged_items {Long}:
         Limits the number of items to merge. If the maximum is exceeded, the
         tool will create multiple merged items. The default is 1,000 rows."""
    ...

@gptooldoc("MosaicDatasetToMobileMosaicDataset_management", None)
def MosaicDatasetToMobileMosaicDataset(
    in_mosaic_dataset=...,
    out_mobile_gdb=...,
    mosaic_dataset_name=...,
    where_clause=...,
    selection_feature=...,
    out_data_folder=...,
    convert_rasters=...,
    out_name_prefix=...,
    format=...,
    compression_method=...,
    compression_quality=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MosaicDatasetToMobileMosaicDataset_management(in_mosaic_dataset, out_mobile_gdb, mosaic_dataset_name, {where_clause}, {selection_feature}, {out_data_folder}, {convert_rasters}, {out_name_prefix}, {format}, {compression_method}, {compression_quality})

       Converts a mosaic dataset into a mobile mosaic dataset compatible with
       ArcGIS Maps SDK. A mobile mosaic dataset resides in a mobile
       geodatabase.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The mosaic dataset to be converted to a mobile mosaic dataset.
     mosaic_dataset_name (String):
         The name of the mobile mosaic dataset to be created.
     where_clause {SQL Expression}:
         An SQL expression to select specific items to add to the mobile mosaic
         dataset.
     selection_feature {Extent}:
         The mosaic dataset items to be included in the output based on the
         extent of another image or feature class. Items that lay along the
         defined extent will be included in the mosaic dataset. They will not
         be clipped.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     out_data_folder {Folder}:
         If specified, the tool will create a copy of the source data in this
         folder. If convert_rasters='ALWAYS', any raster functions associated
         with the mosaic dataset are processed before creating the copy.
     convert_rasters {Boolean}:
         Applies the raster functions associated with the input mosaic dataset
         before creating the mobile mosaic dataset. If you have raster
         functions that are not supported by ArcGIS Maps SDK, the tool will
         return the appropriate error message.

         * AS_REQUIRED-Do not convert raster items with functions that are not
         supported by ArcGIS Maps SDK. This is the default.

         * ALWAYS-Apply the raster function chain and save the output as
         converted raster items.
     out_name_prefix {String}:
         Appends a prefix to each item, which is copied or converted into the
         output data folder.
     format {String}:
         The format for the rasters written to the output data folder.

         * TIFF-TIFF format

         * PNG-PNG format

         * JPEG-JPEG format

         * JP2-JPEG2000 format
     compression_method {String}:
         The method of compression for transmitting the mosaicked image from
         the computer to the display (or from the server to the client).

         * NONE-No compression will be used.

         * JPEG-Compresses up to 8:1 and is suitable for backdrops.

         * LZW-Compresses approximately 2:1. Suitable for analysis.

         * RLE-Lossless compression. Suitable for categorical datasets.
     compression_quality {Long}:
         A value from 0 to 100. A higher number means better image quality but
         less compression. This value only applies when JPEG or JP2 is
         specified as the compression method.

    OUTPUTS:
     out_mobile_gdb (File):
         The geodatabase where the converted mosaic dataset will be created."""
    ...

@gptooldoc("RemoveRastersFromMosaicDataset_management", None)
def RemoveRastersFromMosaicDataset(
    in_mosaic_dataset=...,
    where_clause=...,
    update_boundary=...,
    mark_overviews_items=...,
    delete_overview_images=...,
    delete_item_cache=...,
    remove_items=...,
    update_cellsize_ranges=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveRastersFromMosaicDataset_management(in_mosaic_dataset, {where_clause}, {update_boundary}, {mark_overviews_items}, {delete_overview_images}, {delete_item_cache}, {remove_items}, {update_cellsize_ranges})

       Removes selected rasters from a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset containing the rasters that will be removed.
     where_clause {SQL Expression}:
         An SQL expression to select the raster datasets that will be removed
         from the mosaic dataset.You must specify a selection or a query;
         otherwise, the tool will not
         run. To delete all the records from the mosaic dataset, specify a
         query that selects all the rasters, such as "OBJECTID>=0".
     update_boundary {Boolean}:
         Specifies whether the boundary polygon of the mosaic dataset will be
         updated. By default, the boundary merges all the footprint polygons to
         create a single boundary representing the extent of the valid pixels.

         * UPDATE_BOUNDARY-The boundary polygon of the mosaic dataset will be
         updated. This is the default.

         * NO_BOUNDARY-The boundary polygon of the mosaic dataset will not be
         updated.
     mark_overviews_items {Boolean}:
         Specifies whether affected overviews will be identified.When the
         rasters in a mosaic dataset have been removed, overviews
         created using those rasters may no longer be accurate. Use this
         parameter to identify affected overviews so they can be updated or
         removed if they are no longer needed.

         * MARK_OVERVIEW_ITEMS-The affected overviews will be identified. This
         is the default.

         * NO_MARK_OVERVIEW_ITEMS-The affected overviews will not be
         identified.
     delete_overview_images {Boolean}:
         Specifies whether the overviews associated with the selected rasters
         will be removed.

         * DELETE_OVERVIEW_IMAGES-The overviews associated with the selected
         rasters will be deleted. This is the default.

         * NO_DELETE_OVERVIEW_IMAGES-The overviews associated with the selected
         rasters will not be deleted.
     delete_item_cache {Boolean}:
         Specifies whether the cache that is based on any source raster dataset
         that will be removed from the mosaic dataset will also be removed.

         * DELETE_ITEM_CACHE-The cache that is based on any source raster
         dataset that will be removed from the mosaic dataset will also be
         removed. This is the default.

         * NO_DELETE_ITEM_CACHE-The cache will not be removed and will remain a
         part of the mosaic dataset.
     remove_items {Boolean}:
         Specifies whether mosaic dataset items will be removed.

         * REMOVE_MOSAICDATASET_ITEMS-Mosaic dataset items will be removed.
         This is the default.

         * NO_REMOVE_MOSAICDATASET_ITEMS-Mosaic dataset items will not be
         removed.
     update_cellsize_ranges {Boolean}:
         Specifies whether the cell size ranges for the mosaic dataset will be
         updated.

         * UPDATE_CELL_SIZES-The cell size ranges for the mosaic dataset will
         be updated. Use this if you are removing all of the imagery at a
         specific cell size. This is the default.

         * NO_CELL_SIZES-The cell size ranges for the mosaic dataset will not
         be updated."""
    ...

@gptooldoc("RepairMosaicDatasetPaths_management", None)
def RepairMosaicDatasetPaths(
    in_mosaic_dataset=..., paths_list=..., where_clause=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RepairMosaicDatasetPaths_management(in_mosaic_dataset, paths_list;paths_list..., {where_clause})

       Resets paths to source imagery if you have moved or copied a mosaic
       dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset with the broken paths.
     paths_list (Value Table):
         A list of the paths to remap. Include the current path stored in the
         mosaic dataset and the path to which it will be changed. You can enter
         an asterisk (*) as the original path if you wish to change all your
         paths.
     where_clause {SQL Expression}:
         An SQL expression to limit the repairs to selected rasters within the
         mosaic dataset."""
    ...

@gptooldoc("SetMosaicDatasetProperties_management", None)
def SetMosaicDatasetProperties(
    in_mosaic_dataset=...,
    rows_maximum_imagesize=...,
    columns_maximum_imagesize=...,
    allowed_compressions=...,
    default_compression_type=...,
    JPEG_quality=...,
    LERC_Tolerance=...,
    resampling_type=...,
    clip_to_footprints=...,
    footprints_may_contain_nodata=...,
    clip_to_boundary=...,
    color_correction=...,
    allowed_mensuration_capabilities=...,
    default_mensuration_capabilities=...,
    allowed_mosaic_methods=...,
    default_mosaic_method=...,
    order_field=...,
    order_base=...,
    sorting_order=...,
    mosaic_operator=...,
    blend_width=...,
    view_point_x=...,
    view_point_y=...,
    max_num_per_mosaic=...,
    cell_size_tolerance=...,
    cell_size=...,
    metadata_level=...,
    transmission_fields=...,
    use_time=...,
    start_time_field=...,
    end_time_field=...,
    time_format=...,
    geographic_transform=...,
    max_num_of_download_items=...,
    max_num_of_records_returned=...,
    data_source_type=...,
    minimum_pixel_contribution=...,
    processing_templates=...,
    default_processing_template=...,
    time_interval=...,
    time_interval_units=...,
    product_definition=...,
    product_band_definitions=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetMosaicDatasetProperties_management(in_mosaic_dataset, {rows_maximum_imagesize}, {columns_maximum_imagesize}, {allowed_compressions;allowed_compressions...}, {default_compression_type}, {JPEG_quality}, {LERC_Tolerance}, {resampling_type}, {clip_to_footprints}, {footprints_may_contain_nodata}, {clip_to_boundary}, {color_correction}, {allowed_mensuration_capabilities;allowed_mensuration_capabilities...}, {default_mensuration_capabilities}, {allowed_mosaic_methods;allowed_mosaic_methods...}, {default_mosaic_method}, {order_field}, {order_base}, {sorting_order}, {mosaic_operator}, {blend_width}, {view_point_x}, {view_point_y}, {max_num_per_mosaic}, {cell_size_tolerance}, {cell_size}, {metadata_level}, {transmission_fields;transmission_fields...}, {use_time}, {start_time_field}, {end_time_field}, {time_format}, {geographic_transform;geographic_transform...}, {max_num_of_download_items}, {max_num_of_records_returned}, {data_source_type}, {minimum_pixel_contribution}, {processing_templates;processing_templates...}, {default_processing_template}, {time_interval}, {time_interval_units}, {product_definition}, {product_band_definitions;product_band_definitions...})

       Defines the defaults for displaying a mosaic dataset and serving it as
       an image service.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset with the properties that will be set.
     rows_maximum_imagesize {Long}:
         The maximum number of rows for the mosaicked image, generated by the
         mosaic dataset for each request. This can help control how much work
         the server has to do when clients view the imagery. A higher number
         will create a larger image but will increase the amount of time to
         process the mosaic dataset. If the value is too low, the image may not
         display.
     columns_maximum_imagesize {Long}:
         The maximum number of columns for the mosaicked image, generated by
         the mosaic dataset for each request. This can help control how much
         work the server has to do when clients view the imagery. A higher
         number will create a larger image but will increase the amount of time
         to process the mosaic dataset. If the value is too low, the image may
         not display.
     allowed_compressions {String}:
         Specifies the compression methods that will be used to transmit the
         mosaicked image from the computer to the display (or from the server
         to the client).

         * None-No compression will be used.

         * JPEG-Compression up to 8:1 will be used, which is suitable for
         backdrops.

         * LZ77-Compression of approximately 2:1 will be used, which is
         suitable for analysis.

         * LERC-Compression between 10:1 and 20:1 will be used, which is fast
         and suitable for serving raw imagery with high bit depth (12 bit to 32
         bit).
     default_compression_type {String}:
         Specifies the default compression type. The default compression must
         be in the list of values used for the allowed_compressions parameter
         or must be set in the mosaic dataset's Allowed Compression Methods
         property.

         * None-No compression will be used.

         * JPEG-Compression up to 8:1 will be used, which is suitable for
         backdrops.

         * LZ77-Compression of approximately 2:1 will be used, which is
         suitable for analysis.

         * LERC-Compression between 10:1 and 20:1 will be used, which is fast
         and suitable for serving raw imagery with high bit depth (12 bit to 32
         bit).
     JPEG_quality {Long}:
         The compression quality when using JPEG. Compression quality ranges
         from 1 to 100. A higher number means better image quality but less
         compression.
     LERC_Tolerance {Double}:
         The maximum per pixel error when using LERC compression. This value is
         specified in the units of the mosaic dataset. For example, if the
         error is 10 centimeters and the mosaic dataset is in meters, enter
         0.1.
     resampling_type {String}:
         Specifies how pixel values will be calculated when the dataset is
         displayed at small scales. Choose an appropriate technique based on
         the type of data.

         * NEAREST-The value of each pixel will be from the nearest
         corresponding pixel. This technique is suitable for discrete data,
         such as land cover.This is the fastest resampling technique. It
         minimizes the changes to pixel values since it uses the value from the
         nearest pixel.

         * BILINEAR-The value of each pixel will be calculated by averaging the
         values of the surrounding four pixels (based on distance). This
         technique is suitable for continuous data.

         * CUBIC-The value of each pixel will be calculated by fitting a smooth
         curve based on the surrounding 16 pixels. This technique produces the
         smoothest image but can create values outside of the range in the
         source data. It is suitable for continuous data.

         * MAJORITY-The value of each pixel will be based on the most popular
         value in a 3 by 3 window. This technique is suitable for discrete
         data.
     clip_to_footprints {Boolean}:
         Specifies whether rasters will be clipped to the footprint. Often the
         raster dataset and its footprint have the same extent. If they differ,
         the raster dataset can be clipped to the footprint.

         * NOT_CLIP-The rasters will not be clipped to the footprint. This is
         the default.

         * CLIP-The rasters will be clipped to the footprint.
     footprints_may_contain_nodata {Boolean}:
         Specifies whether pixels with NoData values will be shown.

         * FOOTPRINTS_MAY_CONTAIN_NODATA-Pixels with NoData values will be
         shown. This is the default.

         * FOOTPRINTS_DO_NOT_CONTAIN_NODATA-Pixels with NoData values will not
         be shown. You may notice an improvement in performance; however, if
         the imagery does include NoData values, they will appear as holes in
         the mosaic dataset.
     clip_to_boundary {Boolean}:
         Specifies whether the mosaicked image will be clipped to the boundary.
         Often the mosaic dataset and its boundary have the same extent. If
         they differ, the mosaic dataset can be clipped to the boundary.

         * CLIP-The mosaicked image will be clipped to the boundary. This is
         the default.

         * NOT_CLIP-The mosaicked image will not be clipped to the boundary.
     color_correction {Boolean}:
         Specifies whether color correction will be used on the mosaic dataset.

         * NOT_APPLY-Color correction will not be used. This is the default.

         * APPLY-The color correction that has been set up for the mosaic
         dataset will be used.
     allowed_mensuration_capabilities {String}:
         Specifies the measurements that will be performed on the mosaic
         dataset. The ability to perform vertical measurements is dependent on
         the imagery and may require a DEM.

         * None-No mensuration capabilities will be performed.

         * Basic-Ground measurements such as distance, point, centroid, and
         area calculations will be performed.

         * Base-Top Height-Measurements from the base to the top of features
         will be performed. Rational polynomial coefficients must be imbedded
         in the imagery.

         * Base-Top Shadow Height-Measurements from the base of a feature to
         the top of its shadow will be performed. Sun azimuth and sun elevation
         information is required.

         * Top-Top Shadow Height-Measurements from the top of a feature to the
         top of its shadow will be performed. Sun azimuth, sun elevation, and
         rational polynomial coefficients are required.

         * 3D-Measurements in 3D will be performed if a DEM is available.
     default_mensuration_capabilities {String}:
         Specifies the default mensuration capability for the mosaic dataset.
         The default mensuration value must be set in the list of values used
         for the allowed_mensuration_capabilities parameter or be set in the
         mosaic dataset's Mensuration Capabilities property.

         * None-No mensuration capabilities will be performed.

         * Basic-Ground measurements such as distance, point, centroid, and
         area calculations will be performed.

         * Base-Top Height-Measurements from the base to the top of features
         will be performed. Rational polynomial coefficients must be imbedded
         in the imagery.

         * Base-Top Shadow Height-Measurements from the base of a feature to
         the top of its shadow will be performed. Sun azimuth and sun elevation
         information is required.

         * Top-Top Shadow Height-Measurements from the top of a feature to the
         top of its shadow will be performed. Sun azimuth, sun elevation, and
         rational polynomial coefficients are required.

         * 3D-Measurements in 3D will be performed if a DEM is available.
     allowed_mosaic_methods {String}:
         Specifies the rules for displaying overlapping imagery.

         * None-Rasters will be ordered based on the ObjectID field in the
         mosaic dataset attribute table.

         * Center-Imagery that is closest to the center of the screen will be
         displayed.

         * NorthWest-Imagery that is closest to the northwest corner of the
         mosaic dataset boundary will be displayed.

         * LockRaster-Selected raster datasets will be displayed.

         * ByAttribute-Imagery will be displayed and prioritized based on a
         field in the attribute table.

         * Nadir-Rasters with viewing angles closest to zero will be displayed.

         * Viewpoint-Imagery that is closest to a selected viewing angle will
         be displayed.

         * Seamline-Transitions between images will be smoothed using
         seamlines.
     default_mosaic_method {String}:
         Specifies the default mosaic method that will be used for the mosaic
         dataset. The default mosaic method must be set in the list of values
         used for the allowed_mosaic_methods parameter or be set in the mosaic
         dataset's Allowed Mosaic Methods property.

         * None-Rasters will be ordered based on the ObjectID field in the
         mosaic dataset attribute table.

         * Center-Imagery that is closest to the center of the screen will be
         displayed.

         * NorthWest-Imagery that is closest to the northwest corner of the
         mosaic dataset boundary will be displayed.

         * LockRaster-Selected raster datasets will be displayed.

         * ByAttribute-Imagery will be displayed and prioritized based on a
         field in the attribute table.

         * Nadir-Rasters with viewing angles closest to zero will be displayed.

         * Viewpoint-Imagery that is closest to a selected viewing angle will
         be displayed.

         * Seamline-Transitions between images will be smoothed using
         seamlines.
     order_field {String}:
         The field that will be used when ordering rasters using the
         ByAttribute value of the default_mosaic_method parameter. The list of
         fields is defined as those in the attribute table that are of type
         metadata and are integer. This list can include, but is not limited
         to, the following:

         * MinPS

         * MaxPS

         * LowPS

         * HighPS

         * CenterX

         * CenterY

         * ZOrder

         * Shape_Length

         * Shape_Area
         If the field is a numeric or date field, the order_base parameter must
         be set.This parameter is not needed if the ByAttribute value is not in
         the
         allowed_mosaic_methods list.
     order_base {String}:
         Sorts the rasters based on their difference from this value in the
         field selected in the order_field parameter        If a Date attribute
         is used, it must be in one of the
         following formats:

         * YYYY/MM/DD HH:mm:ss.s

         * YYYY/MM/DD HH:mm:ss

         * YYYY/MM/DD HH:mm

         * YYYY/MM/DD HH

         * YYYY/MM/DD

         * YYYY/MM

         * YYYY
         This parameter is required only if the ByAttribute value is specified
         for the allowed_mosaic_methods parameter.
     sorting_order {Boolean}:
         Specifies whether the rasters will be sorted in an ascending or a
         descending order.

         * ASCENDING-Rasters will be sorted in an ascending order. This is the
         default.

         * DESCENDING-Rasters will be sorted in a descending order.
         This parameter is required only if the ByAttribute value is specified
         for the allowed_mosaic_methods parameter.
     mosaic_operator {String}:
         Specifies the rule that will be used for resolving overlapping pixels.

         * FIRST-The first image in the attribute table will be displayed.

         * LAST-The last image in the attribute table will be displayed.

         * MIN-The lowest pixel values will be displayed.

         * MAX-The highest pixel values will be displayed.

         * MEAN-The arithmetic mean will be used to average overlapping pixels.

         * BLEND-A distance weighted algorithm will be used to average
         overlapping pixels.

         * SUM-All of the overlapping pixel values will be added together.
     blend_width {Long}:
         The number of pixels to which the BLEND value of the mosaic_operator
         parameter will be applied.
     view_point_x {Double}:
         A numeric value that will be used to horizontally shift the center of
         the image. The units are the same as the spatial reference system.This
         parameter is only applicable if the allowed_mosaic_methods
         parameter is set to Viewpoint.
     view_point_y {Double}:
         A numeric value that will be used to vertically shift the center of
         the image. The units are the same as the spatial reference system.This
         parameter is only applicable if the allowed_mosaic_methods
         parameter is set to Viewpoint.
     max_num_per_mosaic {Long}:
         The maximum number of raster datasets that will be displayed at a
         given time in a mosaic dataset.
     cell_size_tolerance {Double}:
         The maximum pixel size difference that is allowed before images are
         considered to have a different cell pixel.This allows images of
         similar spatial resolutions to be considered as
         having the same nominal resolution. For example, with a factor of 0.1,
         all images with cell sizes within 10 percent of each other will be
         grouped for tools and operations that use cell sizes.
     cell_size {Cell Size XY}:
         The cell size of the mosaic dataset using an existing raster dataset
         or its specified width (x) and height (y). If you specify the cell
         size, you can use a single value for a square cell size, or x and y
         values for a rectangular cell size.
     metadata_level {String}:
         Specifies the level of metadata that will be exposed from the server
         to a client when publishing the mosaic dataset.

         * FULL-Metadata regarding the processing applied at the mosaic dataset
         level as well as metadata related to the individual raster datasets
         will be exposed.

         * NONE-No metadata will be exposed to the client.

         * BASIC-Metadata related to individual raster datasets, such as the
         number of columns and rows, cell size, and spatial reference
         information, will be exposed.
     transmission_fields {String}:
         The fields in the attribute table that clients can view. By
         default, the list includes the following:

         * Name

         * MinPS

         * MaxPS

         * LowPS

         * HighPS

         * Tag

         * GroupName

         * ProductName

         * CenterX

         * CenterY

         * ZOrder

         * Shape_Length

         * Shape_Area
     use_time {Boolean}:
         Specifies whether the mosaic dataset will be time aware. If time is
         activated, the start and end fields and the time format must be
         specified.

         * DISABLED-The mosaic dataset will not be time aware. This is the
         default.

         * ENABLED-The mosaic dataset will be time aware. This allows the
         client to use the Time Slider.
     start_time_field {String}:
         The field in the attribute table that shows the start time.
     end_time_field {String}:
         The field in the attribute table that shows the end time.
     time_format {String}:
         Specifies the time format that will be used for the mosaic dataset for
         parameters such as start_time_field and end_time_field.

         * YYYY-The time format will be year.

         * YYYYMM-The time format will be year and month.

         * YYYY/MM-The time format will be year and month.

         * YYYY-MM-The time format will be year and month.

         * YYYYMMDD-The time format will be year, month, and day.

         * YYYY/MM/DD-The time format will be year, month, and day.

         * YYYY-MM-DD-The time format will be year, month, and day.

         * YYYYMMDDhhmmss-The time format will be year, month, day, hour,
         minute, and seconds.

         * YYYY/MM/DD hh:mm:ss-The time format will be year, month, day, hour,
         minute, and seconds.

         * YYYY-MM-DD hh:mm:ss-The time format will be year, month, day, hour,
         minute, and seconds.

         * YYYYMMDDhhmmss.s-The time format will be year, month, day, hour,
         minute, seconds, and fraction of seconds.

         * YYYY/MM/DD hh:mm:ss.s-The time format will be year, month, day,
         hour, minute, seconds, and fraction of seconds.

         * YYYY-MM-DD hh:mm:ss.s-The time format will be year, month, day,
         hour, minute, seconds, and fraction of seconds.
     geographic_transform {String}:
         The geographic transformations that will be associated with the mosaic
         dataset.
     max_num_of_download_items {Long}:
         The maximum number of raster datasets that will be downloaded per
         request.
     max_num_of_records_returned {Long}:
         The maximum number of records that will be downloaded per request.
     data_source_type {String}:
         Specifies the type of imagery in the mosaic dataset.

         * GENERIC-The mosaic dataset contains no specified data type.

         * THEMATIC-The mosaic dataset contains thematic data with discrete
         values, such as land cover.

         * PROCESSED-The mosaic dataset has been color adjusted.

         * ELEVATION-The mosaic dataset contains elevation data.

         * SCIENTIFIC-The mosaic dataset contains scientific data.

         * VECTOR_UV-The mosaic dataset has two variables.

         * VECTOR_MAGDIR-The mosaic dataset has magnitude and direction.
     minimum_pixel_contribution {Long}:
         The minimum number of pixels required for a mosaic dataset item to be
         considered significant enough to be used in the mosaic dataset.
         Because of overlapping imagery, an item may display only a small
         sliver of its overall image. Skipping these mosaic dataset items will
         improve performance of the mosaic dataset.
     processing_templates {rft.xml|rft.json|rft|xml|json}:
         The function chains that will be used to process a mosaic dataset or
         the mosaic dataset items on the fly. You can add, remove, or reorder
         the function chains.All the template names that are added must be
         unique.For information about working with function chains, see Raster
         function template.
     default_processing_template {String}:
         The default function chain. The default function chain will be applied
         when the mosaic dataset is accessed.
     time_interval {Double}:
         The duration of each time step interval. The time step interval
         defines the granularity of the temporal data. The unit of time is
         specified in the time_interval_units parameter.
     time_interval_units {String}:
         Specifies the measurement unit that will be used for the time
         interval.

         * None-No time unit exists or it is unknown.

         * Milliseconds-The time unit will be milliseconds.

         * Seconds-The time unit will be seconds.

         * Minutes-The time unit will be minutes.

         * Hours-The time unit will be hours.

         * Days-The time unit will be days.

         * Weeks-The time unit will be weeks.

         * Months-The time unit will be months.

         * Years-The time unit will be years.

         * Decades-The time unit will be decades.

         * Centuries-The time unit will be centuries.
     product_definition {String}:
         Specifies a template that is either specific to the type of imagery
         you are working with or generic. The generic options include the
         standard supported raster sensor types as follows:

         * NONE-No band ordering is specified for the mosaic dataset. This is
         the default.

         * NATURAL_COLOR_RGB-A 3-band mosaic dataset, with red, green, and blue
         wavelength ranges will be created. This is designed for natural color
         imagery.

         * NATURAL_COLOR_RGBI-A 4-band mosaic dataset, with red, green, blue,
         and near infrared wavelength ranges will be created.

         * VECTOR_FIELD_UV-A mosaic dataset displaying two variables will be
         created.

         * VECTOR_FIELD_MAGNITUDE_DIRECTION-A mosaic dataset displaying
         magnitude and direction will be created.

         * FALSE_COLOR_IRG-A 3-band mosaic dataset, with near infrared, red,
         and green wavelength ranges will be created.

         * BLACKSKY-A 3-band mosaic dataset using the BlackSky wavelength
         ranges will be created

         * DMCII_3BANDS-A 3-band mosaic dataset using the DMCii wavelength
         ranges will be created.

         * DEIMOS2_4BANDS-A 4-band mosaic dataset using the Deimos-2 wavelength
         ranges will be created.

         * DUBAISAT-2_4BANDS-A 4-band mosaic dataset using the DubaiSat-2
         wavelength ranges will be created.

         * FORMOSAT-2_4BANDS-A 4-band mosaic dataset using the FORMOSAT-2
         wavelength ranges will be created.

         * GEOEYE-1_4BANDS-A 4-band mosaic dataset using the GeoEye-1
         wavelength ranges will be created.

         * GF-1 PMS_4BANDS-A 4-band mosaic dataset using the Gaofen-1
         Panchromatic Multispectral Sensor wavelength ranges will be created.

         * GF-1 WFV_4BANDS-A 4-band mosaic dataset using the Gaofen-1 Wide
         Field of View Sensor wavelength ranges will be created.

         * GF-2 PMS_4BANDS-A 4-band mosaic dataset using the Gaofen-2
         Panchromatic Multispectral Sensor wavelength ranges will be created.

         * GF-4 PMI_4BANDS-A 4-band mosaic dataset using the Gaofen-4
         panchromatic and multispectral wavelength ranges will be created.

         * HJ 1A/1B CCD_4BANDS-A 4-band mosaic dataset using the Huan Jing-1
         CCD Multispectral or Hyperspectral Sensor wavelength ranges will be
         created.

         * IKONOS_4BANDS-A 4-band mosaic dataset using the IKONOS wavelength
         ranges will be created.

         * JILIN-1_3BANDS-A 3-band mosaic dataset using the Jilin-1 wavelength
         ranges will be created.

         * KOMPSAT-2_4BANDS-A 4-band mosaic dataset using the KOMPSAT-2
         wavelength ranges will be created.

         * KOMPSAT-3_4BANDS-A 4-band mosaic dataset using the KOMPSAT-3
         wavelength ranges will be created.

         * LANDSAT_6BANDS-A 6-band mosaic dataset using the Landsat 5 and 7
         wavelength ranges from the TM and ETM+ sensors will be created.

         * LANDSAT_8BANDS-An 8-band mosaic dataset using the LANDSAT 8
         wavelength ranges will be created.

         * LANDSAT_9BANDS-An 8-band mosaic dataset using the LANDSAT 9
         wavelength ranges will be created.

         * LANDSAT_MSS_4BANDS-A 4-band mosaic dataset using the Landsat
         wavelength ranges from the MSS sensor will be created.

         * PLANETSCOPE-A 5-band mosaic dataset using the PlanetScope wavelength
         ranges will be created.

         * PLEIADES-1_4BANDS-A 4-band mosaic dataset using the PLEIADES-1
         wavelength ranges will be created.

         * PLEIADES_NEO_6BANDS-A 6-band mosaic dataset using the Pleiades Neo
         wavelength ranges will be created.

         * QUICKBIRD_4BANDS-A 4-band mosaic dataset using the QuickBird
         wavelength ranges will be created.

         * RAPIDEYE_5BANDS-A 5-band mosaic dataset using the RapidEye
         wavelength ranges will be created.

         * SENTINEL2_13BANDS-A 13-band mosaic dataset using the Sentinel 2 MSI
         wavelength ranges will be created.

         * SKYSAT_4BANDS-A 4-band mosaic dataset using the SkySat-C MSI
         wavelength ranges will be created.

         * SPOT-5_4BANDS-A 4-band mosaic dataset using the SPOT-5 wavelength
         ranges will be created.

         * SPOT-6_4BANDS-A 4-band mosaic dataset using the SPOT-6 wavelength
         ranges will be created.

         * SPOT-7_4BANDS-A 4-band mosaic dataset using the SPOT-7 wavelength
         ranges will be created.

         * SUPERVIEW-1_4BANDS-A 4-band mosaic dataset using the SuperView-1
         wavelength ranges will be created.

         * TH-01_4BANDS-A 4-band mosaic dataset using the Tian Hui-1 wavelength
         ranges will be created.

         * WORLDVIEW-2_8BANDS-An 8-band mosaic dataset using the WorldView-2
         wavelength ranges will be created.

         * WORLDVIEW-3_8BANDS-An 8-band mosaic dataset using the WorldView-3
         wavelength ranges will be created.

         * WORLDVIEW-4_4BANDS-A 4-band mosaic dataset using the WorldView-4
         wavelength ranges will be created.

         * ZY1-02C PMS_3BANDS-A 3-band mosaic dataset using the ZiYuan-1
         panchromatic/multispectral wavelength ranges will be created.

         * ZY3-CRESDA_4BANDS-A 4-band mosaic dataset using the ZiYuan-3 CRESDA
         wavelength ranges will be created.

         * ZY3-SASMAC_4BANDS-A 4-band mosaic dataset using the ZiYuan-3 SASMAC
         wavelength ranges will be created.

         * CUSTOM-The number of bands and the average wavelength for each band
         are defined using the Product Band Definitions parameter
         (product_band_definitions in Python).
     product_band_definitions {Value Table}:
         The wavelength ranges, number of bands, and band order definitions.
         You can edit the product_definition values and add new bands using the
         CUSTOM product definition."""
    ...

@gptooldoc("SplitMosaicDatasetItems_management", None)
def SplitMosaicDatasetItems(
    in_mosaic_dataset=..., where_clause=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SplitMosaicDatasetItems_management(in_mosaic_dataset, {where_clause})

       Splits mosaic dataset items that were merged together using Merge
       Mosaic Dataset Items.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset containing the items to split.
     where_clause {SQL Expression}:
         An SQL expression to select items to split.If the query does not
         contain any previously merged items, the tool
         will return an error."""
    ...

@gptooldoc("SynchronizeMosaicDataset_management", None)
def SynchronizeMosaicDataset(
    in_mosaic_dataset=...,
    where_clause=...,
    new_items=...,
    sync_only_stale=...,
    update_cellsize_ranges=...,
    update_boundary=...,
    update_overviews=...,
    build_pyramids=...,
    calculate_statistics=...,
    build_thumbnails=...,
    build_item_cache=...,
    rebuild_raster=...,
    update_fields=...,
    fields_to_update=...,
    existing_items=...,
    broken_items=...,
    skip_existing_items=...,
    refresh_aggregate_info=...,
    estimate_statistics=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SynchronizeMosaicDataset_management(in_mosaic_dataset, {where_clause}, {new_items}, {sync_only_stale}, {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {build_item_cache}, {rebuild_raster}, {update_fields}, {fields_to_update;fields_to_update...}, {existing_items}, {broken_items}, {skip_existing_items}, {refresh_aggregate_info}, {estimate_statistics})

       Synchronizes a mosaic dataset to keep it up to date. In addition to
       syncing data, you can update overviews if the underlying imagery has
       been changed, generate new overviews and cache, and restore the
       original configuration of mosaic dataset items. You can also remove
       paths to source data with this tool. To repair paths, use the Repair
       Mosaic Dataset Paths tool.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that will be synchronized.
     where_clause {SQL Expression}:
         An SQL expression to select which mosaic dataset items will be
         synchronized. If an expression is not provided, all dataset items will
         be updated.
     new_items {Boolean}:
         Specifies whether new items will be included when synchronizing as
         well as the options to use to update the new items.If you use this
         option, the item's workspace will be searched for new
         data. When data is added to the mosaic dataset, it will use the same
         raster type as the other items in the same workspace.

         * NO_NEW_ITEMS-No new items will be added when synchronizing. This is
         the default.

         * UPDATE_WITH_NEW_ITEMS-The mosaic dataset will be updated with new
         items in the workspaces. Optionally, the existing items can be
         modified by setting the skip_existing_items parameter to
         OVERWRITE_EXISTING_ITEMS.
     sync_only_stale {Boolean}:
         Specifies whether mosaic dataset items will be updated only when the
         underlying raster datasets have been modified due to synchronizing.
         For example, building pyramids or updating the georeferencing of
         rasters will affect how the overviews are rendered.

         * SYNC_STALE-Only the items of the underlying raster datasets that
         have been modified will be updated. This is the default.

         * SYNC_ALL-All of the items in the mosaic dataset will be updated.
     update_cellsize_ranges {Boolean}:
         Specifies whether cell size ranges for the mosaic dataset will be
         recalculated.

         * UPDATE_CELL_SIZES-The cell size ranges for the entire mosaic dataset
         will be recalculated, but only for items that have an invalid
         visibility. This is the default.

         * NO_CELL_SIZES-No cell size ranges will be recalculated.
     update_boundary {Boolean}:
         Specifies whether the boundary that shows the full extent of the
         mosaic dataset will be rebuilt. Choose UPDATE_BOUNDARY if syncing will
         change the extent of the mosaic dataset.

         * UPDATE_BOUNDARY-The boundary will be rebuilt after the mosaic
         dataset is synchronized. This is the default.

         * NO_BOUNDARY-The boundary will not be rebuilt.
     update_overviews {Boolean}:
         Specifies whether obsolete overviews will be updated. The overview
         becomes obsolete if any underlying rasters have been modified due to
         synchronizing.

         * NO_OVERVIEWS-The overviews will not be updated. This is the
         default.

         * UPDATE_OVERVIEWS-The affected overviews will be updated after the
         mosaic dataset is synchronized.
     build_pyramids {Boolean}:
         Specifies whether pyramids will be built for the specified mosaic
         dataset items. Pyramids can be built for each raster item in the
         mosaic dataset. Pyramids can improve the speed at which each raster is
         displayed.

         * NO_PYRAMIDS-Pyramids will not be built. This is the default.

         * BUILD_PYRAMIDS-Pyramids will be built for all the mosaic raster
         items that were updated due to synchronization.
         Pyramids will not be built for items that were added due to
         synchronization.
     calculate_statistics {Boolean}:
         Specifies whether statistics will be calculated for the specified
         mosaic dataset items. Statistics are required for a mosaic dataset
         when performing certain tasks, such as applying a contrast stretch.

         * NO_STATISTICS-Statistics will not be calculated. This is the
         default.

         * CALCULATE_STATISTICS-Statistics will be calculated for the mosaic
         dataset items that were updated due to synchronization.
         Statistics will not be calculated for items that were added due to
         synchronization.
     build_thumbnails {Boolean}:
         Specifies whether thumbnails will be built for the specified mosaic
         dataset items. Thumbnails are small, highly resampled images that can
         be created for each raster item in the mosaic definition. Thumbnails
         can be accessed when the mosaic dataset is accessed as an image
         service and will display as part of the item description.

         * NO_THUMBNAILS-Thumbnails will not be built or updated. This is the
         default.

         * BUILD_THUMBNAILS-Thumbnails will be built or updated for all the
         raster items that were updated due to synchronization.
         Thumbnails will not be built for items that were added due to
         synchronization.
     build_item_cache {Boolean}:
         Choose whether to build a cache for the specified mosaic dataset
         items. A cache can be built when you've added data using the LAS,
         Terrain, or LAS dataset raster types.

         * NO_ITEM_CACHE-A cache will not be built or updated. This is the
         default.

         * BUILD_ITEM_CACHE-A cache will be built or updated for all the raster
         items that were updated due to synchronization.
         A cache will not be built for items that were added due to
         synchronization.
     rebuild_raster {Boolean}:
         Specifies whether the raster items will be rebuilt from the data
         source using the original raster type.

         * REBUILD_RASTER-The rasters will be rebuilt from the source data. Any
         changes that you have performed on selected items in the mosaic
         dataset will be lost. This is the default.

         * NO_RASTER-The rasters will not be rebuilt. Other primary fields will
         be reset if the update_fields parameter is set to UPDATE_FIELDS.
         This only affects items that will be synchronized. This parameter is
         not applicable if the new_items parameter is set to
         UPDATE_WITH_NEW_ITEMS.
     update_fields {Boolean}:
         Specifies whether the fields in the table will be updated. This only
         affects items that will be synchronized.

         * UPDATE_FIELDS-The fields will be updated from the source files. This
         is the default.

         * NO_FIELDS-The fields in the table will not be updated from the
         source.
         If you update the fields, you can control which fields are updated
         using the fields_to_update parameter. If you made edits to some of the
         fields, you can remove them using the fields_to_update parameter.
     fields_to_update {String}:
         The fields that will be updated.This parameter is only valid if the
         update_fields parameter is set to
         UPDATE_FIELDS.If you made edits to some of the fields, make sure they
         are not
         listed.The RASTER field can be refreshed, even if REBUILD_RASTER is
         not
         specified. However, if REBUILD_RASTER is specified, the RASTER field
         will be rebuilt, even if the fields_to_update parameter value is not
         specified.
     existing_items {Boolean}:
         Specifies whether existing items in the mosaic dataset will be
         updated.If you use this parameter, choose which existing parameters to
         update:
         sync_only_stale, build_pyramids, calculate_statistics,
         build_thumbnails, build_item_cache, update_fields, or
         fields_to_update.

         * UPDATE_EXISTING_ITEMS-The existing items will be updated with the
         parameters you chose to update. This is the default.

         * IGNORE_EXISTING_ITEMS-The existing items will not be updated.
     broken_items {Boolean}:
         Specifies whether items with broken links will be removed.Ensure that
         all network connections are working properly. This tool
         will remove any items that cannot be accessed.

         * IGNORE_BROKEN_ITEMS-Items with broken links will not be removed from
         the mosaic dataset. This is the default.

         * REMOVE_BROKEN_ITEMS-Items with broken links will be removed from the
         mosaic dataset.
     skip_existing_items {Boolean}:
         Specifies whether existing mosaic dataset items will be skipped or
         updated with the modified files from disk. To use this parameter, the
         new_items parameter must be set to UPDATE_WITH_NEW_ITEMS.

         * SKIP_EXISTING_ITEMS-While adding new mosaic dataset items, existing
         mosaic dataset items will be skipped; they will not be updated. This
         is the default.

         * OVERWRITE_EXISTING_ITEMS-While adding new mosaic dataset items,
         existing mosaic dataset items that correspond to modified files on
         disk will be updated.
     refresh_aggregate_info {Boolean}:
         Specifies whether data that may have been removed from the mosaic
         dataset will be included. To use this parameter, the existing_items
         parameter must be set to IGNORE_EXISTING_ITEMS.

         * NO_REFRESH_INFO-When synchronizing, rasters that may have been
         removed from the mosaic dataset will be excluded. This is the default.

         * REFRESH_INFO-When synchronizing, rasters that may have been removed
         from the mosaic dataset will be included.
     estimate_statistics {Boolean}:
         Specifies whether statistics on the mosaic dataset will be estimated.

         * NO_STATISTICS-When synchronizing, statistics on the mosaic dataset
         will not be estimated. This is the default.

         * ESTIMATE_STATISTICS-When synchronizing, statistics on the mosaic
         dataset will be estimated."""
    ...

@gptooldoc("AnalyzeControlPoints_management", None)
def AnalyzeControlPoints(
    in_mosaic_dataset=...,
    in_control_points=...,
    out_coverage_table=...,
    out_overlap_table=...,
    in_mask_dataset=...,
    minimum_area=...,
    maximum_level=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AnalyzeControlPoints_management(in_mosaic_dataset, in_control_points, out_coverage_table, {out_overlap_table}, {in_mask_dataset}, {minimum_area}, {maximum_level})

       Analyzes the control point coverage and identifies the areas that need
       additional control points to improve the block adjust result.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset against which to analyze the control points.
     in_control_points (Feature Layer):
         The input control point feature class.It is normally created from the
         Compute Tie Points or the Compute
         Control Points tool.
     in_mask_dataset {Feature Layer}:
         A polygon feature class used to exclude areas that you do not want in
         the analysis of the control points computation.A field with a name of
         mask can control the inclusion or exclusion of
         areas. A value of 1 indicates that the areas defined by the polygons
         (inside) will be excluded from the computation. A value of 2 indicates
         the defined polygons (inside) will be included in the computation
         while areas outside of the polygons will be excluded.
     minimum_area {Double}:
         Specify the minimum percent that the overlap area must be, in relation
         to the image. Areas that are lower than the specified percent
         threshold will be excluded from the analysis.Ensure that you do not
         have areas that are too small; otherwise, you
         will have small slivers being analyzed.
     maximum_level {Long}:
         The maximum number of images that can be overlapped when analyzing the
         control points.For example, if there are four images in your mosaic
         dataset, and a
         maximum overlap value of 3 was specified, then there are ten different
         combinations that will appear in the Overlap Window. If the four
         images were named i1, i2, i3, and i4, the ten combinations that would
         appear are [i1, i2, i3], [i1 i2 i4], [i1 i3 i4], [i2 i3 i4], [i1, i2],
         [i1, i3], [i1, i4], [i2, i3], [i2, i4], and [i3, i4].

    OUTPUTS:
     out_coverage_table (Feature Class):
         A polygon feature class output that contains the control point
         coverage and the percentage of the area within the corresponding
         image.
     out_overlap_table {Feature Class}:
         A polygon feature class output that contains all the overlap areas
         between images."""
    ...

@gptooldoc("AppendControlPoints_management", None)
def AppendControlPoints(
    in_master_control_points=...,
    in_input_control_points=...,
    in_z_field=...,
    in_tag_field=...,
    in_dem=...,
    in_xy_accuracy=...,
    in_z_accuracy=...,
    Geoid=...,
    area_of_interest=...,
    append_option=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AppendControlPoints_management(in_master_control_points, in_input_control_points, {in_z_field}, {in_tag_field}, {in_dem}, {in_xy_accuracy}, {in_z_accuracy}, {Geoid}, {area_of_interest}, {append_option})

       Combines control points to an existing control point table.

    INPUTS:
     in_master_control_points (Feature Class / Feature Layer):
         The input control point table. This is usually the output from the
         Compute Tie Points tool.
     in_input_control_points (Feature Class / Feature Layer / File / String):
         A point feature class that stores control points. It could be the
         control point table created from the Compute Control Points tool, the
         Compute Tie Points tool, or a point feature class that has ground
         control points.
     in_z_field {Field}:
         The field that stores the control point z-values.If both the Z Value
         Field Name and the Input DEM parameters are set,
         the Z value field is used. If neither the Z Value Field Name nor the
         Input DEM parameter is set, the z-value is set to 0 for all ground
         control points and check points.
     in_tag_field {Field}:
         A field in the input control point table that has a unique value. This
         field will be added to the target control point table, where the tag
         field can be used to bring in identifiers associated with ground
         control points.
     in_dem {Raster Layer / Mosaic Layer / Raster Dataset / Mosaic Dataset}:
         A DEM to use to obtain the z-value for the control points in the input
         control point table.If both the Z Value Field Name and Input DEM
         parameters are set, the Z
         value field is used. If neither the Z Value Field Name nor the Input
         DEM parameter is set, the z-value is set to 0 for all ground control
         points and check points.
     in_xy_accuracy {Double}:
         The input accuracy for the X and Y coordinates. The accuracy is in the
         same units as the in_input_control_points.This information should be
         provided by the data provider. If the
         accuracy information is not available, skip this optional parameter.
     in_z_accuracy {Double}:
         The input accuracy for the vertical coordinates. The accuracy is in
         the units of the in_input_control_points.This information should be
         provided by the data provider. If the
         accuracy information is not available, skip this optional parameter.
     Geoid {Boolean}:
         The geoid correction is required by rational polynomial coefficients
         (RPC) that reference ellipsoidal heights. Most elevation datasets are
         referenced to sea level orthometric heights, so this correction would
         be required in these cases to convert to ellipsoidal heights.

         * NONE-No geoid correction is made. Use NONE only if your DEM is
         already expressed in ellipsoidal heights. This is the default.

         * GEOID-A geoid correction will be made to convert orthometric heights
         to ellipsoidal heights (based on EGM96 geoid).
     area_of_interest {Envelope / Feature Layer / Feature Class}:
         Defines an area of interest extent by entering minimum and maximum x-
         and y-coordinates in the spatial reference of the input control point
         table.
     append_option {String}:
         Specifies how control points will be appended to the control point
         table.

         * ALL-Add all points in the input control point table to the target
         control point table, including GCPs, check points, and all tie points.
         This is the default.

         * GCP-Add only GCPs in the input point table to the target control
         point table.

         * GCPSET-Add GCPs and tie points specifically associated with the GCPs
         to the target control point table.Use caution with this option-it is
         applicable only when the tie points in the input and target control
         point table have the same transformation. The tie points might not be
         in the desired positions if they were computed using a different
         adjustment process."""
    ...

@gptooldoc("ApplyBlockAdjustment_management", None)
def ApplyBlockAdjustment(
    in_mosaic_dataset=...,
    adjustment_operation=...,
    input_solution_table=...,
    pan_to_ms_scaling_factor=...,
    DEM=...,
    zoffset=...,
    control_point_table=...,
    adjust_footprints=...,
    solution_point_table=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ApplyBlockAdjustment_management(in_mosaic_dataset, adjustment_operation, {input_solution_table}, {pan_to_ms_scaling_factor}, {DEM}, {zoffset}, {control_point_table}, {adjust_footprints}, {solution_point_table})

       Applies the geographic adjustments to the mosaic dataset items. This
       tool uses the solution table from the Compute Block Adjustments tool.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset to adjust.
     adjustment_operation (String):
         Choose whether you want to adjust the mosaic dataset using the
         solution table or if you want to reset the mosaic dataset so there are
         no adjustments applied.

         * ADJUST-Adjust the mosaic dataset using the input solution table.

         * RESET-Reset the mosaic dataset so there are no adjustments applied
         to it.

         * REACTIVATE-Images dropped from the adjustment will be restored to
         active status. Images without the minimum number of control points
         required for adjustment are dropped from the computation in the
         standard adjustment operation, such that the images are categorized as
         Inactive in the footprints table, the maxPS value is set to 0, the
         imagery is not visible in the map, and the tie points statuses for the
         dropped images are disabled. This option will restore the Category
         status to Primary and ensure the maxPS value is resumed. Images that
         were included in the adjustment process are unaffected by this option.
     input_solution_table {Table View}:
         Specify a solution table to use when adjusting your mosaic dataset.
         This is the output from the Compute Block Adjustments tool.
     pan_to_ms_scaling_factor {Double}:
         If your mosaic dataset contains pan-sharpened rasters, specify the
         scaling factor between the pan-sharpened resolution and the
         multispectral resolution.
     DEM {Raster Dataset / Raster Layer / Mosaic Dataset / Mosaic Layer}:
         A DEM to use within the application of the block adjustment. This DEM
         will only be used if it is a higher resolution than any DEM that may
         already exist within the mosaic dataset.If this input DEM is used, the
         geometric function of the mosaic
         dataset will be updated using this input.
     zoffset {Double}:
         The vertical offset used to adjust the elevation layer within the
         mosaic dataset's Geometric function.
     control_point_table {Table View}:
         The input control point table will have the same adjustments applied
         as the solution table adjustments.
     adjust_footprints {Boolean}:
         Choose whether to update the footprint geometry using the same
         transformation that was applied to the image.

         * NO_ADJUST_FOOTPRINTS-Do not update the footprint geometry This is
         the default.

         * ADJUST_FOOTPRINTS-Update the footprint geometry to the image
         geometry. The control point table will also be transformed, if one is
         provided.
     solution_point_table {Table View}:
         Specify a solution points table to use to update the status field for
         the control point table. This parameter is used only when the
         control_point_table parameter is set."""
    ...

@gptooldoc("BuildStereoModel_management", None)
def BuildStereoModel(
    in_mosaic_dataset=...,
    minimum_angle=...,
    maximum_angle=...,
    minimum_overlap=...,
    maximum_diff_OP=...,
    maximum_diff_GSD=...,
    group_by=...,
    same_flight=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildStereoModel_management(in_mosaic_dataset, {minimum_angle}, {maximum_angle}, {minimum_overlap}, {maximum_diff_OP}, {maximum_diff_GSD}, {group_by}, {same_flight})

       Builds a stereo model of a mosaic dataset based on a user-provided
       stereo pair.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The mosaic dataset on which the stereo model will be built.Running the
         Apply Block Adjustment tool on the input mosaic dataset
         first will help create a more accurate stereo model.
     minimum_angle {Double}:
         The value, in degrees, that defines the minimum angle the stereo pair
         must meet. The default is 10.
     maximum_angle {Double}:
         The value, in degrees, that defines the maximum angle the stereo pair
         must meet. The default is 90.
     minimum_overlap {Double}:
         The percentage of the overlapping area over the whole image. The
         default is 0.5.
     maximum_diff_OP {Double}:
         The maximum threshold for the Omega and Phi difference between the two
         image pairs. The Omega values and Phi values for the image pairs are
         compared. If the difference between either the two Omega or the two
         Phi values is above the threshold, the pairs will not be formatted as
         a stereo pair.
     maximum_diff_GSD {Double}:
         The threshold for the maximum GSD between two images in a pair. If the
         resolution ratio between the two images is greater than the threshold
         value, the pairs will not be built as a stereo pair. The default is 2.
     group_by {Field}:
         Builds the stereo model from raster items within the same group,
         defined by a mosaic dataset field such as RGB, Panchromatic, or
         Infrared.
     same_flight {Boolean}:
         Specifies how the stereo models will be selected.

         * SAMEFLIGHT-Stereo pairs will be selected along the same flight line.

         * NO_SAMEFLIGHT-Stereo pairs will be selected across flight lines.
         This parameter is not applicable to satellite-based sensors."""
    ...

@gptooldoc("ComputeBlockAdjustment_management", None)
def ComputeBlockAdjustment(
    in_mosaic_dataset=...,
    in_control_points=...,
    transformation_type=...,
    out_solution_table=...,
    out_solution_point_table=...,
    maximum_residual_value=...,
    adjustment_options=...,
    location_accuracy=...,
    out_quality_table=...,
    DEM=...,
    elevation_accuracy=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeBlockAdjustment_management(in_mosaic_dataset, in_control_points, transformation_type, out_solution_table, {out_solution_point_table}, {maximum_residual_value}, {adjustment_options;adjustment_options...}, {location_accuracy}, {out_quality_table}, {DEM}, {elevation_accuracy})

       Computes the adjustments to the mosaic dataset. This tool will create
       a solution table that can be used to apply the actual adjustments.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset that will be adjusted.
     in_control_points (Feature Layer):
         The control point table that includes tie points and ground control
         points.This feature class is usually the output from the Compute Tie
         Points
         tool.
     transformation_type (String):
         Specifies the type of transformation that will be used when adjusting
         the mosaic dataset.

         * POLYORDER0-A zero-order polynomial will be used in the block
         adjustment computation. This is commonly used when the data is in flat
         area.

         * POLYORDER1-A first-order polynomial (affine) will be used in the
         block adjustment computation. This is the default.

         * RPC-The rational polynomial coefficients (RPCs) will be used for the
         transformation. This is used for satellite imagery that contains RPC
         information in the metadata. This option requires the ArcGIS Desktop
         Advanced license.

         * Frame-The Frame camera model will be used for the transformation.
         This is used for aerial imagery that contains the frame camera
         information in the metadata. This option requires the ArcGIS Desktop
         Advanced license.
     maximum_residual_value {Double}:
         A threshold that is used in block adjustment computation; points with
         residuals exceeding the threshold will not be used. This parameter
         applies when the transformation type is POLYORDER0, POLYORDER1, or
         Frame. If the transformation type is RPC, the proper threshold for
         eliminating invalid points will be automatically determined.When the
         transformation type is POLYORDER0 or POLYORDER1, the units
         for this parameter will be map units, and the default value will be
         2.When the transformation type is Frame, the units for this parameter
         will be pixels, and the default value will be 5.
     adjustment_options {Value Table}:
         Additional options that will be used to fine-tune the adjustment
         computation.

         * MinResidual-The minimum residual value, which is the lower threshold
         value, will be used. When the transformation type is POLYORDER0 or
         POLYORDER1, the units will be map units and the default minimum
         residual value will be 0. The minimum residual value and the
         maximum_residual_value parameter value are used in detecting and
         removing points that generate large errors from the block adjustment
         computation.

         * MaxResidualFactor-The maximum residual factor will be used to
         generate the maximum (upper threshold) residual value if the
         maximum_residual-value parameter is not defined. In this case,
         MaxResidualFactor * RMS will be used to calculate the upper threshold
         value.The minimum residual value and the maximum_residual_factor
         parameter value are used in detecting and removing points that
         generate large errors from block adjustment computation.
         Additional options for the adjustment engine are listed below when you
         select Frame for the Transformation Type. The specifications of many
         of the options are supplied by the data provider. The options
         include the following:

         * CalibrateF-Calibrate the sensor's focal length for use in the block
         adjustment. Assign a value of 1 for focal length calibration, or 0 for
         no calibration. The default is 0.

         * CalibratePP-Calibrate the principle point in the block adjustment.
         Assign a value of 1 for calibration or 0 for no calibration. The
         default is 0.

         * CalibrateP-Calibrate for radial distortion parameters in the block
         adjustment. Assign a value of 1 for calibration or 0 for no
         calibration. The default is 0.

         * CalibrateK-Calibrate for tangential distortion parameters in the
         block adjustment. Assign a value of 1 for calibration or 0 for no
         calibration. The default is 0.
         Calibration parameters, such as perspective data, are usually provided
         for most professional digital aerial cameras, such as UltraCam or DMC.
         The calibration options can be 0 if camera calibration parameters are
         prepared in the camera table.

         * APrioriAccuracyX-Include the accuracy of the x-coordinate provided
         by the airborne Position Orientation System. The units must match
         PerspectiveX. If the value is set to 0, the x-coordinate of the image
         location is not adjusted in adjustment. This is not recommended for
         most UAV data.

         * APrioriAccuracyY-Include the accuracy of the y-coordinate provided
         by the airborne Position Orientation System. The units must match
         PerspectiveY. If the value is set to 0, the y-coordinate of the image
         location is not adjusted in adjustment. This is not recommended for
         most UAV data.

         * APrioriAccuracyZ-Include the accuracy of the z-coordinate provided
         by the airborne Position Orientation System. The units must match
         PerspectiveZ. If the value is set to 0, the z-coordinate of the image
         location is not adjusted in adjustment. This is not recommended for
         most UAV data.

         * APrioriAccuracyXY-Include the accuracy of the planar coordinate
         provided by the metadata. The units must match PerspectiveX. If the
         value is set to 0, planar coordinates (x and y) of the image location
         are not adjusted in adjustment. This is not recommended for most UAV
         data.

         * APrioriAccuracyXYZ-Include the accuracy of image location provided
         by the metadata. The units must match PerspectiveX. If the value is
         set to 0, the image location is not adjusted in adjustment. This is
         not recommended for most UAV data.

         * APrioriAccuracyOmega-Include the accuracy of the Omega angle
         provided by the airborne Position Orientation System. The units are in
         decimal degrees.

         * APrioriAccuracyPhi-Include the accuracy of the Phi angle provided by
         the airborne Position Orientation System. The units are in decimal
         degrees.

         * APrioriAccuracyOmegaPhi-Include the accuracy of the Omega or Phi
         angle provided by the airborne Position Orientation System. The units
         are in decimal degrees.

         * APrioriAccuracyKappa-Include the accuracy of the Kappa angle
         provided by the airborne Position Orientation System. The units are in
         decimal degrees.

         * ComputeAntennaOffset-Compute the offset between GNSS antenna center
         and camera projection center in adjustment. Assign a value of 1 for
         calibration or 0 for no computation. The default is 0.

         * ComputeShift-Compute the GNSS signal shift in flights in bundle
         adjustment. 0 is not computing. Assign a value of 1 for calibration or
         0 for no computation. The default is 0.

         * ComputeImagePosteriorStd-Compute the posterior standard deviation of
         image location and orientation after adjustment. Assign a value of 1
         to compute or 0 for no computation. The default is 1.

         * ComputeSolutionPointPosteriorStd-Compute the posterior standard
         deviation of solution points after adjustment. Assign a value of 1 to
         compute or 0 for no computation. The default is 0.
     location_accuracy {String}:
         Specifies the geometric accuracy level of the images.This parameter is
         only enabled if the transformation_type parameter is
         specified as RPC.

         * HIGH-The accuracy will be 30 meters or less.

         * MEDIUM-The accuracy will be between 31 meters and 100 meters.

         * LOW-The accuracy will be more than 100 meters.

         * VERY_HIGH-The imagery was collected with a high-accuracy,
         differential GPS, such as RTK or PPK. This option will keep image
         locations fixed during block adjustment.
         If LOW is specified, the control points will first be improved by an
         initial triangulation; then they will be used in the block adjustment
         calculation. The medium and high accuracy options do not require
         additional estimation processing.
     DEM {Raster Dataset / Raster Layer / Mosaic Dataset / Mosaic Layer}:
         An input DEM from which elevations will be sampled as ground control
         points for refining the geometric accuracy of the image network in the
         adjustment.The parameter is only enabled when the transformation_type
         parameter
         is specified as Frame.
     elevation_accuracy {Double}:
         The elevation accuracy of the input DEM. The accuracy value will be
         used as a weight for the sampled ground control points in the
         adjustment.The parameter is only enabled when the transformation_type
         parameter
         is specified as Frame.

    OUTPUTS:
     out_solution_table (Table):
         The output solution table containing the adjustments.
     out_solution_point_table {Feature Class}:
         The output solution points table. This will be saved as a polygon
         feature class. This output can be quite large.
     out_quality_table {Table}:
         An output table used to store adjustment quality information.This
         parameter is only enabled if the transformation_type parameter is
         specified as RPC."""
    ...

@gptooldoc("ComputeCameraModel_management", None)
def ComputeCameraModel(
    in_mosaic_dataset=...,
    out_dsm=...,
    gps_accuracy=...,
    estimate=...,
    refine=...,
    apply_adjustment=...,
    maximum_residual=...,
    initial_tiepoint_resolution=...,
    out_control_points=...,
    out_solution_table=...,
    out_solution_point_table=...,
    out_flight_path=...,
    maximum_overlap=...,
    minimum_coverage=...,
    remove=...,
    in_control_points=...,
    options=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeCameraModel_management(in_mosaic_dataset, {out_dsm}, {gps_accuracy}, {estimate}, {refine}, {apply_adjustment}, {maximum_residual}, {initial_tiepoint_resolution}, {out_control_points}, {out_solution_table}, {out_solution_point_table}, {out_flight_path}, {maximum_overlap}, {minimum_coverage}, {remove}, {in_control_points}, {options;options...})

       Estimates the exterior camera model and interior camera model from the
       EXIF header of the raw image and refines the camera models. The model
       is then applied to the mosaic dataset with an option to use a tool-
       generated, high-resolution digital surface model (DSM) to achieve
       better orthorectification.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The mosaic dataset on which the camera model will be built and
         calculated.It is recommended that you run the Apply Block Adjustment
         tool on the
         input mosaic dataset first.
     gps_accuracy {String}:
         Specifies the accuracy level of the input images. The tool will search
         for images in the neighborhood to compute matching points and
         automatically apply an adjustment strategy based on the accuracy
         level.

         * HIGH-The GPS accuracy is 0 to 10 meters, and the tool uses a
         maximum of 4 by 3 images.

         * MEDIUM-The GPS accuracy is 10 to 20 meters, and the tool uses a
         maximum of 4 by 6 images.

         * LOW-The GPS accuracy is 20 to 50 meters, and the tool uses a maximum
         of 4 by 12 images.

         * VERY_LOW-The GPS accuracy is more than 50 meters, and the tool uses
         a maximum of 4 by 20 images.

         * VERY_HIGH-Imagery was collected with high-accuracy, differential
         GPS, such as RTK or PPK. This option will hold image locations fixed
         during block adjustment.
     estimate {Boolean}:
         Specifies whether the camera model will be estimated by computing the
         adjustment based on eight times the mosaic dataset's source
         resolution. Computing the adjustment at this level will be faster but
         less accurate.

         * ESTIMATE-The camera model will be estimated. This is the default.

         * NO_ESTIMATE-The camera model will not be estimated.
     refine {Boolean}:
         Specifies whether the camera model will be refined by computing the
         adjustment at the mosaic dataset resolution. Computing the adjustment
         at this level will provide the most accurate result.

         * REFINE-The camera model will be refined by computing the adjustment
         at the source resolution. This is the default.

         * NO_REFINE-The camera model will not be refined. This option will be
         faster, so it is a good option when the computation does not need to
         be performed at the source resolution.
     apply_adjustment {Boolean}:
         Specifies whether the calculated adjustment will be applied to the
         input mosaic dataset.

         * APPLY-The calculated adjustment will be applied to the input mosaic
         dataset. This is the default.

         * NO_APPLY-The calculated adjustment will not be applied to the input
         mosaic dataset.
     maximum_residual {Double}:
         The maximum residual value allowed to keep a computed control point as
         a valid control point. The default is 5.
     initial_tiepoint_resolution {Double}:
         The resolution factor at which tie points will be generated when
         estimating the camera model. The default value is 8, which means eight
         times the source pixel resolution.For images with only minor
         differentiation of features, such as
         agriculture fields, a lower value such as 2 can be used.
     maximum_overlap {Double}:
         The percentage of overlap between two images to consider them
         duplicates.For example, if the value is 0.9, it means if an image is
         90 percent
         covered by another image, it will be considered a duplicate and
         removed.
     minimum_coverage {Double}:
         The percentage indicating the control point's coverage on an image. If
         the coverage is less than the minimum percentage, the image will be
         unresolved and removed. The default is 0.
     remove {Boolean}:
         Specifies whether images will be automatically removed if they are too
         far from the flight strip.

         * NO_REMOVE-Images will not be removed. This is the default.

         * REMOVE-Images that are too far away from the flight strip will be
         removed.
     in_control_points {Feature Class}:
         The tie point table used to compute the camera model. If a tie point
         table is not specified, the tool will compute its own tie points and
         estimate the camera model.
     options {Value Table}:
         Additional options for the adjustment engine. The specifications of
         many of the options are supplied by the data provider. The
         options include the following:

         * CalibrateF-The sensor's focal length will be calibrated for use in
         the block adjustment. Assign a value of 1 for focal length
         calibration, or 0 to not calibrate. The default is 1.

         * CalibratePP-The principle point in the block adjustment will be
         calibrated. Assign a value of 1 for calibration, or 0 to not
         calibrate. The default is 1.

         * CalibrateP-Radial distortion parameters in the block adjustment will
         be calibrated. Assign a value of 1 for calibration, or 0 to not
         calibrate. The default is 1.

         * CalibrateK-Tangential distortion parameters in the block adjustment
         will be calibrated. Assign a value of 1 for calibration, or 0 to not
         calibrate. The default is 1.

         * EstimateOPK-The Omega, Phi, and Kappa angles will be
         calibrated to define the rotation between the image coordinate system
         and the projected coordinate system. Assign a value of 0 to use
         orientation angles (roll, pitch, and yaw) from UAV metadata as
         attitude initials in the block adjustment. Use a value of 1 to
         estimate orientation angles, and use estimated orientation angles as
         attitude initials in the block adjustment. The default is 1.
         For most DJI and Skydio cameras, a value of 0 is recommended.

         * APrioriAccuracyX-The accuracy of the x-coordinate provided by the
         metadata. The units must match PerspectiveX. This option is not
         recommended for most UAV data.

         * APrioriAccuracyY-The accuracy of the y-coordinate provided by the
         metadata. The units must match PerspectiveY. This option is not
         recommended for most UAV data.

         * APrioriAccuracyZ-The accuracy of the z-coordinate provided by the
         metadata. The units must match PerspectiveZ. This option is not
         recommended for most UAV data.

         * APrioriAccuracyXY-The accuracy of the planar coordinate provided by
         the metadata. The units must match PerspectiveX. This option is not
         recommended for most UAV data.

         * APrioriAccuracyXYZ-The accuracy of image location provided by the
         metadata. The units must match PerspectiveX. This option is not
         recommended for most UAV data.

         * APrioriAccuracyOmega-The accuracy of the Omega angle provided by the
         airborne Position Orientation System (POS). The units are in decimal
         degrees.

         * APrioriAccuracyPhi-The accuracy of the Phi angle provided by the
         airborne Position Orientation System (POS). The units are in decimal
         degrees.

         * APrioriAccuracyOmegaPhi-The accuracy of the Omega or Phi angle
         provided by the airborne Position Orientation System (POS). The units
         are in decimal degrees.

         * APrioriAccuracyKappa-The accuracy of the Kappa angle provided by the
         airborne Position Orientation System (POS). The units are in decimal
         degrees.

         * ComputeImagePosteriorStd-The posterior standard deviation of image
         location and orientation after adjustment will be computed. Assign a
         value of 1 to compute, or 0 to not compute. The default is 1.

         * ComputeSolutionPointPosteriorStd-The posterior standard deviation of
         solution points after adjustment will be computed. Assign a value of 1
         to compute, or 0 to not compute. The default is 0.

    OUTPUTS:
     out_dsm {Raster Dataset}:
         A DSM raster dataset generated from the adjusted images in the mosaic
         dataset. If apply_adjustment is set to APPLY, this DSM will replace
         the DEM in the geometric function to achieve better
         orthorectification.
     out_control_points {Feature Class}:
         The optional control points feature class.
     out_solution_table {Table}:
         The optional adjustment solution table. The solution table contains
         the root mean square (RMS) of the adjustment error and solution
         matrix.
     out_solution_point_table {Feature Class}:
         The optional solution point feature class. The solution points are the
         final controls points used to generate the adjustment solution.
     out_flight_path {Feature Class}:
         The optional flight path line feature class."""
    ...

@gptooldoc("ComputeControlPoints_management", None)
def ComputeControlPoints(
    in_mosaic_dataset=...,
    in_reference_images=...,
    out_control_points=...,
    similarity=...,
    out_image_feature_points=...,
    density=...,
    distribution=...,
    area_of_interest=...,
    location_accuracy=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeControlPoints_management(in_mosaic_dataset, in_reference_images, out_control_points, {similarity}, {out_image_feature_points}, {density}, {distribution}, {area_of_interest}, {location_accuracy})

       Creates the control points between the mosaic dataset and the
       reference image. The control points can then be used in conjunction
       with tie points to compute the adjustments for the mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset that will be used to create control points.
     in_reference_images (Raster Dataset / Image Service / Map Server / WMS Map / Raster Layer / Mosaic Layer / Internet Tiled Layer / Map Server Layer):
         The reference images that will be used to create control points for
         your mosaic dataset. If you have multiple images, create a mosaic
         dataset from the images and use the mosaic dataset as the reference.
     similarity {String}:
         Specifies the similarity level that will be used for matching tie
         points.

         * LOW-The similarity criteria for the two matching points will be low.
         This option will produce the most matching points, but some of the
         matches may have a higher level of error.

         * MEDIUM-The similarity criteria for the matching points will be
         medium.

         * HIGH-The similarity criteria for the matching points will be high.
         This option will produce the fewest matching points, but each match
         will have a lower level of error.
     density {String}:
         Specifies the number of tie points to be created.

         * LOW-The density of points will be low, creating the fewest number of
         tie points.

         * MEDIUM-The density of points will be medium, creating a moderate
         number of points.

         * HIGH-The density of points will be high, creating the highest number
         of points.
     distribution {String}:
         Specifies whether the points will have regular or random distribution.

         * RANDOM-Points will be generated randomly. Randomly generated points
         are better for overlapping areas with irregular shapes.

         * REGULAR-Points will be generated based on a fixed pattern. Points
         based on a fixed pattern use the point density to determine how
         frequently to create points.
     area_of_interest {Feature Layer}:
         Limit the area in which tie points are generated to only this polygon
         feature class.
     location_accuracy {String}:
         Specifies the keyword that describes the accuracy of the imagery.

         * LOW-Images have a large shift and a large rotation (> 5 degrees).The
         SIFT algorithm will be used in the point-matching computation.

         * MEDIUM-Images have a medium shift and a small rotation (<5
         degrees).The Harris algorithm will be used in the point-matching
         computation.

         * HIGH-Images have a small shift and a small rotation.The Harris
         algorithm will be used in the point-matching computation.

    OUTPUTS:
     out_control_points (Feature Class):
         The output control point table. This table will contain the control
         points that were created.
     out_image_feature_points {Feature Class}:
         The output image feature points table. This will be saved as a polygon
         feature class. This output can be quite large."""
    ...

@gptooldoc("ComputeFiducials_management", None)
def ComputeFiducials(
    in_mosaic_dataset=...,
    out_fiducial_table=...,
    where_clause=...,
    fiducial_templates=...,
    film_coordinate_system=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeFiducials_management(in_mosaic_dataset, out_fiducial_table, {where_clause}, {fiducial_templates}, {film_coordinate_system})

       Computes the fiducial coordinates in image and film space for each
       image in a mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset created from scanned aerial photos using scanned
         raster type or frame camera raster type.
     where_clause {SQL Expression}:
         A query definition string that defines a subset of rasters for
         computing fiducials.
     fiducial_templates {Table View / File / String}:
         The fiducial template table that contains required fields for storing
         fiducial pictures and other properties.
     film_coordinate_system {String}:
         A keyword that defines the film coordinate system of the scanned
         aerial photograph. It is used in computing fiducial information and
         affine transformation construction.

         * NO_CHANGE-Maintain the coordinate system of the mosaic dataset. Do
         not change the film coordinate system of the scanned aerial
         photograph. Maintain the coordinate system of the mosaic dataset.

         * X_RIGHT_Y_UP-The origin of the scanned photo's coordinate system is
         the center, and positive X points right and positive Y points up.

         * X_UP_Y_LEFT-The origin of the scanned photo's coordinate system is
         the center, and positive X points up and positive Y points left.

         * X_LEFT_Y_DOWN-The origin of the scanned photo's coordinate system is
         the center, and positive X points left and positive Y points down.

         * X_DOWN_Y_RIGHT-The origin of the scanned photo's coordinate system
         is the center, and positive X points down and positive Y points right.

    OUTPUTS:
     out_fiducial_table (Table):
         The output table that stores all the fiducial coordinate information
         in image and film space."""
    ...

@gptooldoc("ComputeTiePoints_management", None)
def ComputeTiePoints(
    in_mosaic_dataset=...,
    out_control_points=...,
    similarity=...,
    in_mask_dataset=...,
    out_image_features=...,
    density=...,
    distribution=...,
    location_accuracy=...,
    options=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputeTiePoints_management(in_mosaic_dataset, out_control_points, {similarity}, {in_mask_dataset}, {out_image_features}, {density}, {distribution}, {location_accuracy}, {options;options...})

       Computes the tie points between overlapped mosaic dataset items. The
       tie points can then be used to compute the block adjustments for the
       mosaic dataset.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset that will be used to create tie points.
     similarity {String}:
         Specifies the similarity level that will be used for matching tie
         points.

         * LOW-The similarity criteria for the two matching points will be low.
         This option will produce the most matching points, but some of the
         matches may have a higher level of error.

         * MEDIUM-The similarity criteria for the matching points will be
         medium.

         * HIGH-The similarity criteria for the matching points will be high.
         This option will produce the fewest matching points, but each match
         will have a lower level of error.
     in_mask_dataset {Feature Layer}:
         A polygon feature class used to exclude areas that will not be
         included in the computation of control points.A field with a name of
         mask can control the inclusion or exclusion of
         areas. A value of 1 indicates that the areas defined by the polygons
         (inside) will be excluded from the computation. A value of 2 indicates
         the defined polygons (inside) will be included in the computation
         while areas outside of the polygons will be excluded.
     density {String}:
         Specifies the number of tie points to be created.

         * LOW-The density of points will be low, creating the fewest number of
         tie points.

         * MEDIUM-The density of points will be medium, creating a moderate
         number of points.

         * HIGH-The density of points will be high, creating the highest number
         of points.
     distribution {String}:
         Specifies whether the points will have regular or random distribution.

         * RANDOM-Points will be generated randomly. Randomly generated points
         are better for overlapping areas with irregular shapes.

         * REGULAR-Points will be generated based on a fixed pattern. Points
         based on a fixed pattern use the point density to determine how
         frequently to create points.
     location_accuracy {String}:
         Specifies the keyword that describes the accuracy of the imagery.

         * LOW-Images have a large shift and a large rotation (> 5 degrees).The
         SIFT algorithm will be used in the point-matching computation.

         * MEDIUM-Images have a medium shift and a small rotation (<5
         degrees).The Harris algorithm will be used in the point-matching
         computation.

         * HIGH-Images have a small shift and a small rotation.The Harris
         algorithm will be used in the point-matching computation.
     options {Value Table}:
         Additional options for the adjustment engine. The options are only
         used by third-party adjustment engines.

    OUTPUTS:
     out_control_points (Feature Class):
         The output control point table. The table will contain the tie points
         created by this tool.
     out_image_features {Feature Class}:
         The output image feature points table. This will be saved as a polygon
         feature class. This output can be quite large."""
    ...

@gptooldoc("ExportFrameAndCameraParameters_management", None)
def ExportFrameAndCameraParameters(
    input_mosaic_dataset=..., output_file=..., output_format=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportFrameAndCameraParameters_management(input_mosaic_dataset, output_file, {output_format})

       Exports frame and camera parameters from a mosaic dataset that
       contains frame imagery.

    INPUTS:
     input_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset.
     output_format {String}:
         Specifies the output file format for the frame and camera parameters.

         * ESRI_FRAME_AND_CAMERA_TABLE-The frame and camera parameters will be
         exported as an Esri Frames and Camera table (.csv file). This is the
         default.

         * PIX4D_CALIBRATED_CAMERA_PARAMETERS-The frame and camera parameters
         will be exported using the Pix4D calibrated camera parameters format
         (.txt file).

    OUTPUTS:
     output_file (File):
         The output file containing the frame and camera parameters. Supported
         file formats include .csv and .txt."""
    ...

@gptooldoc("GenerateBlockAdjustmentReport_management", None)
def GenerateBlockAdjustmentReport(
    input_mosaic_dataset=...,
    input_solution_table=...,
    input_solution_point=...,
    output_report=...,
    input_control_point_for_adjustment=...,
    report_format=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateBlockAdjustmentReport_management(input_mosaic_dataset, input_solution_table, input_solution_point, output_report, {input_control_point_for_adjustment}, {report_format})

       Generates a report after performing ortho mapping block adjustment to
       a mosaic dataset. The report is critical in evaluating the quality and
       accuracy of the ortho mapping products.

    INPUTS:
     input_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset path.
     input_solution_table (Table View):
         The associated solution point table after block adjustment.
     input_solution_point (Table View):
         The solution point feature class.
     input_control_point_for_adjustment {Table View}:
         The associated control points table, which may include tie points and
         ground control points.
     report_format {String}:
         The output format of the block adjustment report.

         * HTML-Adjustment report is created as an HTML file. This is the
         default.

         * PDF-Adjustment report is created as a PDF file.

    OUTPUTS:
     output_report (File):
         The output ortho mapping report file path and name. The supported
         output format for a website is HTML."""
    ...

@gptooldoc("GeneratePointCloud_management", None)
def GeneratePointCloud(
    in_mosaic_dataset=...,
    matching_method=...,
    out_folder=...,
    out_base_name=...,
    object_size=...,
    ground_spacing=...,
    minimum_pairs=...,
    minimum_area=...,
    minimum_adjustment_quality=...,
    maximum_diff_gsd=...,
    maximum_diff_OP=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeneratePointCloud_management(in_mosaic_dataset, matching_method, out_folder, out_base_name, {object_size}, {ground_spacing}, {minimum_pairs}, {minimum_area}, {minimum_adjustment_quality}, {maximum_diff_gsd}, {maximum_diff_OP})

       Computes 3D points from stereo pairs and outputs a point cloud as a
       set of LAS files.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The input mosaic dataset, which must have completed the block
         adjustment process and have a stereo model.To block adjust the mosaic
         dataset, use the Apply Block Adjustment
         tool. To build a stereo model on the mosaic dataset, use the Build
         Stereo Model tool.
     matching_method (String):
         The method used to generate 3D points.

         * ETM-A feature-based stereo matching in which the Harris operator is
         used in detecting feature points. Since less feature points are
         extracted, this method is fast and can be used for data with less
         terrain variations and detail.

         * SGM-Semi-Global Matching (SGM) produces points that are denser and
         have more detailed terrain information. It can be used for images of
         urban areas. This is more computational intensive than ETM.1

         * MVM-Multi-view image matching (MVM) is based on the SGM matching
         method followed by a fusion step in which the redundant depth
         estimations across a single stereo model are merged. It produces dense
         3D points and is computationally efficient.2
                 References:

         * Heiko Hirschmuller et al., "Memory Efficient Semi-Global Matching,"
         ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
         Information Sciences, Volume 1-3, (2012): 371-376.

         * Hirschmuller, H. "Stereo Processing by Semiglobal Matching and
         Mutual Information." Pattern Analysis and Machine Intelligence,
         (2008).
     object_size {Double}:
         A search radius within which surface objects, such as buildings or
         trees, will be identified. It is the linear size in map units.
     ground_spacing {Double}:
         The ground spacing, in meters, at which the 3D points are
         generated.The default is five times the source image pixel size.
     minimum_pairs {Double}:
         The number of pairs used to generate 3D points. The default value is a
         minimum of 2 image pairs.Sometimes a location may be covered with many
         image pairs. In this
         case, the tool will order the pairs based on the various threshold
         parameters specified in this tool. The pairs with the highest scores
         will be used to generate the points.
     minimum_area {Double}:
         Specify a minimum overlap threshold area that is acceptable, which is
         a percentage of overlap between a pair of images. Image pairs with
         overlap areas smaller than this threshold will receive a score of 0
         for this criteria and will descend in the ordered list. The range of
         values for the threshold is from 0 to 1. The default threshold value
         is 0.6, which is equal to 60 percent.
     minimum_adjustment_quality {Double}:
         Specify the minimum adjustment quality that is acceptable. The
         threshold value will be compared to the adjustment quality value that
         is stored in the stereo model. Image pairs with an adjustment quality
         less than the specified threshold will receive a score of 0 for this
         criteria and will descend in the ordered list. The range of values for
         the threshold is from 0 to 1. The default value is 0.2, which is equal
         to 20 percent.
     maximum_diff_gsd {Double}:
         Specify the maximum allowable threshold for the ground sample distance
         (GSD) between two images in a pair. The resolution ratio between the
         two images will be compared to the threshold value. Image pairs with a
         ground sample ratio greater than this threshold will receive a score
         of 0 for this criteria and will descend in the ordered list. The
         default threshold ratio is 2.
     maximum_diff_OP {Double}:
         Specify the maximum threshold for the Omega\\Phi difference between the
         two image pairs. The Omega values and Phi values for the image pairs
         are compared. Image pairs with an Omega or a Phi difference greater
         than this threshold will receive a score of 0 for this criteria and
         will descend in the ordered list. The default threshold difference for
         each comparison is 8.

    OUTPUTS:
     out_folder (Folder):
         The folder used to store the output LAS files.If this tool is run
         multiple times with the same input parameters, the
         output may be slightly different due to random sampling.
     out_base_name (String):
         A string used as a prefix to formulate the output LAS file names. For
         example, if name is used as the base, the output files will be named
         name1.las, name2.las, and so on."""
    ...

@gptooldoc("InterpolateFromPointCloud_management", None)
def InterpolateFromPointCloud(
    in_container=...,
    out_raster=...,
    cell_size=...,
    interpolation_method=...,
    smooth_method=...,
    surface_type=...,
    fill_dem=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """InterpolateFromPointCloud_management(in_container, out_raster, cell_size, interpolation_method, smooth_method, {surface_type}, {fill_dem})

       Interpolates a digital terrain model (DTM) or a digital surface model
       (DSM) from a point cloud.

    INPUTS:
     in_container (Folder / File / Feature Layer):
         The path and name of the file, folder, or feature layer. The input can
         be a folder of LAS files or a solution point table from orthomapping
         tools.The LAS files can be the output from the Generate Point Cloud
         tool, in
         which LAS points are categorized as ground and above ground. The
         solution point table is output from either the Compute Block
         Adjustments tool or the Compute Camera Model tool.
     cell_size (Double):
         The cell size of the output raster dataset.
     interpolation_method (String):
         Specifies the method that will be used to interpolate the output
         raster dataset from the point cloud.

         * TRIANGULATION-The triangulation method will be used. It is also
         known as triangulated irregular network (TIN) linear interpolation and
         is designed for irregularly distributed sparse points, such as
         solution points from block adjustment computation.

         * NATURAL_NEIGHBOR-The natural neighbor method will be used. It is
         similar to triangulation but generates a smoother surface and is more
         computationally intensive.

         * IDW-The inverse distance weighted (IDW) average method will be used.
         It is used for regularly distributed dense points, such as point cloud
         LAS files from the Generate Point Cloud tool. The IDW search radius is
         automatically computed based on average point density.
     smooth_method (String):
         Specifies the filter that will be used to smooth the output raster
         dataset.

         * GAUSS3x3-A Gaussian filter with a 3 by 3 window will be used.

         * GAUSS5x5-A Gaussian filter with a 5 by 5 window will be used.

         * GAUSS7x7-A Gaussian filter with a 7 by 7 window will be used.

         * GAUSS9x9-A Gaussian filter with a 9 by 9 window will be used.

         * NONE-No smoothing filter will be used.
     surface_type {String}:
         Specifies whether a digital terrain model or a digital surface model
         will be created.

         * DTM-A digital terrain model will be created by interpolating only
         the ground points.

         * DSM-A digital surface model will be created by interpolating all the
         points.
     fill_dem {Raster Dataset / Raster Layer / Mosaic Dataset / Mosaic Layer}:
         A DEM raster input that is used to fill NoData areas. Areas of NoData
         may exist where pixels do not have enough information from the input
         to generate values.

    OUTPUTS:
     out_raster (Raster Dataset):
         The output raster dataset location, name, and file extension.The
         output can be created in most writable raster formats, such as
         TIFF, CRF, or IMG."""
    ...

@gptooldoc("MatchControlPoints_management", None)
def MatchControlPoints(
    in_mosaic_dataset=..., in_control_points=..., out_control_points=..., similarity=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MatchControlPoints_management(in_mosaic_dataset, in_control_points, out_control_points, {similarity})

       Creates matching tie points for a given ground control point and
       initial tie point in one of the overlapping images.

    INPUTS:
     in_mosaic_dataset (Mosaic Dataset / Mosaic Layer):
         The mosaic dataset that contains the source imagery from which the tie
         points will be created.
     in_control_points (File / Feature Class / Feature Layer / String):
         The input control point set that contains a list of ground control
         point features and at least one initial tie point for each ground
         control point.
     similarity {String}:
         Specifies the similarity level that will be used for matching tie
         points.

         * LOW-The similarity criteria for the two matching points will be low.
         This option will produce the most matching points, but some of the
         matches may have a higher level of error.

         * MEDIUM-The similarity criteria for the matching points will be
         medium.

         * HIGH-The similarity criteria for the matching points will be high.
         This option will produce the fewest matching points, but each match
         will have a lower level of error.

    OUTPUTS:
     out_control_points (Feature Class):
         The output control point features that contain ground control points."""
    ...

@gptooldoc("UpdateInteriorOrientation_management", None)
def UpdateInteriorOrientation(
    in_mosaic_dataset=...,
    where_clause=...,
    fiducial_table=...,
    film_coordinate_system=...,
    update_footprints=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpdateInteriorOrientation_management(in_mosaic_dataset, {where_clause}, {fiducial_table}, {film_coordinate_system}, {update_footprints})

       Refines the interior orientation for each image in the mosaic dataset
       by constructing an affine transformation from a fiducial table.

    INPUTS:
     in_mosaic_dataset (Mosaic Layer):
         The mosaic dataset that is created from scanned aerial photos using
         the scanned raster type or frame camera raster type.
     where_clause {SQL Expression}:
         A query definition string that defines a subset of rasters for
         computing fiducials.
     fiducial_table {Table View}:
         The fiducial table created using the Compute Fiducials tool.
     film_coordinate_system {String}:
         Defines the film coordinate system of the scanned aerial photograph.
         It is used in computing fiducial information and affine transformation
         construction.

         * NO_CHANGE-Maintain the coordinate system of the mosaic dataset. Do
         not change the film coordinate system of the scanned aerial
         photograph. Maintain the coordinate system of the mosaic dataset.

         * X_RIGHT_Y_UP-The origin of the scanned photo's coordinate system is
         the center, and positive X points right and positive Y points up.

         * X_UP_Y_LEFT-The origin of the scanned photo's coordinate system is
         the center, and positive X points up and positive Y points left.

         * X_LEFT_Y_DOWN-The origin of the scanned photo's coordinate system is
         the center, and positive X points left and positive Y points down.

         * X_DOWN_Y_RIGHT-The origin of the scanned photo's coordinate system
         is the center, and positive X points down and positive Y points right.
     update_footprints {Boolean}:
         Generates or updates the footprints of the digital photos in the
         mosaic dataset.

         * UPDATE-The footprints will be generated or updated.

         * NO_UPDATE-The footprints will not be generated or updated. This is
         the default"""
    ...

@gptooldoc("CopyRaster_management", None)
def CopyRaster(
    in_raster=...,
    out_rasterdataset=...,
    config_keyword=...,
    background_value=...,
    nodata_value=...,
    onebit_to_eightbit=...,
    colormap_to_RGB=...,
    pixel_type=...,
    scale_pixel_value=...,
    RGB_to_Colormap=...,
    format=...,
    transform=...,
    process_as_multidimensional=...,
    build_multidimensional_transpose=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CopyRaster_management(in_raster, out_rasterdataset, {config_keyword}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {colormap_to_RGB}, {pixel_type}, {scale_pixel_value}, {RGB_to_Colormap}, {format}, {transform}, {process_as_multidimensional}, {build_multidimensional_transpose})

       Saves a copy of a raster dataset or converts a mosaic dataset into a
       single raster dataset.

    INPUTS:
     in_raster (Raster Dataset / Mosaic Dataset / Mosaic Layer / Raster Layer / File / Image Service):
         The raster dataset or mosaic dataset to be copied.
     config_keyword {String}:
         The storage parameters (configuration) for a geodatabase.
         Configuration keywords are set up by your database administrator.
     background_value {Double}:
         Remove the unwanted values created around the raster data. The value
         specified will be distinguished from other valuable data in the raster
         dataset. For example, a value of zero along the raster dataset's
         borders will be distinguished from zero values in the raster
         dataset.The pixel value specified will be set to NoData in the output
         raster
         dataset.For file-based rasters, Ignore Background Value must be set to
         the
         same value as NoData Value for the background value to be ignored.
         Enterprise and geodatabase rasters will work without this extra step.
     nodata_value {String}:
         All the pixels with the specified value will be set to NoData in the
         output raster dataset.
     onebit_to_eightbit {Boolean}:
         Specifies whether the input 1-bit raster dataset will be converted to
         an 8-bit raster dataset. In this conversion, the value 1 in the input
         raster dataset will be changed to 255 in the output raster dataset.
         This is useful when importing a 1-bit raster dataset to a geodatabase.
         One-bit raster datasets have 8-bit pyramid layers when stored in a
         file system, but in a geodatabase, 1-bit raster datasets can only have
         1-bit pyramid layers, which results in a lower-quality display. By
         converting the data to 8 bit in a geodatabase, the pyramid layers are
         built as 8 bit instead of 1 bit, resulting in a proper raster dataset
         in the display.

         * NONE-No conversion will occur. This is the default.

         * OneBitTo8Bit-The input raster will be converted.
     colormap_to_RGB {Boolean}:
         Specifies whether the input raster dataset will be converted to a
         three-band output raster dataset if the input raster dataset includes
         a color map. This is useful when mosaicking rasters with different
         color maps.

         * NONE-No conversion will occur. This is the default.

         * ColormapToRGB-The input dataset will be converted.
     pixel_type {String}:
         Specifies the bit depth, or radiometric resolution, that will be used
         for the raster or mosaic dataset. If not defined, the value from the
         first raster dataset will be used.

         * 1_BIT-The pixel type will be a 1-bit unsigned integer. The values
         can be 0 or 1.

         * 2_BIT-The pixel type will be a 2-bit unsigned integer. The values
         supported can range from 0 to 3.

         * 4_BIT-The pixel type will be a 4-bit unsigned integer. The values
         supported can range from 0 to 15.

         * 8_BIT_UNSIGNED-The pixel type will be an unsigned 8-bit data type.
         The values supported can range from 0 to 255.

         * 8_BIT_SIGNED-The pixel type will be a signed 8-bit data type. The
         values supported can range from -128 to 127.

         * 16_BIT_UNSIGNED-The pixel type will be a 16-bit unsigned data type.
         The values can range from 0 to 65,535.

         * 16_BIT_SIGNED-The pixel type will be a 16-bit signed data type. The
         values can range from -32,768 to 32,767.

         * 32_BIT_UNSIGNED-The pixel type will be a 32-bit unsigned data type.
         The values can range from 0 to 4,294,967,295.

         * 32_BIT_SIGNED-The pixel type will be a 32-bit signed data type. The
         values can range from -2,147,483,648 to 2,147,483,647.

         * 32_BIT_FLOAT-The pixel type will be a 32-bit data type supporting
         decimals.

         * 64_BIT-The pixel type will be a 64-bit data type supporting
         decimals.
     scale_pixel_value {Boolean}:
         Specifies whether pixel values will be scaled. When the output is a
         pixel type other than the input (such as 16 bit to 8 bit), you can
         scale the values to fit into the new range; otherwise, the values that
         do not fit into the new pixel range will be discarded.If scaling up,
         such as 8 bit to 16 bit, the minimum and maximum of the
         8-bit values will be scaled to the minimum and maximum in the 16-bit
         range. If scaling down, such as 16 bit to 8 bit, the minimum and
         maximum of the 16-bit values will be scaled to the minimum and maximum
         in the 8-bit range.

         * NONE-The pixel values will remain the same and will not be scaled.
         Any values that do not fit within the value range will be discarded.
         This is the default.

         * ScalePixelValue-The pixel values will be scaled to the new pixel
         type. When you scale the pixel depth, the raster will display the
         same, but the values will be scaled to the new bit depth that was
         specified.
     RGB_to_Colormap {Boolean}:
         Specifies whether an 8-bit, 3-band (RGB) raster dataset will be
         converted to a single-band raster dataset with a color map. This
         operation suppresses noise that is often found in scanned images and
         is ideal for screen captures, scanned maps, or scanned documents. This
         is not recommended for satellite or aerial imagery or thematic raster
         data.

         * NONE-The RGB raster dataset will not be converted.

         * RGBToColormap-The RGB raster dataset will be converted to a color
         map.
     format {String}:
         Specifies the output raster format.

         * TIFF-The output format will be TIFF.

         * COG-The output format will be Cloud Optimized GeoTIFF.

         * IMAGINE Image-The output format will be ERDAS IMAGINE.

         * BMP-The output format will be BMP.

         * GIF-The output format will be GIF.

         * PNG-The output format will be PNG.

         * JPEG-The output format will be JPEG.

         * JP2-The output format will be JPEG 2000.

         * GRID-The output format will be Esri Grid.

         * BIL-The output format will be Esri BIL.

         * BSQ-The output format will be Esri BSQ.

         * BIP-The output format will be Esri BIP.

         * ENVI-The output format will be ENVI DAT.

         * CRF-The output format will be CRF.

         * MRF-The output format will be MRF.

         * NetCDF-The output format will be NetCDF.

         * ZARR-The output format will be Zarr.
     transform {Boolean}:
         Specifies whether a transformation associated with the input raster
         will be applied to the output. The input raster can have a
         transformation associated with it that is not saved in the input, such
         as a world file or a geometric function.

         * NONE-No associated transformation will be applied to the output.

         * Transform-Any associated transformation will be applied to the
         output.
     process_as_multidimensional {Boolean}:
         Specifies whether the input mosaic dataset will be processed as a
         multidimensional raster dataset.

         * CURRENT_SLICE-The input will not be processed as a multidimensional
         raster dataset. If the input is multidimensional, only the slice that
         is currently displayed will be processed. This is the default.

         * ALL_SLICES-The input will be processed as a multidimensional raster
         dataset and all slices will be processed to produce a new
         multidimensional raster dataset. Set the format parameter to CRF to
         use this option.
     build_multidimensional_transpose {Boolean}:
         Specifies whether the transpose for the input multidimensional raster
         dataset will be built, which will chunk the data along each dimension
         to optimize performance when accessing pixel values across all slices.

         * NO_TRANSPOSE-No transpose will be built. This is the default.

         * TRANSPOSE-The input multidimensional raster dataset will be
         transposed. The process_as_multidimensional parameter must be set to
         ALL_SLICES to use this option.

    OUTPUTS:
     out_rasterdataset (Raster Dataset):
         The name and format for the raster dataset being created.

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .crf-CRF

         * .dat-ENVI DAT

         * .img-ERDAS IMAGINE

         * .gif-GIF

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .mrf-MRF

         * .nc-NetCDF

         * .png-PNG

         * .tif-TIFF and Cloud Optimized GeoTIFF

         * .zarr-Zarr

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to JPEG, JPEG 2000, or TIFF format, or
         to a geodatabase, you can specify a compression type and compression
         quality."""
    ...

@gptooldoc("CreateRandomRaster_management", None)
def CreateRandomRaster(
    out_path=...,
    out_name=...,
    distribution=...,
    raster_extent=...,
    cellsize=...,
    build_rat=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRandomRaster_management(out_path, out_name, {distribution}, {raster_extent}, {cellsize}, {build_rat})

       Creates a raster dataset of random values with a distribution you
       define.

    INPUTS:
     out_path (Workspace):
         The folder or geodatabase where the output raster dataset will be
         stored.
     out_name (String):
         The name and format of the raster dataset you are creating.To store
         the output as a raster dataset in a geodatabase, do not add a
         file extension to the raster dataset name.For file-based rasters, use
         the appropriate extension to specify the
         format to create as follows:

         * .tif-TIFF raster

         * .img-ERDAS IMAGINE raster

         * .crf-CRF raster

         * No extension-Esri Grid
     distribution {String}:
         Specifies the random value distribution method to use.Each type has
         one or two settings to control the distribution.

         * UNIFORM {Minimum}, {Maximum}-A uniform distribution with the defined
         range. The default values are 0.0 for {Minimum} and 1.0 for {Maximum}.
         Both values are of type double.

         * INTEGER {Minimum}, {Maximum}-An integer distribution with the
         defined range. The default values are 1 for {Minimum} and 10 for
         {Maximum}. Both values are of type long.

         * NORMAL {Mean}, {Standard Deviation}-A normal distribution with
         defined {Mean} and {Standard Deviation} values. The default values are
         0.0 for {Mean} and 1.0 for {Standard Deviation}. Both values are of
         type double.

         * EXPONENTIAL {Mean}-An exponential distribution with a defined {Mean}
         value. The default value is 1.0. The value is of type double.

         * POISSON {Mean}-A Poisson distribution with a defined {Mean} value.
         The default value is 1.0. The value is of type double.

         * GAMMA {Alpha}, {Beta}-A gamma distribution with defined {Alpha} and
         {Beta} values. The default values are 1.0 for {Alpha} and 1.0 for
         {Beta}. Both values are of type double.

         * BINOMIAL {N}, {Probability}-A binomial distribution with defined {N}
         and {Probability} values. The {N} value is of type long with a default
         of 10. The {Probability} value is of type double with a default of
         0.5.

         * GEOMETRIC {Probability}-A geometric distribution with a defined
         {Probability} value. The default value is 0.5. The value is of type
         double.

         * NEGATIVE BINOMIAL {r}, {Probability}-A Pascal distribution with
         defined {r} and {Probability} values. The {r} value is of type double
         with a default of 10.0. The {Probability} value is of type double with
         a default of 0.5.
     raster_extent {Extent}:
         The extent of the output raster dataset.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     cellsize {Double}:
         The spatial resolution of the output raster dataset.
     build_rat {Boolean}:
         Specifies whether the tool will unconditionally build a raster
         attribute table for the output raster in which the selected
         distribution results in an integer output raster.This parameter has no
         effect if the output raster is floating point.

         * BUILD-A raster attribute table will be unconditionally built for
         integer output rasters. This is the default.

         * DO_NOT_BUILD-A raster attribute table will not be built for integer
         output rasters if the number of unique values is greater than or equal
         to 65535. If the number of unique values is less than 65535, a raster
         attribute table will be built."""
    ...

@gptooldoc("CreateRasterDataset_management", None)
def CreateRasterDataset(
    out_path=...,
    out_name=...,
    cellsize=...,
    pixel_type=...,
    raster_spatial_reference=...,
    number_of_bands=...,
    config_keyword=...,
    pyramids=...,
    tile_size=...,
    compression=...,
    pyramid_origin=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRasterDataset_management(out_path, out_name, {cellsize}, pixel_type, {raster_spatial_reference}, number_of_bands, {config_keyword}, {pyramids}, {tile_size}, {compression}, {pyramid_origin})

       Creates an empty raster dataset.

    INPUTS:
     out_path (Workspace):
         The folder or geodatabase where the raster dataset will be stored.
     out_name (String):
         The name, location, and format for the newly created dataset.When
         storing the raster dataset in a file format, specify the file
         extension as follows:

         * .bil for Esri BIL

         * .bip for Esri BIP

         * .bmp for BMP

         * .bsq for Esri BSQ

         * .crf for CRF

         * .dat for ENVI DAT

         * .gif for GIF

         * .img for ERDAS IMAGINE

         * .jpg for JPEG

         * .jp2 for JPEG 2000

         * .png for PNG

         * .tif for TIFF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments.
     cellsize {Double}:
         The pixel size that will be used for the new raster dataset.
     pixel_type (String):
         The bit depth (radiometric resolution) of the output raster dataset.
         If this is not specified, the raster dataset will be created with a
         default pixel type of 8-bit unsigned integer.Not all data types are
         supported by all raster formats. Check the List
         of supported sensors help topic to ensure that the format you are
         using will support the necessary data type.

         * 1_BIT-The pixel type will be a 1-bit unsigned integer. The values
         can be 0 or 1.

         * 2_BIT-The pixel type will be a 2-bit unsigned integer. The values
         supported can range from 0 to 3.

         * 4_BIT-The pixel type will be a 4-bit unsigned integer. The values
         supported can range from 0 to 15.

         * 8_BIT_UNSIGNED-The pixel type will be an unsigned 8-bit data type.
         The values supported can range from 0 to 255.

         * 8_BIT_SIGNED-The pixel type will be a signed 8-bit data type. The
         values supported can range from -128 to 127.

         * 16_BIT_UNSIGNED-The pixel type will be a 16-bit unsigned data type.
         The values can range from 0 to 65,535.

         * 16_BIT_SIGNED-The pixel type will be a 16-bit signed data type. The
         values can range from -32,768 to 32,767.

         * 32_BIT_UNSIGNED-The pixel type will be a 32-bit unsigned data type.
         The values can range from 0 to 4,294,967,295.

         * 32_BIT_SIGNED-The pixel type will be a 32-bit signed data type. The
         values can range from -2,147,483,648 to 2,147,483,647.

         * 32_BIT_FLOAT-The pixel type will be a 32-bit data type supporting
         decimals.

         * 64_BIT-The pixel type will be a 64-bit data type supporting
         decimals.
     raster_spatial_reference {Coordinate System}:
         The coordinate system for the output raster dataset.If this is not
         specified, the coordinate system set in the environment
         settings will be used.
     number_of_bands (Long):
         The number of bands of the output raster dataset.
     config_keyword {String}:
         The storage parameters (configuration) for a file or enterprise
         geodatabase. Configuration keywords are set up by your database
         administrator.
     pyramids {Pyramid}:
         Creates pyramids.For Pyramid Levels, specify a number of -1 or higher.
         A value of 0
         will not create pyramids, and a value of -1 will automatically
         determine the correct number of pyramid layers to create.Pyramid
         Resampling Technique defines how the data will be resampled
         when creating the pyramids.

         * NEAREST-Use nearest neighbor for nominal data or raster datasets
         with color maps, such as land-use or pseudo color images.

         * BILINEAR-Use bilinear interpolation with continuous data, such as
         satellite imagery or aerial photography.

         * CUBIC-Use cubic convolution with continuous data, such as satellite
         imagery or aerial photography. It is similar to bilinear
         interpolation; however, it resamples the data using a larger matrix.
         Pyramid Compression Type defines the method used when compressing the
         pyramids.

         * DEFAULT-The compression that is normally used by the raster dataset
         format will be used.

         * LZ77-A lossless compression will be used. The values of the cells in
         the raster will not be changed.

         * JPEG-A lossy compression will be used.

         * NONE-No data compression will be used.
     tile_size {Tile Size}:
         The size of the tiles.The tile width controls the number of pixels
         that can be stored in
         each tile. This is specified as a number of pixels in x. The default
         tile width is 128.The tile height controls the number of pixels that
         can be stored in
         each tile. This is specified as a number of pixels in y. The default
         tile height is 128.Only geodatabases and enterprise geodatabases use
         tile size.
     compression {Compression}:
         Specifies the type of compression that will be used to store the
         raster dataset.

         * LZ77-Lossless compression that preserves all raster cell values will
         be used.

         * JPEG-Lossy compression that uses the public JPEG compression
         algorithm will be used. If you choose JPEG, you can also specify the
         compression quality. The valid compression quality value ranges are
         from 0 to 100. This compression can be used for .jpg files and .tif
         files.

         * JPEG 2000-Lossy compression will be used.

         * PACKBITS-PackBits compression will be used for .tif files.

         * LZW-Lossless compression that preserves all raster cell values will
         be used.

         * RLE-Run-length encoding will be used for .img files.

         * CCITT GROUP 3-Lossless compression for 1-bit data will be used.

         * CCITT GROUP 4-Lossless compression for 1-bit data will be used.

         * CCITT_1D-Lossless compression for 1-bit data will be used.

         * NONE-No compression will be used. This is the default.
     pyramid_origin {Point}:
         The origination location of the raster pyramid. It is recommended that
         you specify this point if you plan to build large mosaics in a file
         geodatabase or enterprise geodatabase, especially if you plan to
         mosaic them over time (for example, when updating).The pyramid
         reference point should be set to the upper left corner of
         your raster dataset.In setting this point for a file geodatabase or
         enterprise
         geodatabase, partial pyramiding will be used when updating with a new
         mosaicked raster dataset. Partial pyramiding updates the parts of the
         pyramid that do not exist due to the new mosaicked datasets. It is a
         good practice to set a pyramid reference point so that the entire
         raster mosaic will be below and to the right of this point. However, a
         pyramid reference point should not be set too large either."""
    ...

@gptooldoc("DownloadRasters_management", None)
def DownloadRasters(
    in_image_service=...,
    out_folder=...,
    where_clause=...,
    selection_feature=...,
    clipping=...,
    convert_rasters=...,
    format=...,
    compression_method=...,
    compression_quality=...,
    MAINTAIN_FOLDER=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DownloadRasters_management(in_image_service, out_folder, {where_clause}, {selection_feature}, {clipping}, {convert_rasters}, {format}, {compression_method}, {compression_quality}, {MAINTAIN_FOLDER})

       Downloads the source files from an image service or mosaic dataset.

    INPUTS:
     in_image_service (Image Service / String / Mosaic Layer / Raster Layer):
         The image service or mosaic dataset to download.
     out_folder (Folder):
         The destination for the image service or mosaic dataset.
     where_clause {SQL Expression}:
         An SQL expression to limit the download to raster datasets that
         satisfy the expression.
     selection_feature {Extent}:
         Limits the download to an extent of a feature class or bounding box.
         All raster datasets that intersect the extent will be downloaded.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     clipping {Boolean}:
         Specify if you want to clip the downloaded images based on the
         geometry of a feature. Any raster that intersects the clipping
         geometry will be clipped and then downloaded. This is useful when your
         area of interest is not a rectangle. When downloaded images are
         clipped, you need to specify an output format for the clipped images.

         * NO_CLIPPING-The files will be clipped based on the minimum bounding
         rectangle that has been specified. This is the default.

         * CLIPPING-The files will be clipped based on the geometry of the
         selection_feature.
     convert_rasters {Boolean}:
         Choose whether to always convert your rasters to the specified format,
         or to only convert when it is necessary.

         * CONVERT_AS_REQUIRED-Do not convert the raster datasets to a new
         format.

         * ALWAYS_CONVERT-Convert the downloaded raster datasets into another
         format. If you used selection_feature to limit the extent, then you
         need to specify a format in the format parameter.
     format {String}:
         Choose a output format for the downloaded raster datasets.

         * TIFF-Tagged Image File Format. This is the default.

         * BIL-Esri band interleaved by line.

         * BSQ-Esri band sequential.

         * BIP-Esri band interleaved by pixel.

         * BMP-Bitmap.

         * ENVI-ENVI DAT file.

         * IMAGINE Image-ERDAS IMAGINE.

         * JPEG-Joint Photographics Experts Group. If chosen, you can also
         specify the compression quality. The valid compression quality value
         ranges are from 0 to 100.

         * GIF-Graphic interchange format.

         * JP2-JPEG 2000. If chosen, you can also specify the compression
         quality. The valid compression quality value ranges are from 0 to 100.

         * PNG-Portable Network Graphics.
     compression_method {String}:
         Choose the compression method to use with the specified Output Format.

         * NONE-No compression will occur. This is the default.

         * JPEG-Lossy compression that uses the public JPEG compression
         algorithm. If you choose JPEG, you can also specify the compression
         quality. The valid compression quality value ranges are from 0 to 100.
         This compression can be used for JPEG files and TIFF files.

         * LZW-Lossless compression that preserves all raster cell values.

         * PACKBITS-PackBits compression for TIFF files.

         * RLE-Run-length encoding for IMG files.

         * CCITT_GROUP3-Lossless compression for 1-bit data.

         * CCITT_GROUP4-Lossless compression for 1-bit data.

         * CCITT_1D-Lossless compression for 1-bit data.
     compression_quality {Long}:
         Set a value from 1 - 100. Higher values will have better image
         quality, but less compression.
     MAINTAIN_FOLDER {Boolean}:
         Determines the folder structure of the downloaded rasters.

         * MAINTAIN_FOLDER-Replicate the hierarchical folder structure used to
         store the source raster datasets.

         * NO_MAINTAIN_FOLDER-Raster datasets will be downloaded into the
         out_folder as a flat folder structure."""
    ...

@gptooldoc("GenerateRasterFromRasterFunction_management", None)
def GenerateRasterFromRasterFunction(
    raster_function=...,
    out_raster_dataset=...,
    raster_function_arguments=...,
    raster_properties=...,
    format=...,
    process_as_multidimensional=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateRasterFromRasterFunction_management(raster_function, out_raster_dataset, {raster_function_arguments;raster_function_arguments...}, {raster_properties;raster_properties...}, {format}, {process_as_multidimensional})

       Generates a raster dataset from an input raster function or function
       chain.

    INPUTS:
     raster_function (File / String):
         The name of a raster function, raster function JSON object, or
         function chain (in .rft.xml format).
     raster_function_arguments {Value Table}:
         The parameters associated with the function chain. For example, if the
         function chain applies the Hillshade raster function, set the data
         source, azimuth, and altitude.
     raster_properties {Value Table}:
         The output raster dataset key properties, such as the sensor or
         wavelength.
     format {String}:
         The output raster format.The default format will be derived from the
         file extension specified
         in the output_raster_dataset value.

         * TIFF-Tagged Image File Format for raster datasets will be used.

         * Cloud Optimized GeoTIFF-Cloud Optimized GeoTIFF will be used.

         * IMAGINE Image-ERDAS IMAGINE raster data format will be used.

         * Esri Grid-Esri Grid raster dataset format will be used.

         * CRF-Cloud Raster Format will be used.

         * MRF-Meta Raster Format will be used.
     process_as_multidimensional {Boolean}:
         Specifies whether the input mosaic dataset will be processed as a
         multidimensional raster dataset.

         * CURRENT_SLICE-The input will not be processed as a multidimensional
         raster dataset. If the input is multidimensional, only the slice that
         is currently displayed will be processed. This is the default.

         * ALL_SLICES-The input will be processed as a multidimensional raster
         dataset and all slices will be processed to produce a new
         multidimensional raster dataset. Set the format parameter to CRF to
         use this option.

    OUTPUTS:
     out_raster_dataset (Raster Dataset):
         The output raster dataset."""
    ...

@gptooldoc("Mosaic_management", None)
def Mosaic(
    inputs=...,
    target=...,
    mosaic_type=...,
    colormap=...,
    background_value=...,
    nodata_value=...,
    onebit_to_eightbit=...,
    mosaicking_tolerance=...,
    MatchingMethod=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Mosaic_management(inputs;inputs..., target, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod})

       Merges multiple existing raster datasets or mosaic datasets into an
       existing raster dataset.

    INPUTS:
     inputs (Mosaic Dataset / Raster Dataset / Raster Layer):
         The raster datasets to be merged.
     target (Raster Dataset):
         The raster to which the input rasters will be added. This must be an
         existing raster dataset. By default, the target raster is considered
         the first raster in the list of input raster datasets. You can create
         an empty raster using the Create Raster Dataset tool.
     mosaic_type {String}:
         Specifies the method that will be used to mosaic overlapping areas.

         * FIRST-The output cell value of the overlapping areas will be the
         value from the first raster dataset mosaicked into that location.

         * LAST-The output cell value of the overlapping areas will be the
         value from the last raster dataset mosaicked into that location. This
         is the default.

         * BLEND-The output cell value of the overlapping areas will be a
         horizontally weighted calculation of the values of the cells in the
         overlapping area.

         * MEAN-The output cell value of the overlapping areas will be the
         average value of the overlapping cells.

         * MINIMUM-The output cell value of the overlapping areas will be the
         minimum value of the overlapping cells.

         * MAXIMUM-The output cell value of the overlapping areas will be the
         maximum value of the overlapping cells.

         * SUM-The output cell value of the overlapping areas will be the total
         sum of the overlapping cells.
     colormap {String}:
         Specifies the method that will be used to choose which color map from
         the input rasters will be applied to the mosaic output.

         * FIRST-The color map from the first raster dataset in the list will
         be applied to the output raster mosaic. This is the default.

         * LAST-The color map from the last raster dataset in the list will be
         applied to the output raster mosaic.

         * MATCH-All the color maps will be considered when mosaicking. If all
         possible values are already used (for the bit depth), the tool will
         match the value with the closest available color.

         * REJECT-Only the raster datasets that do not have a color map
         associated with them will be mosaicked.
     background_value {Double}:
         Remove the unwanted values created around the raster data. The value
         specified will be distinguished from other valuable data in the raster
         dataset. For example, a value of zero along the raster dataset's
         borders will be distinguished from zero values in the raster
         dataset.The pixel value specified will be set to NoData in the output
         raster
         dataset.For file-based rasters and geodatabase rasters, Ignore
         Background
         Value must be set to the same value as NoData for the background value
         to be ignored. Enterprise geodatabase rasters will work without this
         extra step.
     nodata_value {Double}:
         All the pixels with the specified value will be set to NoData in the
         output raster dataset.
     onebit_to_eightbit {Boolean}:
         Specifies whether the input 1-bit raster dataset will be converted to
         an 8-bit raster dataset. In this conversion, the value 1 in the input
         raster dataset will be changed to 255 in the output raster dataset.
         This is useful when importing a 1-bit raster dataset to a geodatabase.
         One-bit raster datasets have 8-bit pyramid layers when stored in a
         file system, but in a geodatabase, 1-bit raster datasets can only have
         1-bit pyramid layers, which results in a lower-quality display. By
         converting the data to 8 bit in a geodatabase, the pyramid layers are
         built as 8 bit instead of 1 bit, resulting in a proper raster dataset
         in the display.

         * NONE-No conversion will occur. This is the default.

         * OneBitTo8Bit-The input raster will be converted.
     mosaicking_tolerance {Double}:
         When mosaicking occurs, the target and the source pixels do not always
         line up exactly. When there is a misalignment of pixels, you need to
         decide whether to resample or shift the data. The mosaicking tolerance
         controls whether resampling of the pixels will occur or the pixels
         will be shifted.If the difference in pixel alignment (of the incoming
         dataset and the
         target dataset) is greater than the tolerance, resampling will occur.
         If the difference in pixel alignment (of the incoming dataset and the
         target dataset) is less than the tolerance, resampling will not occur
         and a shift will be performed.The unit of tolerance is a pixel with a
         valid value range of 0 to 0.5.
         A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero
         guarantees resampling will occur if there is a misalignment in
         pixels.For example, the source and target pixels have a misalignment
         of 0.25.
         If the mosaicking tolerance is set to 0.2, resampling will occur since
         the pixel misalignment is greater than the tolerance. If the
         mosaicking tolerance is set to 0.3, the pixels will be shifted.
     MatchingMethod {String}:
         Specifies the color matching method that will be applied to the
         rasters.

         * NONE-No color matching method will be applied when mosaicking the
         raster datasets.

         * STATISTIC_MATCHING-Descriptive statistics from the overlapping areas
         will be matched; the transformation will then be applied to the entire
         target dataset.

         * HISTOGRAM_MATCHING-The histogram from the reference overlap area
         will be matched to the source overlap area; the transformation will
         then be applied to the entire target dataset.

         * LINEARCORRELATION_MATCHING-Overlapping pixels will be matched and
         the rest of the source dataset will be interpolated; pixels without a
         one-to-one relationship will use a weighted average."""
    ...

@gptooldoc("MosaicToNewRaster_management", None)
def MosaicToNewRaster(
    input_rasters=...,
    output_location=...,
    raster_dataset_name_with_extension=...,
    coordinate_system_for_the_raster=...,
    pixel_type=...,
    cellsize=...,
    number_of_bands=...,
    mosaic_method=...,
    mosaic_colormap_mode=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MosaicToNewRaster_management(input_rasters;input_rasters..., output_location, raster_dataset_name_with_extension, {coordinate_system_for_the_raster}, {pixel_type}, {cellsize}, number_of_bands, {mosaic_method}, {mosaic_colormap_mode})

       Merges multiple raster datasets into a new raster dataset.

    INPUTS:
     input_rasters (Mosaic Dataset / Raster Dataset / Raster Layer):
         The raster datasets that you want to merge together. The inputs must
         have the same number of bands and same bit depth.
     output_location (Workspace):
         The folder or geodatabase to store the raster.
     raster_dataset_name_with_extension (String):
         The name of the dataset you are creating.When storing the raster
         dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments.
     coordinate_system_for_the_raster {Coordinate System}:
         The coordinate system for the output raster dataset.
     pixel_type {String}:
         The bit depth, or radiometric resolution of the mosaic dataset.If you
         do not set the pixel type, the 8-bit default will be used and
         your output may be incorrect.

         * 1_BIT-The pixel type will be a 1-bit unsigned integer. The values
         can be 0 or 1.

         * 2_BIT-The pixel type will be a 2-bit unsigned integer. The values
         supported can range from 0 to 3.

         * 4_BIT-The pixel type will be a 4-bit unsigned integer. The values
         supported can range from 0 to 15.

         * 8_BIT_UNSIGNED-The pixel type will be an unsigned 8-bit data type.
         The values supported can range from 0 to 255.

         * 8_BIT_SIGNED-The pixel type will be a signed 8-bit data type. The
         values supported can range from -128 to 127.

         * 16_BIT_UNSIGNED-The pixel type will be a 16-bit unsigned data type.
         The values can range from 0 to 65,535.

         * 16_BIT_SIGNED-The pixel type will be a 16-bit signed data type. The
         values can range from -32,768 to 32,767.

         * 32_BIT_UNSIGNED-The pixel type will be a 32-bit unsigned data type.
         The values can range from 0 to 4,294,967,295.

         * 32_BIT_SIGNED-The pixel type will be a 32-bit signed data type. The
         values can range from -2,147,483,648 to 2,147,483,647.

         * 32_BIT_FLOAT-The pixel type will be a 32-bit data type supporting
         decimals.

         * 64_BIT-The pixel type will be a 64-bit data type supporting
         decimals.
     cellsize {Double}:
         The pixel size that will be used for the new raster dataset.
     number_of_bands (Long):
         The number of bands that the output raster will have.
     mosaic_method {String}:
         The method used to mosaic overlapping areas.

         * FIRST-The output cell value of the overlapping areas will be the
         value from the first raster dataset mosaicked into that location.

         * LAST-The output cell value of the overlapping areas will be the
         value from the last raster dataset mosaicked into that location. This
         is the default.

         * BLEND-The output cell value of the overlapping areas will be a
         horizontally weighted calculation of the values of the cells in the
         overlapping area.

         * MEAN-The output cell value of the overlapping areas will be the
         average value of the overlapping cells.

         * MINIMUM-The output cell value of the overlapping areas will be the
         minimum value of the overlapping cells.

         * MAXIMUM-The output cell value of the overlapping areas will be the
         maximum value of the overlapping cells.

         * SUM-The output cell value of the overlapping areas will be the total
         sum of the overlapping cells.
         For more information about each mosaic operator, refer to the Mosaic
         Operator help topic.
     mosaic_colormap_mode {String}:
         Applies when the input raster datasets have a colormap.Specifies the
         method that will be used to choose which color map from
         the input rasters will be applied to the mosaic output.

         * FIRST-The color map from the first raster dataset in the list will
         be applied to the output raster mosaic. This is the default.

         * LAST-The color map from the last raster dataset in the list will be
         applied to the output raster mosaic.

         * MATCH-All the color maps will be considered when mosaicking. If all
         possible values are already used (for the bit depth), the tool will
         match the value with the closest available color.

         * REJECT-Only the raster datasets that do not have a color map
         associated with them will be mosaicked."""
    ...

@gptooldoc("WorkspaceToRasterDataset_management", None)
def WorkspaceToRasterDataset(
    in_workspace=...,
    in_raster_dataset=...,
    include_subdirectories=...,
    mosaic_type=...,
    colormap=...,
    background_value=...,
    nodata_value=...,
    onebit_to_eightbit=...,
    mosaicking_tolerance=...,
    MatchingMethod=...,
    colormap_to_RGB=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """WorkspaceToRasterDataset_management(in_workspace, in_raster_dataset, {include_subdirectories}, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod}, {colormap_to_RGB})

       Merges all of the raster datasets in a folder into one raster dataset.

    INPUTS:
     in_workspace (Workspace):
         The folder containing the raster datasets to merge.
     in_raster_dataset (Raster Dataset):
         An existing raster dataset in which to merge all of the raster
         datasets from the input workspace.
     include_subdirectories {Boolean}:
         Specifies whether subdirectories will be included.

         * NONE-Subdirectories will not be included. This is the default.

         * INCLUDE_SUBDIRECTORIES-All raster datasets in the subdirectories
         will be included when loading.
     mosaic_type {String}:
         Specifies the method that will be used to mosaic overlapping areas.

         * FIRST-The output cell value of the overlapping areas will be the
         value from the first raster dataset mosaicked into that location.

         * LAST-The output cell value of the overlapping areas will be the
         value from the last raster dataset mosaicked into that location. This
         is the default.

         * BLEND-The output cell value of the overlapping areas will be a
         horizontally weighted calculation of the values of the cells in the
         overlapping area.

         * MEAN-The output cell value of the overlapping areas will be the
         average value of the overlapping cells.

         * MINIMUM-The output cell value of the overlapping areas will be the
         minimum value of the overlapping cells.

         * MAXIMUM-The output cell value of the overlapping areas will be the
         maximum value of the overlapping cells.

         * SUM-The output cell value of the overlapping areas will be the total
         sum of the overlapping cells.
     colormap {String}:
         Specifies the method that will be used to choose which color map from
         the input rasters will be applied to the mosaic output.

         * FIRST-The color map from the first raster dataset in the list will
         be applied to the output raster mosaic. This is the default.

         * LAST-The color map from the last raster dataset in the list will be
         applied to the output raster mosaic.

         * MATCH-All the color maps will be considered when mosaicking. If all
         possible values are already used (for the bit depth), the tool will
         match the value with the closest available color.

         * REJECT-Only the raster datasets that do not have a color map
         associated with them will be mosaicked.
     background_value {Double}:
         Remove the unwanted values created around the raster data. The value
         specified will be distinguished from other valuable data in the raster
         dataset. For example, a value of zero along the raster dataset's
         borders will be distinguished from zero values in the raster
         dataset.The pixel value specified will be set to NoData in the output
         raster
         dataset.For file-based rasters the Ignore Background Value must be set
         to the
         same value as NoData in order for the background value to be ignored.
         Enterprise and file geodatabase rasters will work without this extra
         step.
     nodata_value {Double}:
         All the pixels with the specified value will be set to NoData in the
         output raster dataset.
     onebit_to_eightbit {Boolean}:
         Specifies whether the input 1-bit raster dataset will be converted to
         an 8-bit raster dataset. In this conversion, the value 1 in the input
         raster dataset will be changed to 255 in the output raster dataset.
         This is useful when importing a 1-bit raster dataset to a geodatabase.
         One-bit raster datasets have 8-bit pyramid layers when stored in a
         file system, but in a geodatabase, 1-bit raster datasets can only have
         1-bit pyramid layers, which results in a lower-quality display. By
         converting the data to 8 bit in a geodatabase, the pyramid layers are
         built as 8 bit instead of 1 bit, resulting in a proper raster dataset
         in the display.

         * NONE-No conversion will occur. This is the default.

         * OneBitTo8Bit-The input raster will be converted.
     mosaicking_tolerance {Double}:
         When mosaicking occurs, the target and the source pixels do not always
         line up exactly. When there is a misalignment of pixels, you need to
         decide whether to resample or shift the data. The mosaicking tolerance
         controls whether resampling of the pixels will occur or the pixels
         will be shifted.If the difference in pixel alignment (of the incoming
         dataset and the
         target dataset) is greater than the tolerance, resampling will occur.
         If the difference in pixel alignment (of the incoming dataset and the
         target dataset) is less than the tolerance, resampling will not occur
         and a shift will be performed.The unit of tolerance is a pixel with a
         valid value range of 0 to 0.5.
         A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero
         guarantees resampling will occur if there is a misalignment in
         pixels.For example, the source and target pixels have a misalignment
         of 0.25.
         If the mosaicking tolerance is set to 0.2, resampling will occur since
         the pixel misalignment is greater than the tolerance. If the
         mosaicking tolerance is set to 0.3, the pixels will be shifted.
     MatchingMethod {String}:
         The color matching method to apply to the rasters.

         * NONE-This option will not use the color matching operation when
         mosaicking your raster datasets.

         * STATISTIC_MATCHING-This method will use descriptive statistics from
         the overlapping areas; the transformation will then be applied to the
         entire target dataset.

         * HISTOGRAM_MATCHING-This method will match the histogram from the
         reference overlap area to the source overlap area; the transformation
         will then be applied to the entire target dataset.

         * LINEARCORRELATION_MATCHING-This method will match overlapping pixels
         and then interpolated the rest of the source dataset; pixels without a
         one-to-one relationship will use a weighted average.
     colormap_to_RGB {Boolean}:
         Specifies whether the input raster dataset will be converted to a
         three-band output raster dataset if the input raster dataset includes
         a color map. This is useful when mosaicking rasters with different
         color maps.

         * NONE-No conversion will occur. This is the default.

         * ColormapToRGB-The input dataset will be converted."""
    ...

@gptooldoc("Clip_management", None)
def Clip(
    in_raster=...,
    rectangle=...,
    out_raster=...,
    in_template_dataset=...,
    nodata_value=...,
    clipping_geometry=...,
    maintain_clipping_extent=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Clip_management(in_raster, rectangle, out_raster, {in_template_dataset}, {nodata_value}, {clipping_geometry}, {maintain_clipping_extent})

       Cuts out a portion of a raster dataset, mosaic dataset, or image
       service layer.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Dataset / Raster Layer):
         The raster dataset, mosaic dataset, or image service to be clipped.
     rectangle (Envelope / Feature Class / Feature Layer):
         The four coordinates that define the extent of the bounding box that
         will be used to clip the raster. Coordinates are expressed in the
         order of x-min, y-min, x-max, y-max.If the in_template_dataset
         parameter is set, it will automatically set
         this parameter. If the in_template_dataset parameter is a feature
         layer, the clipping extent is extracted from the bounding box. In this
         case, rectangle can be left empty as long as the in_template_dataset
         parameter value is specified.If both the rectangle and
         in_template_dataset parameters are set, the
         rectangle parameter value will be used.If the value specified is not
         aligned with the input raster dataset,
         the tool verifies that the proper alignment is used. This may cause
         the output to have a slightly different extent than specified.
     in_template_dataset {Feature Layer / Raster Layer}:
         A raster dataset or feature class that will be used as the extent. The
         clip output includes pixels that intersect the minimum bounding
         rectangle.If a feature class is used as the output extent and you want
         to clip
         the raster based on the polygon features, set the clipping_geometry
         parameter to ClippingGeometry. This option may promote the pixel depth
         of the output. Ensure that the output format can support the proper
         pixel depth.
     nodata_value {String}:
         The value for pixels to be considered as NoData.
     clipping_geometry {Boolean}:
         Specifies whether the data will be clipped to the minimum bounding
         rectangle or to the geometry of the feature class.

         * NONE-The minimum bounding rectangle will be used to clip the data.
         This is the default.

         * ClippingGeometry-The geometry of the specified feature class will be
         used to clip the data. The pixel depth of the output may be increased;
         ensure that the output format can support the proper pixel depth.
     maintain_clipping_extent {Boolean}:
         Specifies the extent that will be used in the clipping output.

         * MAINTAIN_EXTENT-The number of columns and rows will be adjusted and
         the pixels will be resampled to exactly match the clipping extent
         specified.

         * NO_MAINTAIN_EXTENT-The cell alignment of the input raster will be
         maintained and the output extent will be adjusted accordingly.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format of the dataset being created. Ensure
         that it can support the necessary bit depth.When storing the raster
         dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("CompositeBands_management", None)
def CompositeBands(
    in_rasters=..., out_raster=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CompositeBands_management(in_rasters;in_rasters..., out_raster)

       Creates a single raster dataset from multiple bands.

    INPUTS:
     in_rasters (Mosaic Dataset / Mosaic Layer / Raster Dataset / Raster Layer):
         The raster datasets that you want to use as the bands.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location and format for the raster dataset you are creating.
         Make sure that it can support the necessary bit-depth.When storing the
         raster dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("ComputePansharpenWeights_management", None)
def ComputePansharpenWeights(
    in_raster=..., in_panchromatic_image=..., band_indexes=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ComputePansharpenWeights_management(in_raster, in_panchromatic_image, {band_indexes})

       Calculates an optimal set of pan sharpened weights for new or custom
       sensor data.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Dataset / Raster Layer):
         A multispectral raster that has a panchromatic band.
     in_panchromatic_image (Raster Layer):
         The panchromatic band associated with the multispectral raster.
     band_indexes {String}:
         The band order for the pan sharpened weights.If a raster product is
         used as the in_raster parameter, the band order
         within the raster product template will be used."""
    ...

@gptooldoc("CreateColorComposite_management", None)
def CreateColorComposite(
    in_raster=...,
    out_raster=...,
    method=...,
    red_expression=...,
    green_expression=...,
    blue_expression=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateColorComposite_management(in_raster, out_raster, method, red_expression, green_expression, blue_expression)

       Creates a three-band raster dataset from a multiband raster dataset.

    INPUTS:
     in_raster (Raster Dataset / Raster Layer):
         The input multiband raster data.
     method (String):
         Specifies the method that will be used to extract bands.

         * BAND_NAMES-The band name representing the wavelength interval on the
         electromagnetic spectrum (such as Red, Near Infrared, or Thermal
         Infrared) or the polarization (such as VH, VV, HH, or HV) will be
         used. This is the default.

         * BAND_IDS-The band number (such as B1, B2, or B3) will be used.
     red_expression (String):
         The calculation that will be assigned to the first band.A band name,
         band ID, or an algebraic expression using the bands.The supported
         operators are unary: plus (+), minus (-), times (*), and
         divide (/).
     green_expression (String):
         The calculation that will be assigned to the second band.A band name,
         band ID, or an algebraic expression using the bands.The supported
         operators are unary: plus (+), minus (-), times (*), and
         divide (/).
     blue_expression (String):
         The calculation that will be assigned to the third band.A band name,
         band ID, or an algebraic expression using the bands.The supported
         operators are unary: plus (+), minus (-), times (*), and
         divide (/).

    OUTPUTS:
     out_raster (Raster Dataset):
         The output three-band composite raster."""
    ...

@gptooldoc("CreateOrthoCorrectedRasterDataset_management", None)
def CreateOrthoCorrectedRasterDataset(
    in_raster=...,
    out_raster_dataset=...,
    Ortho_type=...,
    constant_elevation=...,
    in_DEM_raster=...,
    ZFactor=...,
    ZOffset=...,
    Geoid=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateOrthoCorrectedRasterDataset_management(in_raster, out_raster_dataset, Ortho_type, constant_elevation, {in_DEM_raster}, {ZFactor}, {ZOffset}, {Geoid})

       Creates an orthocorrected raster dataset by incorporating elevation
       data and the rational polynomial coefficients (RPC) associated with
       satellite data to accurately line up imagery.

    INPUTS:
     in_raster (Raster Dataset / Mosaic Dataset / Mosaic Layer / Raster Layer):
         The raster dataset to orthorectify. The raster must have RPCs in its
         metadata.
     Ortho_type (String):
         The DEM or specified value that represents the average elevation
         across the image.

         * CONSTANT_ELEVATION-A specified elevation value will be used.

         * DEM-A specified digital elevation model raster will be used.
     constant_elevation (Double):
         The constant elevation value to be used when the Ortho_type parameter
         is CONSTANT_ELEVATION.If a DEM is used in the orthocorrection process,
         this value is not
         used.
     in_DEM_raster {Raster Dataset / Mosaic Dataset / Mosaic Layer / Raster Layer / Image Service}:
         The DEM raster to be used for orthorectification when the Ortho_type
         parameter is DEM.
     ZFactor {Double}:
         The scaling factor used to convert the elevation values in the DEM.If
         the vertical units are meters, set the parameter to 1. If the
         vertical units are feet, set the parameter to 0.3048. If any other
         vertical units are used, use this parameter to scale the units to
         meters.
     ZOffset {Double}:
         The base value to be added to the elevation value in the DEM. This can
         be used to offset elevation values that do not start at sea level.
     Geoid {Boolean}:
         Specifies whether the geoid correction required by RPCs that reference
         ellipsoidal heights will be made. Most elevation datasets are
         referenced to sea level orthometric heights, so this correction is
         required in these cases to convert to ellipsoidal heights.

         * NONE-No geoid correction will be made. Use NONE only if the DEM is
         already expressed in ellipsoidal heights.

         * GEOID-A geoid correction will be made to convert orthometric heights
         to ellipsoidal heights (based on EGM96 geoid).

    OUTPUTS:
     out_raster_dataset (Raster Dataset):
         The name, location, and format of the dataset to be created.When
         storing the raster dataset in a file format, specify the file
         extension as follows:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, or a geodatabase, you can specify a Compression Type
         value and a Compression Quality value in the geoprocessing
         environments."""
    ...

@gptooldoc("CreatePansharpenedRasterDataset_management", None)
def CreatePansharpenedRasterDataset(
    in_raster=...,
    red_channel=...,
    green_channel=...,
    blue_channel=...,
    infrared_channel=...,
    out_raster_dataset=...,
    in_panchromatic_image=...,
    pansharpening_type=...,
    red_weight=...,
    green_weight=...,
    blue_weight=...,
    infrared_weight=...,
    sensor=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreatePansharpenedRasterDataset_management(in_raster, red_channel, green_channel, blue_channel, {infrared_channel}, out_raster_dataset, in_panchromatic_image, pansharpening_type, {red_weight}, {green_weight}, {blue_weight}, {infrared_weight}, {sensor})

       Combines a high-resolution panchromatic raster dataset with a lower-
       resolution multiband raster dataset to create a high-resolution
       multiband raster dataset for visual analysis.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Dataset / Raster Layer):
         The raster dataset that will be pan sharpened.
     red_channel (Long):
         The input raster band that will display with the red color channel.
     green_channel (Long):
         The input raster band that will display with the green color channel.
     blue_channel (Long):
         The input raster band that will display with the blue color channel.
     infrared_channel {Long}:
         The input raster band that will display with the infrared color
         channel.
     in_panchromatic_image (Raster Layer):
         The higher-resolution panchromatic image.
     pansharpening_type (String):
         Specifies the algorithm that will be used to combine the panchromatic
         and multispectral bands.

         * IHS-Intensity, Hue, and Saturation color space will be used.

         * BROVEY-The Brovey algorithm based on spectral modeling will be used.

         * Esri-The Esri algorithm based on spectral modeling will be used.

         * SIMPLE_MEAN-The averaged value between the red, green, and blue
         values and the panchromatic pixel value will be used.

         * Gram-Schmidt-The Gram-Schmidt spectral-sharpening algorithm to
         sharpen multispectral data will be used.
     red_weight {Double}:
         A value from 0 to 1 that will be used to weight the red band.
     green_weight {Double}:
         A value from 0 to 1 that will be used to weight the green band.
     blue_weight {Double}:
         A value from 0 to 1 that will be used to weight the blue band.
     infrared_weight {Double}:
         A value from 0 to 1 that will be used to weight the infrared band.
     sensor {String}:
         Specifies the sensor of the multiband raster input.You can specify the
         sensor when the pansharpening_type parameter is
         set to Gram-Schmidt. Specifying the sensor will set appropriate band
         weights.

         * UNKNOWN-The sensor is unknown or unlisted.

         * BlackSky-The sensor is a BlackSky satellite sensor.

         * DubaiSat-2-The sensor is a DubaiSat-2 satellite sensor.

         * GeoEye-1-The sensor is a GeoEye-1 and OrbView-3 satellite sensor.

         * GF-1 PMS-The sensor is a Gao Fen satellite 1, Panchromatic and
         Multispectral CCD Camera sensor.

         * GF-2 PMS-The sensor is a Gao Fen 2 satellite, Panchromatic and
         Multispectral CCD Camera sensor.

         * IKONOS-The sensor is an IKONOS satellite sensor.

         * Jilin-1-The sensor is a Jilin-1 satellite sensor.

         * KOMPSAT-2-The sensor is a KOMPSAT-2 satellite sensor.

         * KOMPSAT-3-The sensor is a KOMPSAT-3 satellite sensor.

         * Landsat 1-5 MSS-The sensor is a Landsat MSS satellite sensor.

         * Landsat 7 ETM+-The sensor is a Landsat 7 satellite sensor.

         * Landsat 8-The sensor is a Landsat 8 satellite sensor.

         * Landsat 9-The sensor is a Landsat 9 satellite sensor.

         * Pleiades-1-The sensor is a Pléiades satellite sensor.

         * Pleiades Neo-The sensor is a Pléiades Neo satellite sensor.

         * QuickBird-The sensor is a QuickBird satellite sensor.

         * SkySat-The sensor is a SkySat-C satellite sensor.

         * SPOT 5-The sensor is a SPOT 5 satellite sensor.

         * SPOT 6-The sensor is a SPOT 6 satellite sensor.

         * SPOT 7-The sensor is a SPOT 7 satellite sensor.

         * SuperView-1-The sensor is a SuperView-1 satellite sensor.

         * TH-01-The sensor is a Tian Hui 1 satellite sensor.

         * UltraCam-The sensor is an UltraCam aerial sensor.

         * WorldView-2-The sensor is a WorldView-2 satellite sensor.

         * WorldView-3-The sensor is a WorldView-3 satellite sensor.

         * WorldView-4-The sensor is a WorldView-4 satellite sensor.

         * ZY1-02C PMS-The sensor is a Ziyuan High Panchromatic Multispectral
         Sensor.

         * ZY3-CRESDA-The sensor is a Ziyuan CRESDA satellite sensor.

         * ZY3-SASMAC-The sensor is a Ziyuan SASMAC satellite sensor.

    OUTPUTS:
     out_raster_dataset (Raster Dataset):
         The name, location, and format of the raster dataset that will be
         created.When storing the raster dataset in a file format, specify the
         file
         extension as follows:        When storing a raster dataset in a
         geodatabase, do not add a
         file extension to the name of the raster dataset. When storing the
         raster dataset in a file format, you must specify the file extension:

         * .bil for Esri BIL

         * .bip for Esri BIP

         * .bmp for BMP

         * .bsq for Esri BSQ

         * .dat for ENVI DAT

         * .gif for GIF

         * .img for ERDAS IMAGINE

         * .jpg for JPEG

         * .jp2 for JPEG 2000

         * .png for PNG

         * .tif for TIFF

         * .mrf for MRF

         * .crf for CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing a raster
         dataset to a JPEG format file, a JPEG 2000
         format file, a TIFF format file, or a geodatabase, you can specify
         Compression Type and Compression Quality values in the geoprocessing
         environments."""
    ...

@gptooldoc("ExtractSubDataset_management", None)
def ExtractSubDataset(
    in_raster=..., out_raster=..., subdataset_index=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExtractSubDataset_management(in_raster, out_raster, {subdataset_index;subdataset_index...})

       Creates a new raster dataset from a selection of an HDF or NITF
       dataset.

    INPUTS:
     in_raster (Raster Layer):
         The HDF or NITF dataset that has the layers you want to extract.
     subdataset_index {Value Table}:
         The subdatasets that you want to extract.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format for the dataset you are creating.When
         storing the raster dataset in a file format, you need to specify
         the file extension:

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE file

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * no extension-Esri GRID
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset.When storing your raster
         dataset to a JPEG file, a JPEG 2000 file, or
         a geodatabase, you can specify a Compression type and Compression
         Quality within the Environment Settings."""
    ...

@gptooldoc("GenerateTableFromRasterFunction_management", None)
def GenerateTableFromRasterFunction(
    raster_function=..., out_table=..., raster_function_arguments=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateTableFromRasterFunction_management(raster_function, out_table, {raster_function_arguments;raster_function_arguments...})

       Converts a raster function dataset to a table or feature class. The
       input raster function should be a raster function designed to output a
       table or feature class.

    INPUTS:
     raster_function (File / String):
         The function template or function JSON object that outputs a table or
         feature class.
     raster_function_arguments {Value Table}:
         The function arguments and their values to be set. Each raster
         function has its own arguments and values, which are listed in the
         dialog of the tool.

    OUTPUTS:
     out_table (Table):
         The path, file name, and type (extension) of the output table or
         feature class."""
    ...

@gptooldoc("RasterToDTED_management", None)
def RasterToDTED(
    in_raster=..., out_folder=..., dted_level=..., resampling_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RasterToDTED_management(in_raster, out_folder, dted_level, {resampling_type})

       Splits a raster dataset into separate files based on the DTED tiling
       structure.

    INPUTS:
     in_raster (Raster Layer):
         Select a single band raster dataset that represents elevation.
     out_folder (Folder):
         Select a destination where the folder structure and DTED files will be
         created.
     dted_level (String):
         Select an appropriate level based on the resolution of your elevation
         data.

         * DTED_0-900 m

         * DTED_1-90 m

         * DTED_2-30 m
     resampling_type {String}:
         Choose an appropriate technique based on the type of data you have.

         * NEAREST-The fastest resampling method, and it minimizes changes to
         pixel values. Suitable for discrete data, such as land cover.

         * BILINEAR-Calculates the value of each pixel by averaging (weighted
         for distance) the values of the surrounding 4 pixels. Suitable for
         continuous data.

         * CUBIC-Calculates the value of each pixel by fitting a smooth curve
         based on the surrounding 16 pixels. Produces the smoothest image, but
         can create values outside of the range found in the source data.
         Suitable for continuous data."""
    ...

@gptooldoc("Resample_management", None)
def Resample(
    in_raster=..., out_raster=..., cell_size=..., resampling_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Resample_management(in_raster, out_raster, {cell_size}, {resampling_type})

       Changes the spatial resolution of a raster dataset and sets rules for
       aggregating or interpolating values across the new pixel sizes.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Dataset / Raster Layer):
         The raster dataset with the spatial resolution to be changed.
     cell_size {Cell Size XY}:
         The cell size of the new raster using an existing raster dataset or by
         specifying its width (x) and height (y). You can specify the
         cell size in the following ways:

         * Use a single number specifying a square cell size.

         * Use two numbers that specify the x and y cell size, which is space
         delimited.

         * Use the path of a raster dataset from which the square cell size
         will be imported.
     resampling_type {String}:
         Specifies the resampling technique to be used.

         * NEAREST-The nearest neighbor technique will be used. It minimizes
         changes to pixel values since no new values are created and is the
         fastest resampling technique. It is suitable for discrete data, such
         as land cover.

         * BILINEAR-The bilinear interpolation technique will be used. It
         calculates the value of each pixel by averaging (weighted for
         distance) the values of the surrounding four pixels. It is suitable
         for continuous data.

         * CUBIC-The cubic convolution technique will be used. It calculates
         the value of each pixel by fitting a smooth curve based on the
         surrounding 16 pixels. This produces the smoothest image but can
         create values outside of the range found in the source data. It is
         suitable for continuous data.

         * MAJORITY-The majority resampling technique will be used. It
         determines the value of each pixel based on the most popular value in
         a 4 by 4 window. It is suitable for discrete data.

    OUTPUTS:
     out_raster (Raster Dataset):
         The name, location, and format of the dataset being created.

         * .bil-Esri BIL

         * .bip-Esri BIP

         * .bmp-BMP

         * .bsq-Esri BSQ

         * .dat-ENVI DAT

         * .gif-GIF

         * .img-ERDAS IMAGINE

         * .jpg-JPEG

         * .jp2-JPEG 2000

         * .png-PNG

         * .tif-TIFF

         * .mrf-MRF

         * .crf-CRF

         * No extension for Esri Grid
         When storing a raster dataset in a geodatabase, do not add a file
         extension to the name of the raster dataset. When storing a raster
         dataset to JPEG, JPEG 2000, or TIFF format, or in a geodatabase, you
         can specify a compression type and compression quality."""
    ...

@gptooldoc("SplitRaster_management", None)
def SplitRaster(
    in_raster=...,
    out_folder=...,
    out_base_name=...,
    split_method=...,
    format=...,
    resampling_type=...,
    num_rasters=...,
    tile_size=...,
    overlap=...,
    units=...,
    cell_size=...,
    origin=...,
    split_polygon_feature_class=...,
    clip_type=...,
    template_extent=...,
    nodata_value=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SplitRaster_management(in_raster, out_folder, out_base_name, split_method, format, {resampling_type}, {num_rasters}, {tile_size}, {overlap}, {units}, {cell_size}, {origin}, {split_polygon_feature_class}, {clip_type}, {template_extent}, {nodata_value})

       Divides a raster dataset into smaller pieces, by tiles or features
       from a polygon.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Layer):
         The raster to split.
     out_folder (Folder):
         The destination for the new raster datasets.
     out_base_name (String):
         The prefix for each of the raster datasets you will create. A number
         will be appended to each prefix, starting with 0.
     split_method (String):
         Determines how to split the raster dataset.

         * SIZE_OF_TILE-Specify the width and height of the tile.

         * NUMBER_OF_TILES-Specify the number of raster tiles to create by
         breaking the dataset into a number of columns and rows.

         * POLYGON_FEATURES-Use the individual polygon geometries in a feature
         class to split the raster.
     format (String):
         The format for the output raster datasets.

         * TIFF-Tagged Image File Format. This is the default.

         * BMP-Microsoft Bitmap.

         * ENVI-ENVI DAT.

         * Esri BIL-Esri Band Interleaved by Line.

         * Esri BIP-Esri Band Interleaved by Pixel.

         * Esri BSQ-Esri Band Sequential.

         * GIF-Graphic Interchange Format.

         * GRID-Esri Grid.

         * IMAGINE IMAGE-ERDAS IMAGINE.

         * JP2-JPEG 2000.

         * JPEG-Joint Photographic Experts Group.

         * PNG-Portable Network Graphics.
     resampling_type {String}:
         Choose an appropriate technique based on the type of data you have.

         * NEAREST-The fastest resampling method, and it minimizes changes to
         pixel values. Suitable for discrete data, such as land cover.

         * BILINEAR-Calculates the value of each pixel by averaging (weighted
         for distance) the values of the surrounding 4 pixels. Suitable for
         continuous data.

         * CUBIC-Calculates the value of each pixel by fitting a smooth curve
         based on the surrounding 16 pixels. Produces the smoothest image, but
         can create values outside of the range found in the source data.
         Suitable for continuous data.
     num_rasters {Point}:
         The number of columns (x) and rows (y) to split the raster dataset
         into. This is a point whose X and Y coordinates define number of rows
         and columns. The X coordinate is the number of columns and the Y
         coordinate is the number of rows.
     tile_size {Point}:
         The x and y dimensions of the output tiles. The default unit of
         measurement is in pixels. You can change this with the units
         parameter. This is a point whose X and Y coordinates define the
         dimensions of output tiles. The X coordinate is the horizontal
         dimension of the output and the Y coordinate is the vertical dimension
         of the output.
     overlap {Double}:
         The tiles do not have to line up perfectly; set the amount of overlap
         between tiles with this parameter. The default unit of measurement is
         in pixels. You can change this with the units parameter.
     units {String}:
         Set the units of measurement for the tile_size and the overlap
         parameters.

         * PIXELS-The unit is in pixels. This is the default.

         * METERS-The unit is in meters.

         * FEET-The unit is in feet.

         * DEGREES-The unit is in decimal degrees.

         * MILES-The unit is in miles.

         * KILOMETERS-The unit is in kilometers.
     cell_size {Point}:
         The spatial resolution of the output raster. If left blank, the output
         cell size will match the input raster. When you change the cell size
         values, the tile size is reset to the image size and the tile count is
         reset to 1.
     origin {Point}:
         Change the coordinates for the lower left origin point, where the
         tiling scheme will begin. If left blank, the lower left origin would
         be the same as the input raster.
     split_polygon_feature_class {Feature Layer}:
         A feature class that will be used to split the raster dataset.
     clip_type {String}:
         Limits the extent of your raster dataset before you split it.

         * NONE-Use the full extent of the input raster dataset.

         * EXTENT-Specify bounding box as your clipping boundary.

         * FEATURE_CLASS-Specify a feature class to clip the extent.
     template_extent {Extent}:
         An extent or a dataset used to define the clipping boundary. The
         dataset can be a raster or feature class.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     nodata_value {String}:
         All the pixels with the specified value will be set to NoData in the
         output raster dataset."""
    ...

@gptooldoc("AddColormap_management", None)
def AddColormap(
    in_raster=..., in_template_raster=..., input_CLR_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddColormap_management(in_raster, {in_template_raster}, {input_CLR_file})

       Adds a new color map or replaces an existing color map on a raster
       dataset.

    INPUTS:
     in_raster (Raster Layer):
         The raster dataset to add or replace a color map.
     in_template_raster {Raster Layer}:
         A raster dataset that has a color map that you want to apply to the
         input raster dataset. If this is entered the input_CLR_file parameter
         is ignored.
     input_CLR_file {File}:
         Specify a .clr or .act file to use as the color map."""
    ...

@gptooldoc("BatchBuildPyramids_management", None)
def BatchBuildPyramids(
    Input_Raster_Datasets=...,
    Pyramid_levels=...,
    Skip_first_level=...,
    Pyramid_resampling_technique=...,
    Pyramid_compression_type=...,
    Compression_quality=...,
    Skip_Existing=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BatchBuildPyramids_management(Input_Raster_Datasets;Input_Raster_Datasets..., {Pyramid_levels}, {Skip_first_level}, {Pyramid_resampling_technique}, {Pyramid_compression_type}, {Compression_quality}, {Skip_Existing})

       Builds pyramids for multiple raster datasets.

    INPUTS:
     Input_Raster_Datasets (Raster Dataset):
         The raster datasets for which raster pyramids will be built.Each input
         should have more than 1,024 rows and 1,024 columns.
     Pyramid_levels {Long}:
         The number of reduced-resolution dataset layers that will be built.
         The default value is -1, which will build full pyramids. A value of 0
         will result in no pyramid levels.
     Skip_first_level {Boolean}:
         Specifies whether the first pyramid level will be skipped. Skipping
         the first level will take up slightly less disk space, but it will
         slow down performance at these scales.

         * NONE-The first pyramid level will not be skipped; it will be built.
         This is the default.

         * SKIP_FIRST-The first pyramid level will be skipped; it will not be
         built.
     Pyramid_resampling_technique {String}:
         Specifies the resampling technique that will be used to build the
         pyramids.

         * NEAREST-The new value of a cell will be based on the closest cell
         when resampling. This is the default.

         * BILINEAR-The new value of a cell will be based on a weighted
         distance average of the four nearest input cell centers.

         * CUBIC-The new value of a cell will be determined by fitting a smooth
         curve through the 16 nearest input cell centers.
     Pyramid_compression_type {String}:
         Specifies the compression type that will be used when building the
         pyramids.

         * DEFAULT-If the source data is compressed using a wavelet
         compression, pyramids will be built with the JPEG compression type;
         otherwise, LZ77 will be used. This is the default.

         * LZ77-The LZ77 compression algorithm will be used to build the
         pyramids. LZ77 can be used for any data type.

         * JPEG-The JPEG compression algorithm will be used to build the
         pyramids. Only data that adheres to the JPEG compression specification
         can use this compression type. If JPEG is chosen, you can then set the
         compression quality.

         * NONE-No compression will be used when building pyramids.
     Compression_quality {Long}:
         The compression quality that will be used when pyramids are built with
         the JPEG compression type. The value must be between 0 and 100. The
         values closer to 100 will produce a higher-quality image, but the
         compression ratio will be lower.
     Skip_Existing {Boolean}:
         Specifies whether pyramids will be built only if they do not exist or
         built even if they exist.

         * OVERWRITE-Pyramids will be built even if they already exist;
         existing pyramids will be overwritten. This is the default.

         * SKIP_EXISTING-Pyramids will only be built if they do not exist;
         existing pyramids will be skipped."""
    ...

@gptooldoc("BatchCalculateStatistics_management", None)
def BatchCalculateStatistics(
    Input_Raster_Datasets=...,
    Number_of_columns_to_skip=...,
    Number_of_rows_to_skip=...,
    Ignore_values=...,
    Skip_Existing=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BatchCalculateStatistics_management(Input_Raster_Datasets;Input_Raster_Datasets..., {Number_of_columns_to_skip}, {Number_of_rows_to_skip}, {Ignore_values;Ignore_values...}, {Skip_Existing})

       Calculates statistics for multiple raster datasets.

    INPUTS:
     Input_Raster_Datasets (Raster Dataset):
         The input raster datasets.
     Number_of_columns_to_skip {Long}:
         The number of horizontal pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of columns in the raster. The default is 1 or the last skip
         factor used.The skip factors for raster datasets stored in a file
         geodatabase or
         an enterprise geodatabase are different. First, if the x and y skip
         factors are different, the smaller skip factor will be used for both
         the x and y skip factors. Second, the skip factor is related to the
         pyramid level that most closely fits the skip factor chosen. If the
         skip factor value is not equal to the number of pixels in a pyramid
         layer, the number is rounded down to the next pyramid level, and those
         statistics are used.
     Number_of_rows_to_skip {Long}:
         The number of vertical pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of rows in the raster. The default is 1 or the last y skip
         factor used.The skip factors for raster datasets stored in a file
         geodatabase or
         an enterprise geodatabase are different. First, if the x and y skip
         factors are different, the smaller skip factor will be used for both
         the x and y skip factors. Second, the skip factor is related to the
         pyramid level that most closely fits the skip factor chosen. If the
         skip factor value is not equal to the number of pixels in a pyramid
         layer, the number is rounded down to the next pyramid level, and those
         statistics are used.
     Ignore_values {Double}:
         The pixel values that are not to be included in the statistics
         calculation.The default is no value.
     Skip_Existing {Boolean}:
         Specifies whether statistics will be calculated only when they are
         missing or will be regenerated even if they exist.

         * OVERWRITE-Statistics will be calculated even if they already exist,
         and existing statistics will be overwritten. This is the default.

         * SKIP_EXISTING-Statistics will only be calculated if they do not
         already exist."""
    ...

@gptooldoc("BuildPyramids_management", None)
def BuildPyramids(
    in_raster_dataset=...,
    pyramid_level=...,
    SKIP_FIRST=...,
    resample_technique=...,
    compression_type=...,
    compression_quality=...,
    skip_existing=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildPyramids_management(in_raster_dataset, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {compression_type}, {compression_quality}, {skip_existing})

       Builds raster pyramids for your raster dataset.

    INPUTS:
     in_raster_dataset (Raster Dataset / Raster Layer):
         The raster dataset for which you want to build pyramids.The input
         should have more than 1,024 rows and 1,024 columns.
     pyramid_level {Long}:
         The number of reduced-resolution dataset layers that will be built.
         The default value is -1, which will build full pyramids. A value of 0
         will result in no pyramid levels.To delete pyramids, set the number of
         levels to 0.The maximum number of pyramid levels you can specify is
         29. Any value
         that is 30 or higher will revert to a value of -1, which will create a
         full set of pyramids.
     SKIP_FIRST {Boolean}:
         Specifies whether the first pyramid level will be skipped. Skipping
         the first level will take up slightly less disk space, but it will
         slow down performance at these scales.

         * NONE-The first pyramid level will not be skipped; it will be built.
         This is the default.

         * SKIP_FIRST-The first pyramid level will be skipped; it will not be
         built.
     resample_technique {String}:
         Specifies the resampling technique that will be used to build the
         pyramids.

         * NEAREST-The value of the closest pixel will be used to assign a
         value to the output pixel when resampling. This is the default.

         * BILINEAR-The new value of a pixel will be based on a weighted
         distance average of the four nearest input pixel centers.

         * CUBIC-The new value of a pixel will be based on fitting a smooth
         curve through the 16 nearest input pixel centers.
     compression_type {String}:
         Specifies the compression type that will be used when building the
         raster pyramids.

         * DEFAULT-If the source data is compressed using a wavelet
         compression, it will build pyramids with the JPEG compression type;
         otherwise, LZ77 will be used. This is the default compression method.

         * LZ77-The LZ77 compression algorithm will be used to build the
         pyramids. LZ77 can be used for any data type.

         * JPEG-The JPEG compression algorithm will be used to build pyramids.
         Only data that adheres to the JPEG compression specification can use
         this compression type. If JPEG is chosen, you can then set the
         compression quality.

         * JPEG_YCbCr-A lossy compression using the luma (Y) and chroma (Cb and
         Cr) color space components will be used to build pyramids.

         * NONE-No compression will be used when building pyramids.
     compression_quality {Long}:
         The compression quality that will be used when pyramids are built with
         the JPEG compression type. The value must be between 0 and 100. The
         values closer to 100 will produce a higher-quality image, but the
         compression ratio will be lower.
     skip_existing {Boolean}:
         Specifies whether pyramids will be built only when they are missing or
         will be regenerated even if they exist.

         * OVERWRITE-Pyramids will be built even if they already exist, and
         existing pyramids will be overwritten. This is the default.

         * SKIP_EXISTING-Pyramids will only be built if they do not already
         exist."""
    ...

@gptooldoc("BuildPyramidsandStatistics_management", None)
def BuildPyramidsandStatistics(
    in_workspace=...,
    include_subdirectories=...,
    build_pyramids=...,
    calculate_statistics=...,
    BUILD_ON_SOURCE=...,
    block_field=...,
    estimate_statistics=...,
    x_skip_factor=...,
    y_skip_factor=...,
    ignore_values=...,
    pyramid_level=...,
    SKIP_FIRST=...,
    resample_technique=...,
    compression_type=...,
    compression_quality=...,
    skip_existing=...,
    where_clause=...,
    sips_mode=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildPyramidsandStatistics_management(in_workspace, {include_subdirectories}, {build_pyramids}, {calculate_statistics}, {BUILD_ON_SOURCE}, {block_field}, {estimate_statistics}, {x_skip_factor}, {y_skip_factor}, {ignore_values;ignore_values...}, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {compression_type}, {compression_quality}, {skip_existing}, {where_clause}, {sips_mode})

       Traverses a folder structure, building pyramids and calculating
       statistics for all the raster datasets it contains. It can also build
       pyramids and calculate statistics for all the items in a mosaic
       dataset.

    INPUTS:
     in_workspace (Text File / Workspace / Raster Layer / Mosaic Layer):
         The workspace that contains all the raster datasets or mosaic datasets
         to be processed.If the workspace includes a mosaic dataset, only the
         statistics
         associated with the mosaic dataset will be included. The statistics
         associated with the items within the mosaic dataset will not be
         included.
     include_subdirectories {Boolean}:
         Specifies whether subdirectories will be included.

         * NONE-Does not include subdirectories.

         * INCLUDE_SUBDIRECTORIES-Includes all the raster datasets within the
         subdirectories when loading. This is the default.
         If the workspace includes a mosaic dataset, only the statistics
         associated with mosaic dataset will be included. The statistics
         associated with the items within the mosaic dataset will not be
         included.
     build_pyramids {Boolean}:
         Specify whether to build pyramids.

         * NONE-Does not build pyramids.

         * BUILD_PYRAMIDS-Builds pyramids. This is the default.
     calculate_statistics {Boolean}:
         Specify whether to calculate statistics.

         * NONE-Does not calculate statistics.

         * CALCULATE_STATISTICS-Calculates statistics. This is the default.
     BUILD_ON_SOURCE {Boolean}:
         Specify whether to calculate statistics on the source raster datasets,
         or calculate statistics on the raster items in a mosaic dataset. This
         option only applies to mosaic datasets.

         * NONE-Statistics will be calculated for each raster item in the
         mosaic dataset (on each row in the attribute table). Any functions
         added to the raster item will be applied before generating the
         statistics. This is the default.

         * BUILD_ON_SOURCE-Calculates statistics on the source data of the
         mosaic dataset.
     block_field {String}:
         The name of the field within a mosaic dataset's attribute table used
         to identify items that should be considered one item when performing
         some calculations and operations.
     estimate_statistics {Boolean}:
         Specify whether to calculate statistics for the mosaic dataset (not
         the rasters within it). The statistics are derived from the existing
         statistics that have been calculated for each raster in the mosaic
         dataset.

         * NONE-Statistics are not calculated for the mosaic dataset. This is
         the default.

         * ESTIMATE_STATISTICS-Statistics will be calculated for the mosaic
         dataset.
     x_skip_factor {Long}:
         The number of horizontal pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of columns in the raster. The default is 1 or the last skip
         factor used.
     y_skip_factor {Long}:
         The number of vertical pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of rows in the raster. The default is 1 or the last y skip
         factor used.
     ignore_values {Long}:
         The pixel values that are not to be included in the statistics
         calculation.The default is no value.
     pyramid_level {Long}:
         The number of reduced-resolution dataset layers that will be built.
         The default value is -1, which will build full pyramids. A value of 0
         will result in no pyramid levels.The maximum number of pyramid levels
         you can specify is 29. Any value
         of 30 or higher will create a full set of pyramids.
     SKIP_FIRST {Boolean}:
         Specifies whether the first pyramid level will be skipped. Skipping
         the first level will take up slightly less disk space, but it will
         slow down performance at these scales.

         * NONE-The first pyramid level will not be skipped; it will be built.
         This is the default.

         * SKIP_FIRST-The first pyramid level will be skipped; it will not be
         built.
     resample_technique {String}:
         Specifies the resampling technique that will be used to build the
         pyramids.

         * NEAREST-The value of the closest pixel will be used to assign a
         value to the output pixel when resampling. This is the default.

         * BILINEAR-The new value of a pixel will be based on a weighted
         distance average of the four nearest input pixel centers.

         * CUBIC-The new value of a pixel will be based on fitting a smooth
         curve through the 16 nearest input pixel centers.
     compression_type {String}:
         Specifies the compression type that will be used when building the
         raster pyramids.

         * DEFAULT-If the source data is compressed using a wavelet
         compression, pyramids will be built using the JPEG compression type;
         otherwise, LZ77 will be used. This is the default.

         * LZ77-The LZ77 compression algorithm will be used to build pyramids.
         This compression type can be used for any data type.

         * JPEG-The JPEG compression algorithm will be used to build pyramids.
         Only data that adheres to the JPEG compression specification can use
         this compression type. If this compression type is specified, you can
         then set the Compression quality value.

         * JPEG_YCBCR-A lossy compression using the luma (Y) and chroma (Cb and
         Cr) color space components will be used.

         * NONE-No compression will be used when building pyramids.
     compression_quality {Long}:
         The compression quality that will be used when pyramids are built with
         the JPEG compression type. The value must be between 0 and 100. The
         values closer to 100 will produce a higher-quality image, but the
         compression ratio will be lower.
     skip_existing {Boolean}:
         Specify whether to calculate statistics only where they are missing,
         or regenerate them even if they exist.

         * SKIP_EXISTING-Statistics will only be calculated if they do not
         already exist. This is the default.

         * OVERWRITE-Statistics will be calculated even if they already exist;
         existing statistics will be overwritten.
     where_clause {SQL Expression}:
         An SQL expression to select raster datasets that will be processed.
     sips_mode {Boolean}:
         Specifies whether to enable building of pyramid files using key
         processes and algorithms defined in the Softcopy Image Processing
         Standard (SIPS), NGA.STND.0014.

         * NONE-Pyramids will be built using standard subsampling methods. This
         is the default.

         * SIPS_MODE-Pyramids will be built using SIPS processing."""
    ...

@gptooldoc("BuildRasterAttributeTable_management", None)
def BuildRasterAttributeTable(
    in_raster=..., overwrite=..., convert_colormap=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildRasterAttributeTable_management(in_raster, {overwrite}, {convert_colormap})

       Adds a raster attribute table to a raster dataset or updates and
       existing one. This is used primarily with discrete data.

    INPUTS:
     in_raster (Raster Layer):
         The input raster dataset to which a table will be added. This tool
         will not run if the pixel type is floating point or double precision.
     overwrite {Boolean}:
         Specifies whether the existing table will be overwritten.

         * NONE-The existing raster attribute table will not be overwritten and
         any edits will be appended to it. This is the default.

         * Overwrite-The existing raster attribute table will be overwritten
         and a new raster attribute table will be created.
     convert_colormap {Boolean}:
         Specifies whether the color map will be converted to a raster
         attribute table. The output raster attribute table will include Red,
         Green, and Blue fields containing color values from the color map.
         These fields define the display colors for the corresponding class
         values.This parameter only applies when the Input Raster parameter
         value
         includes an associated color map.

         * Checked-The color map will be converted to a new raster attribute
         table.

         * Unchecked-The color map will not be converted to a raster attribute
         table. This is the default.

         * ConvertColormap-The color map will be converted to a new raster
         attribute table.

         * NONE-The color map will not be converted to a raster attribute
         table. This is the default."""
    ...

@gptooldoc("CalculateStatistics_management", None)
def CalculateStatistics(
    in_raster_dataset=...,
    x_skip_factor=...,
    y_skip_factor=...,
    ignore_values=...,
    skip_existing=...,
    area_of_interest=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateStatistics_management(in_raster_dataset, {x_skip_factor}, {y_skip_factor}, {ignore_values;ignore_values...}, {skip_existing}, {area_of_interest})

       Calculates statistics for a raster dataset or a mosaic dataset.

    INPUTS:
     in_raster_dataset (Mosaic Dataset / Mosaic Layer / Raster Layer / Raster Dataset):
         The input raster dataset or mosaic dataset.
     x_skip_factor {Long}:
         The number of horizontal pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of columns in the raster. The default is 1 or the last skip
         factor used.The skip factors for raster datasets stored in a file
         geodatabase or
         an enterprise geodatabase are different. First, if the x and y skip
         factors are different, the smaller skip factor will be used for both
         the x and y skip factors. Second, the skip factor is related to the
         pyramid level that most closely fits the skip factor chosen. If the
         skip factor value is not equal to the number of pixels in a pyramid
         layer, the number is rounded down to the next pyramid level, and those
         statistics are used.
     y_skip_factor {Long}:
         The number of vertical pixels between samples.A skip factor controls
         the portion of the raster that is used when
         calculating the statistics. The input value indicates the horizontal
         or vertical skip factor, where a value of 1 will use each pixel and a
         value of 2 will use every second pixel. The skip factor can only range
         from 1 to the number of columns/rows in the raster.The value must be
         greater than zero and less than or equal to the
         number of rows in the raster. The default is 1 or the last y skip
         factor used.The skip factors for raster datasets stored in a file
         geodatabase or
         an enterprise geodatabase are different. First, if the x and y skip
         factors are different, the smaller skip factor will be used for both
         the x and y skip factors. Second, the skip factor is related to the
         pyramid level that most closely fits the skip factor chosen. If the
         skip factor value is not equal to the number of pixels in a pyramid
         layer, the number is rounded down to the next pyramid level, and those
         statistics are used.
     ignore_values {Long}:
         The pixel values that are not to be included in the statistics
         calculation.The default is no value or the last ignore value used.
     skip_existing {Boolean}:
         Specifies whether statistics will be calculated only when they are
         missing or will be regenerated even if they exist.

         * OVERWRITE-Statistics will be calculated even if they already exist,
         and existing statistics will be overwritten. This is the default.

         * SKIP_EXISTING-Statistics will only be calculated if they do not
         already exist.
     area_of_interest {Feature Set}:
         The feature class that represents the area in the dataset from where
         you want the statistics to be calculated, so they are not generated
         from the entire dataset."""
    ...

@gptooldoc("ConvertRasterFunctionTemplate_management", None)
def ConvertRasterFunctionTemplate(
    in_raster_function_template=..., out_raster_function_template_file=..., format=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConvertRasterFunctionTemplate_management(in_raster_function_template, out_raster_function_template_file, {format})

       Converts a raster function template between formats (rft.xml, json,
       and binary).

    INPUTS:
     in_raster_function_template (File / String):
         The input raster function template file. The input template file can
         be XML, JSON, or binary format.
     format {String}:
         The output function template file format.

         * XML-XML output format.

         * JSON-JSON output format. This is the default.

         * BINARY-Binary output format.

    OUTPUTS:
     out_raster_function_template_file (File):
         The output raster function template file path and file name."""
    ...

@gptooldoc("DeleteColormap_management", None)
def DeleteColormap(
    in_raster=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteColormap_management(in_raster)

       Removes the color map associated with a raster dataset.

    INPUTS:
     in_raster (Raster Layer):
         The raster dataset that containing the color map you want to remove."""
    ...

@gptooldoc("DeleteRasterAttributeTable_management", None)
def DeleteRasterAttributeTable(
    in_raster=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteRasterAttributeTable_management(in_raster)

       Removes the raster attribute table associated with a raster dataset.

    INPUTS:
     in_raster (Raster Layer):
         The raster dataset containing the attribute table you want to remove."""
    ...

@gptooldoc("ExportRasterWorldFile_management", None)
def ExportRasterWorldFile(
    in_raster_dataset=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportRasterWorldFile_management(in_raster_dataset)

       Creates a world file based on the pixel size and the location of the
       upper left pixel.

    INPUTS:
     in_raster_dataset (Raster Dataset):
         The raster dataset from which you want to create the world file."""
    ...

@gptooldoc("GetCellValue_management", None)
def GetCellValue(
    in_raster=..., location_point=..., band_index=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GetCellValue_management(in_raster, location_point, {band_index;band_index...})

       Retrieves the value of a given pixel using its x, y coordinates.

    INPUTS:
     in_raster (Mosaic Dataset / Mosaic Layer / Raster Layer):
         The raster that you want to query.
     location_point (Point):
         The X and Y coordinates of the pixel location.
     band_index {Value Table}:
         Specify the bands that you want to query. Leave blank to query all
         bands in a multiband dataset."""
    ...

@gptooldoc("GetRasterProperties_management", None)
def GetRasterProperties(
    in_raster=..., property_type=..., band_index=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GetRasterProperties_management(in_raster, {property_type}, {band_index})

       Retrieves information from the metadata and descriptive statistics
       about a raster dataset.

    INPUTS:
     in_raster (Composite Geodataset):
         The raster containing the properties to retrieve.
     property_type {String}:
         The property to be obtained from the input raster.

         * MINIMUM-Smallest value of all cells in the input raster.

         * MAXIMUM-Largest value of all cells in the input raster.

         * MEAN-Average of all cells in the input raster.

         * STD-Standard deviation of all cells in the input raster.

         * UNIQUEVALUECOUNT-Number of unique values in the input raster.

         * TOP-Top or YMax value of the extent.

         * LEFT-Left or XMin value of the extent.

         * RIGHT-Right or XMax value of the extent.

         * BOTTOM-Bottom or YMin value of the extent.

         * CELLSIZEX-Cell size in the x-direction.

         * CELLSIZEY-Cell size in the y-direction.

         * VALUETYPE-Type of the cell value in the input raster:

         * 0 = 1-bit

         * 1 = 2-bit

         * 2 = 4-bit

         * 3 = 8-bit unsigned integer

         * 4 = 8-bit signed integer

         * 5 = 16-bit unsigned integer

         * 6 = 16-bit signed integer

         * 7 = 32-bit unsigned integer

         * 8 = 32-bit signed integer

         * 9 = 32-bit floating point

         * 10 = 64-bit double precision

         * 11 = 8-bit complex

         * 12 = 16-bit complex

         * 13 = 32-bit complex

         * 14 = 64-bit complex

         * COLUMNCOUNT-Number of columns in the input raster.

         * ROWCOUNT-Number of rows in the input raster.

         * BANDCOUNT-Number of bands in the input raster.

         * ANYNODATA-Returns whether there is NoData in the raster.

         * ALLNODATA-Returns whether all the pixels are NoData. This is the
         same as ISNULL.

         * SENSORNAME-Name of the sensor.

         * PRODUCTNAME-Product name related to the sensor.

         * ACQUISITIONDATE-Date that the data was captured.

         * SOURCETYPE-Source type.

         * CLOUDCOVER-Amount of cloud cover as a percentage.

         * SUNAZIMUTH-Sun azimuth, in degrees.

         * SUNELEVATION-Sun elevation, in degrees.

         * SENSORAZIMUTH-Sensor azimuth, in degrees.

         * SENSORELEVATION-Sensor elevation, in degrees.

         * OFFNADIR-Off-nadir angle, in degrees.

         * WAVELENGTH-Wavelength range of the band, in nanometers.
     band_index {String}:
         Choose the band name from which to get the properties. If no band is
         chosen, then the first band will be used."""
    ...

@gptooldoc("SetRasterProperties_management", None)
def SetRasterProperties(
    in_raster=...,
    data_type=...,
    statistics=...,
    stats_file=...,
    nodata=...,
    key_properties=...,
    multidimensional_info=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetRasterProperties_management(in_raster, {data_type}, {statistics;statistics...}, {stats_file}, {nodata;nodata...}, {key_properties;key_properties...}, {multidimensional_info;multidimensional_info...})

       Sets the data type, statistics, and NoData values on a raster or
       mosaic dataset.

    INPUTS:
     in_raster (Mosaic Layer / Raster Layer):
         The raster or mosaic dataset with the properties to be set.
     data_type {String}:
         Specifies the type of imagery in the mosaic dataset.

         * GENERIC-The mosaic dataset does not have a specified data type.

         * ELEVATION-The mosaic dataset contains elevation data.

         * THEMATIC-The mosaic dataset has thematic data, which has discrete
         values, such as land cover.

         * PROCESSED-The mosaic dataset has been color balanced.

         * SCIENTIFIC-The data has scientific information and will be displayed
         with the blue to red color ramp by default.

         * VECTOR_UV-The data is a two-band raster that contains a U and a V
         component of vector field data.

         * VECTOR_MAGDIR-The data is a two-band raster that contains the
         magnitude and direction of vector field data.

         * DATE-The data has date information and will be displayed in date
         format.
     statistics {Value Table}:
         The bands and values for the minimum, maximum, mean, and standard
         deviation.
     stats_file {File}:
         An .xml file that contains the statistics.
     nodata {Value Table}:
         The NoData value for each band. Each band can have a unique NoData
         value defined, or the same value can be specified for all bands. To
         define multiple NoData values for each band selection, use a space
         delimiter between each NoData value.
     key_properties {Value Table}:
         The natively supported properties. The data used may have additional
         properties not included in the following list. The properties are not
         case sensitive.

         * AcquisitionDate

         * BandName

         * BlockName

         * CloudCover

         * DatasetTag

         * Dimensions

         * FlowDirection

         * Footprint

         * HighCellSize

         * LowCellSize

         * MinCellSize

         * MaxCellSize

         * OffNadir

         * ParentRasterType

         * ParentTemplate

         * PerspectiveX

         * PerspectiveY

         * PerspectiveZ

         * ProductName

         * RadianceBias

         * RadianceGain

         * ReflectanceBias

         * ReflectanceGain

         * Segmented

         * SensorAzimuth

         * SensorElevation

         * SensorName

         * SolarIrradiance

         * SourceBandIndex

         * StdPressure

         * StdPressure_Max

         * StdTemperature

         * StdTemperature_Max

         * StdTime

         * StdTime_Max

         * StdZ

         * StdZ_max

         * SunAzimuth

         * SunElevation

         * ThermalConstant_K1

         * ThermalConstant_K2

         * Variable

         * VerticalAccuracy

         * WavelengthMin

         * WavelengthMax
     multidimensional_info {Value Table}:
         The dimensional information for the raster dataset. Setting
         dimensional information will convert the dimensionless raster into a
         multidimensional raster.If the dimension is time, the dimension name
         must be StdTime. The
         format for time is either year-month-day (2021-10-01) or year-month-
         dayThh:mm:ss (2021-10-01T01:00:00).To define a variable with both time
         and elevation, add the variable
         with time first; then add the same variable with the z-dimension."""
    ...

@gptooldoc("AddRuleToRelationshipClass_management", None)
def AddRuleToRelationshipClass(
    in_rel_class=...,
    origin_subtype=...,
    origin_minimum=...,
    origin_maximum=...,
    destination_subtype=...,
    destination_minimum=...,
    destination_maximum=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddRuleToRelationshipClass_management(in_rel_class, {origin_subtype}, {origin_minimum}, {origin_maximum}, {destination_subtype}, {destination_minimum}, {destination_maximum})

       Adds a rule to a relationship class.

    INPUTS:
     in_rel_class (Relationship Class):
         The relationship class to which a rule will be added.
     origin_subtype {String}:
         Specifies the subtype of the origin class. If the origin class has
         subtypes, choose the subtype to which you want to associate a
         relationship class rule. If the origin class has no subtypes, the
         relationship rule will apply to all features.
     origin_minimum {Long}:
         Specifies the minimum range cardinality for the origin class if the
         relationship class is many-to-many.
     origin_maximum {Long}:
         Specifies the maximum range cardinality for the origin class if the
         relationship class is many-to-many or one-to-many.
     destination_subtype {String}:
         Specifies the subtype of the destination class. If the destination
         class has subtypes, choose the subtype to which you want to associate
         a relationship class rule. If the destination class has no subtypes,
         the relationship rule will apply to all features.
     destination_minimum {Long}:
         Specifies the minimum range cardinality for the destination class if
         the relationship class is many-to-many or one-to-many.
     destination_maximum {Long}:
         Specifies the maximum range cardinality for the destination class if
         the relationship class is many-to-many or one-to-many."""
    ...

@gptooldoc("CreateRelationshipClass_management", None)
def CreateRelationshipClass(
    origin_table=...,
    destination_table=...,
    out_relationship_class=...,
    relationship_type=...,
    forward_label=...,
    backward_label=...,
    message_direction=...,
    cardinality=...,
    attributed=...,
    origin_primary_key=...,
    origin_foreign_key=...,
    destination_primary_key=...,
    destination_foreign_key=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRelationshipClass_management(origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, attributed, origin_primary_key, origin_foreign_key, {destination_primary_key}, {destination_foreign_key})

       This tool creates a relationship class to store an association between
       fields or features in the origin table and the destination table.

    INPUTS:
     origin_table (Table View):
         The table or feature class that is associated to the destination
         table.
     destination_table (Table View):
         The table that is associated to the origin table.
     relationship_type (String):
         The type of relationship to create between the origin and destination
         tables.

         * SIMPLE-A relationship between independent objects (parent to
         parent). This is the default.

         * COMPOSITE-A relationship between dependent objects where the
         lifetime of one object controls the lifetime of the related object
         (parent to child).
     forward_label (String):
         A name to uniquely identify the relationship when navigating from the
         origin table to the destination table.
     backward_label (String):
         A name to uniquely identify the relationship when navigating from the
         destination table to the origin table.
     message_direction (String):
         The direction in which messages are passed between the origin and
         destination tables. For example, in a relationship between poles and
         transformers, when the pole is deleted, it sends a message to its
         related transformer objects informing them it was deleted.

         * FORWARD-Messages are passed from the origin to the destination
         table.

         * BACKWARD-Messages are passed from the destination to the origin
         table.

         * BOTH-Messages are passed from the origin to the destination table
         and from the destination to the origin table.

         * NONE-No messages passed. This is the default.
     cardinality (String):
         Determines how many relationships exist between rows or features in
         the origin and rows or features in the destination table.

         * ONE_TO_ONE-Each row or feature in the origin table can be related to
         zero or one row or feature in the destination table. This is the
         default.

         * ONE_TO_MANY-Each row or feature in the origin table can be related
         to one or several rows or features in the destination table.

         * MANY_TO_MANY-Several fields or features in the origin table can be
         related to one or several rows or features in the destination table.
     attributed (Boolean):
         Specifies if the relationship will have attributes.

         * NONE-Indicates the relationship class will not have attributes. This
         is the default.

         * ATTRIBUTED-Indicates the relationship class will have attributes.
     origin_primary_key (String):
         The field in the origin table, typically the OID field, that links it
         to the Origin Foreign Key field in the relationship class table.
     origin_foreign_key (String):
         The field in the relationship class table that links it to the Origin
         Primary Key field in the origin table.
     destination_primary_key {String}:
         The field in the destination table, typically the OID field, that
         links it to the Destination Foreign Key field in the relationship
         class table.
     destination_foreign_key {String}:
         The field in the relationship class table that links it to the
         Destination Primary Key field in the destination table.

    OUTPUTS:
     out_relationship_class (Relationship Class):
         The relationship class that is created."""
    ...

@gptooldoc("MigrateRelationshipClass_management", None)
def MigrateRelationshipClass(
    in_relationship_class=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MigrateRelationshipClass_management(in_relationship_class)

       Migrates an ObjectID-based relationship class to a GlobalID-based
       relationship class.

    INPUTS:
     in_relationship_class (Relationship Class):
         ObjectID-based relationship class that will be migrated to a GlobalID-
         based relationship class. The origin and destination feature classes
         or tables must already have GlobalIDs."""
    ...

@gptooldoc("RemoveRuleFromRelationshipClass_management", None)
def RemoveRuleFromRelationshipClass(
    in_rel_class=..., origin_subtype=..., destination_subtype=..., remove_all=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveRuleFromRelationshipClass_management(in_rel_class, {origin_subtype}, {destination_subtype}, {remove_all})

       Removes a rule from a relationship class.

    INPUTS:
     in_rel_class (Relationship Class):
         The relationship class with the rule to remove.
     origin_subtype {String}:
         If the origin class has subtypes, the subtype that is associated with
         the relationship class rule to be deleted.
     destination_subtype {String}:
         If the destination class has subtypes, the subtype that is associated
         with the relationship class rule to be deleted.
     remove_all {Boolean}:
         Specifies the relationship rules to be removed from the relationship
         class.

         * REMOVE-All relationship rules will be removed from the input
         relationship class.

         * NOT_ALL-Only rules from the origin and destination subtypes
         specified will be removed. This is the default."""
    ...

@gptooldoc("SetRelationshipClassSplitPolicy_management", None)
def SetRelationshipClassSplitPolicy(
    in_rel_class=..., split_policy=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetRelationshipClassSplitPolicy_management(in_rel_class, split_policy)

       Defines the split policy for related features.

    INPUTS:
     in_rel_class (Relationship Class):
         The relationship class on which the split policy will be set. The
         origin feature class must be a polyline or polygon feature class and
         the destination must be a nonspatial table.
     split_policy (String):
         Specifies the split policy to apply to the relationship class.

         * DEFAULT_COMPOSITE-If the feature class split model is
         Delete/Insert/Insert, the relationships and the part objects will be
         deleted. If the feature class split model is Update/Insert, the
         relationships on the largest resulting feature will be preserved. This
         is the default split policy for composite relationship classes.

         * DEFAULT_SIMPLE-The relationships on the largest resulting feature
         will be preserved. This is the default split policy for simple
         relationship classes.

         * DUPLICATE_RELATED_OBJECTS-Copies of the related objects will be
         generated and assigned to both resulting parts. The relationship class
         must be Global ID based to use this split policy."""
    ...

@gptooldoc("TableToRelationshipClass_management", None)
def TableToRelationshipClass(
    origin_table=...,
    destination_table=...,
    out_relationship_class=...,
    relationship_type=...,
    forward_label=...,
    backward_label=...,
    message_direction=...,
    cardinality=...,
    relationship_table=...,
    attribute_fields=...,
    origin_primary_key=...,
    origin_foreign_key=...,
    destination_primary_key=...,
    destination_foreign_key=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TableToRelationshipClass_management(origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, relationship_table, attribute_fields;attribute_fields..., origin_primary_key, origin_foreign_key, destination_primary_key, destination_foreign_key)

       Creates an attributed relationship class from the origin, destination,
       and relationship tables.

    INPUTS:
     origin_table (Table View):
         The table or feature class that will be associated to the destination
         table.
     destination_table (Table View):
         The table or feature class that will be associated to the origin
         table.
     relationship_type (String):
         Specifies the type of association that will be created between the
         origin and destination tables.

         * SIMPLE-Each object will be independent of each other (a parent-to-
         parent relationship). This is the default.

         * COMPOSITE-The lifetime of one object will control the lifetime of
         its related object (a parent-child relationship).
     forward_label (String):
         A label describing the relationship as it is traversed from the origin
         table or feature class to the destination table or feature class.
     backward_label (String):
         A label describing the relationship as it is traversed from the
         destination table or feature class to the origin table or feature
         class.
     message_direction (String):
         Specifies the direction messages that will be propagated between the
         objects in the relationship. For example, in a relationship between
         poles and transformers, when the pole is deleted, it sends a message
         to its related transformer objects informing them it was deleted.

         * NONE-No messages will be propagated. This is the default.

         * FORWARD-Messages will be propagated from the origin to the
         destination.

         * BACKWARD-Messages will be propagated from the destination to the
         origin.

         * BOTH-Messages will be propagated from the origin to the destination
         and from the destination to the origin.
     cardinality (String):
         Specifies the cardinality of the relationship between the origin and
         destination.

         * ONE_TO_ONE-Each object of the origin table or feature class can be
         related to zero or one object of the destination table or feature
         class. This is the default.

         * ONE_TO_MANY-Each object of the origin table or feature class can be
         related to multiple objects in the destination table or feature class.

         * MANY_TO_MANY-Multiple objects of the origin table or feature class
         can be related to multiple objects in the destination table or feature
         class.
     relationship_table (Table View):
         The table containing attributes that will be added to the relationship
         class.
     attribute_fields (Field):
         The names of the fields containing attribute values that will be added
         to the relationship class. The fields must be present in the
         relationship_table parameter value.
     origin_primary_key (String):
         The field in the origin table that will be used to create the
         relationship.
     origin_foreign_key (String):
         The name of the field in the relationship table that refers to the
         primary key field in the origin table or feature class. For table-
         based relationship classes, these values are used to populate the
         relationships in the relationship class so they cannot be null.
     destination_primary_key (String):
         The field in the destination table that will be used to create the
         relationship.
     destination_foreign_key (String):
         The field in the relationship table that refers to the primary key
         field in the destination table or feature class. For table-based
         relationship classes, these values are used to populate the
         relationships in the relationship class so they cannot be null.

    OUTPUTS:
     out_relationship_class (Relationship Class):
         The relationship class that will be created."""
    ...

@gptooldoc("CreateFishnet_management", None)
def CreateFishnet(
    out_feature_class=...,
    origin_coord=...,
    y_axis_coord=...,
    cell_width=...,
    cell_height=...,
    number_rows=...,
    number_columns=...,
    corner_coord=...,
    labels=...,
    template=...,
    geometry_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFishnet_management(out_feature_class, origin_coord, y_axis_coord, cell_width, cell_height, number_rows, number_columns, {corner_coord}, {labels}, {template}, {geometry_type})

       Creates a fishnet of rectangular cells. The output can be polyline or
       polygon features.

    INPUTS:
     origin_coord (Point):
         The starting pivot point of the fishnet.
     y_axis_coord (Point):
         The y-axis coordinate is used to orient the fishnet. The fishnet is
         rotated by the same angle as defined by the line connecting the origin
         and the y-axis coordinate.
     cell_width (Double):
         The width of each cell. To calculate the cell width using the
         number_rows parameter value, leave this parameter unspecified or set
         the value to zero; the width will be calculated when the tool is run.
     cell_height (Double):
         The height of each cell. To calculate the cell height using the
         number_columns parameter value, leave this parameter unspecified or
         set the value to zero; the height will be calculated when the tool is
         run.
     number_rows (Long):
         The number of rows the fishnet will have. To calculate the number of
         rows using the cell_width parameter value, leave this parameter
         unspecified or set the value to zero; the number of rows will be
         calculated when the tool is run.
     number_columns (Long):
         The number of columns the fishnet will have. To calculate the number
         of columns using the cell_height parameter value, leave this parameter
         unspecified or set the value to zero; the number of columns will be
         calculated when the tool is run.
     corner_coord {Point}:
         The opposite corner of the fishnet set by the origin_coord
         parameter.This parameter is disabled if the origin_coord,
         y_axis_coord,
         cell_width, cell_height, number_rows and number_columns parameters are
         specified.
     labels {Boolean}:
         Specifies whether a point feature class will be created containing
         label points at the center of each fishnet cell.

         * LABELS-A point feature class will be created. This is the default.

         * NO_LABELS-A point feature class will not be created.
     template {Extent}:
         The extent of the fishnet. The extent can be entered by specifying the
         coordinates or using a template dataset.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     geometry_type {String}:
         Specifies whether the output fishnet cells will be polyline or polygon
         features.

         * POLYLINE-Output will be a polyline feature class. Each cell is
         defined by four line features.

         * POLYGON-Output will be a polygon feature class. Each cell is defined
         by one polygon feature.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class containing the fishnet of rectangular cells."""
    ...

@gptooldoc("CreateRandomPoints_management", None)
def CreateRandomPoints(
    out_path=...,
    out_name=...,
    constraining_feature_class=...,
    constraining_extent=...,
    number_of_points_or_field=...,
    minimum_allowed_distance=...,
    create_multipoint_output=...,
    multipoint_size=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateRandomPoints_management(out_path, out_name, {constraining_feature_class}, {constraining_extent}, {number_of_points_or_field}, {minimum_allowed_distance}, {create_multipoint_output}, {multipoint_size})

       Creates a specified number of random point features. Random points can
       be generated in an extent window, inside polygon features, on point
       features, or along line features.

    INPUTS:
     out_path (Workspace / Feature Dataset):
         The location or workspace in which the random points feature class
         will be created. This location or workspace must already exist.
     out_name (String):
         The name of the random points feature class to be created.
     constraining_feature_class {Feature Layer}:
         Random points will be generated inside or along the features in this
         feature class. The constraining feature class can be point,
         multipoint, line, or polygon. Points will be randomly placed inside
         polygon features, along line features, or at point feature locations.
         Each feature in this feature class will have the specified number of
         points generated inside it (for example, if you specify 100 points,
         and the constraining feature class has 5 features, 100 random points
         will be generated in each feature, totaling 500 points).
     constraining_extent {Extent / Feature Layer / Raster Layer}:
         Random points will be generated inside the extent. The constraining
         extent will only be used if no constraining feature class is
         specified.
     number_of_points_or_field {Long / Field}:
         The number of points to be randomly generated.The number of points can
         be specified as a long integer number or as a
         field from the constraining features containing numeric values for how
         many random points to place within each feature. The field option is
         only valid for polygon or line constraining features. If the number of
         points is supplied as a long integer number, each feature in the
         constraining feature class will have that number of random points
         generated inside or along it.
     minimum_allowed_distance {Linear Unit / Field}:
         The shortest distance allowed between any two randomly placed points.
         If a value of 1 Meter is specified, all random points will be farther
         than 1 meter away from the closest point.
     create_multipoint_output {Boolean}:
         Determines if the output feature class will be a multipart or single-
         part feature.

         * POINT-The output will be geometry type point (each point is a
         separate feature). This is the default.

         * MULTIPOINT-The output will be geometry type multipoint (all points
         are a single feature).
     multipoint_size {Long}:
         If create_multipoint_output is set to MULTIPOINT, specify the number
         of random points to be placed in each multipoint geometry. The default
         is 10."""
    ...

@gptooldoc("GeneratePointsAlongLines_management", None)
def GeneratePointsAlongLines(
    Input_Features=...,
    Output_Feature_Class=...,
    Point_Placement=...,
    Distance=...,
    Percentage=...,
    Include_End_Points=...,
    Add_Chainage_Fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeneratePointsAlongLines_management(Input_Features, Output_Feature_Class, Point_Placement, {Distance}, {Percentage}, {Include_End_Points}, {Add_Chainage_Fields})

       Creates point features along lines or polygons at fixed intervals or
       by percentage of a feature's length.

    INPUTS:
     Input_Features (Feature Layer):
         The line or polygon features to be converted into points.
     Point_Placement (String):
         Specifies the method that will be used to create points.

         * PERCENTAGE-The Percentage parameter value will be used to place
         points along the features by percentage.

         * DISTANCE-The Distance parameter value will be used to place points
         at fixed distances along the features. This is the default.
     Distance {Linear Unit}:
         The interval from the beginning of the feature at which points will be
         placed.
     Percentage {Double}:
         The percentage from the beginning of the feature at which points will
         be placed. For example, if a percentage of 40 is used, points will be
         placed at 40 percent and 80 percent of the feature's distance.
     Include_End_Points {Boolean}:
         Specifies whether additional points will be included at the start
         point and end point of the feature.

         * END_POINTS-Additional points will be included at the start point and
         end point of the feature.

         * NO_END_POINTS-No additional points will be included at the start
         point and end point of the feature. This is the default.
     Add_Chainage_Fields {Boolean}:
         Specifies whether the accumulated distance and sequence fields will be
         added to the output.

         * ADD_CHAINAGE-The accumulated distance (ORIG_LEN) and sequence
         (ORIG_SEQ) fields will be added to the output. Distance values are
         added in the units of the Input_Features value's spatial reference.

         * NO_CHAINAGE-The accumulated distance or sequence fields will not be
         added to the output. This is the default.

    OUTPUTS:
     Output_Feature_Class (Feature Class):
         The point feature class that will be created from the input features."""
    ...

@gptooldoc("GenerateRectanglesAlongLines_management", None)
def GenerateRectanglesAlongLines(
    in_features=...,
    out_feature_class=...,
    length_along_line=...,
    length_perpendicular_to_line=...,
    spatial_sort_method=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateRectanglesAlongLines_management(in_features, out_feature_class, {length_along_line}, {length_perpendicular_to_line}, {spatial_sort_method})

       Creates a series of rectangular polygons that follow a single linear
       feature or a group of linear features.

    INPUTS:
     in_features (Feature Layer):
         The input polyline features defining the path of the features.
     length_along_line {Linear Unit}:
         The length of the output polygon features along the input line
         features. The default value is determined by the spatial reference of
         the input line features. This value will be 1/100 of the input feature
         class extent along the x-axis.
     length_perpendicular_to_line {Linear Unit}:
         The length of the output polygon features perpendicular to the input
         line features. The default value is determined by the spatial
         reference of the input line features. This value will be one-half the
         number used for the length along the line.
     spatial_sort_method {String}:
         Output features are created in a sequential order and require a
         spatial starting point. Setting the direction type to upper right will
         start the output features in the upper right of each input feature.

         * UR-Features start in the upper right corner. This is the default.

         * UL-Features start in the upper left corner.

         * LR-Features starts in the lower right corner.

         * LL-Features starts in the lower left corner.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output polygon feature class."""
    ...

@gptooldoc("GenerateTessellation_management", None)
def GenerateTessellation(
    Output_Feature_Class=...,
    Extent=...,
    Shape_Type=...,
    Size=...,
    Spatial_Reference=...,
    H3_Resolution=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateTessellation_management(Output_Feature_Class, Extent, {Shape_Type}, {Size}, {Spatial_Reference}, {H3_Resolution})

       Generates a tessellated grid of regular polygon features to cover a
       given extent. The tessellation can be of triangles, squares, diamonds,
       hexagons, H3 hexagons, or transverse hexagons.

    INPUTS:
     Extent (Extent):
         The extent that the tessellation will cover. This can be the currently
         visible area, the extent of a dataset, or manually entered values.

         * MAXOF-The maximum extent of all inputs will be used.

         * MINOF-The minimum area common to all inputs will be used.

         * DISPLAY-The extent is equal to the visible display.

         * Layer name-The extent of the specified layer will be used.

         * Extent object-The extent of the specified object will be used.

         * Space delimited string of coordinates-The extent of the specified
         string will be used. Coordinates are expressed in the order of x-min,
         y-min, x-max, y-max.
     Shape_Type {String}:
         Specifies the shape that will be generated.

         * HEXAGON-Hexagon-shaped features will be generated. The top and
         bottom side of each hexagon will be parallel with the x-axis of the
         coordinate system (the top and bottom are flat).

         * TRANSVERSE_HEXAGON-Transverse hexagon-shaped features will be
         generated. The right and left side of each hexagon will be parallel
         with the y-axis of the dataset's coordinate system (the top and bottom
         are pointed).

         * SQUARE-Square-shaped features will be generated. The top and bottom
         side of each square will be parallel with the x-axis of the coordinate
         system, and the right and left sides will be parallel with the y-axis
         of the coordinate system.

         * DIAMOND-Diamond-shaped features will be generated. The sides of each
         polygon will be rotated 45 degrees away from the x-axis and y-axis of
         the coordinate system.

         * TRIANGLE-Triangular-shaped features will be generated. Each triangle
         will be a regular three-sided equilateral polygon.

         * H3_HEXAGON-Hexagon-shaped features will be generated based on the H3
         Hexagonal hierarchical geospatial indexing system.
     Size {Areal Unit}:
         The area of each individual shape that comprises the tessellation.
     Spatial_Reference {Spatial Reference}:
         The spatial reference to which the output dataset will be projected.
         If a spatial reference is not provided, the output will be projected
         to the spatial reference of the input extent. If neither has a spatial
         reference, the output will be projected in GCS_WGS_1984.
     H3_Resolution {Long}:
         Specifies the H3 resolution of the hexagons.With each increasing
         resolution value, the area of the polygons will
         be one seventh the size.

         * 0-Hexagons will be created at the H3 resolution of 0, with an
         average area of 4,357,449.416078381 square kilometers.

         * 1-Hexagons will be created at the H3 resolution of 1, with an
         average area of 609,788.441794133 square kilometers.

         * 2-Hexagons will be created at the H3 resolution of 2, with an
         average area of 86,801.780398997 square kilometers.

         * 3-Hexagons will be created at the H3 resolution of 3, with an
         average area of 12,393.434655088 square kilometers.

         * 4-Hexagons will be created at the H3 resolution of 4, with an
         average area of 1,770.347654491 square kilometers.

         * 5-Hexagons will be created at the H3 resolution of 5, with an
         average area of 252.903858182 square kilometers.

         * 6-Hexagons will be created at the H3 resolution of 6, with an
         average area of 36.129062164 square kilometers.

         * 7-Hexagons will be created at the H3 resolution of 7, with an
         average area of 5.161293360 square kilometers. This is the default.

         * 8-Hexagons will be created at the H3 resolution of 8, with an
         average area of 0.737327598 square kilometers.

         * 9-Hexagons will be created at the H3 resolution of 9, with an
         average area of 0.105332513 square kilometers.

         * 10-Hexagons will be created at the H3 resolution of 10, with an
         average area of 0.015047502 square kilometers.

         * 11-Hexagons will be created at the H3 resolution of 11, with an
         average area of 0.002149643 square kilometers.

         * 12-Hexagons will be created at the H3 resolution of 12, with an
         average area of 0.000307092 square kilometers.

         * 13-Hexagons will be created at the H3 resolution of 13, with an
         average area of 0.000043870 square kilometers.

         * 14-Hexagons will be created at the H3 resolution of 14, with an
         average area of 0.000006267 square kilometers.

         * 15-Hexagons will be created at the H3 resolution of 15, with an
         average area of 0.000000895 square kilometers.
         This parameter is enabled when the Shape_Type parameter is set to
         H3_HEXAGON.

    OUTPUTS:
     Output_Feature_Class (Feature Class):
         The path and name of the output feature class containing the
         tessellated grid."""
    ...

@gptooldoc("GenerateTransectsAlongLines_management", None)
def GenerateTransectsAlongLines(
    in_features=...,
    out_feature_class=...,
    interval=...,
    transect_length=...,
    include_ends=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateTransectsAlongLines_management(in_features, out_feature_class, interval, transect_length, {include_ends})

       Creates perpendicular transect lines at a regular interval along
       lines.

    INPUTS:
     in_features (Feature Layer):
         The line features along which perpendicular transect lines will be
         generated.
     interval (Linear Unit):
         The interval from the beginning of the feature at which transects will
         be placed.
     transect_length (Linear Unit):
         The length or width of the transect line. Each transect will be placed
         in such a way along the input line that half its length falls on one
         side of the line, and half its length falls on the other side of the
         line.This is the overall length of each transect line, not the
         distance
         that the transect extends from the input line. To specify how far the
         transect line should extend from the input line-for example, 100
         meters-double this value to specify the transect length (200 meters).
     include_ends {Boolean}:
         Specifies whether transects will be generated at the start and end of
         the input line.

         * END_POINTS-Transects will be generated at the start and end of the
         input line.

         * NO_END_POINTS-Transects will not be generated at the start and end
         of the input line. This is the default.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output perpendicular transect lines generated along the input
         features."""
    ...

@gptooldoc("AddSubtype_management", None)
def AddSubtype(
    in_table=..., subtype_code=..., subtype_description=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddSubtype_management(in_table, subtype_code, subtype_description)

       Adds a new subtype to the subtypes in the input table.

    INPUTS:
     in_table (Table View):
         The feature class or table containing the subtype definition to be
         updated.
     subtype_code (Long):
         A unique integer value for the subtype to be added.
     subtype_description (String):
         A description of the subtype code."""
    ...

@gptooldoc("RemoveSubtype_management", None)
def RemoveSubtype(
    in_table=..., subtype_code=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveSubtype_management(in_table, subtype_code;subtype_code...)

       Removes a subtype from the input table using its code.

    INPUTS:
     in_table (Table View):
         The feature class or table containing the subtype definition.
     subtype_code (String):
         The subtype code to remove a subtype from the input table or feature
         class."""
    ...

@gptooldoc("SetDefaultSubtype_management", None)
def SetDefaultSubtype(
    in_table=..., subtype_code=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetDefaultSubtype_management(in_table, subtype_code)

       Sets the default value or code for the input table's subtype.

    INPUTS:
     in_table (Table View):
         The input table or feature class whose subtype default value will be
         set.
     subtype_code (Long):
         The unique default value for a subtype."""
    ...

@gptooldoc("SetSubtypeField_management", None)
def SetSubtypeField(
    in_table=..., field=..., clear_value=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetSubtypeField_management(in_table, {field}, {clear_value})

       Defines the field in the input table or feature class that stores the
       subtype codes.

    INPUTS:
     in_table (Table View):
         The input table or feature class that contains the field to set as a
         subtype field.
     field {Field}:
         The integer field that will store the subtype codes.
     clear_value {Boolean}:
         Specifies whether to clear the subtype field.

         * CLEAR_SUBTYPE_FIELD-The subtype field will be cleared (set to null).

         * DO_NOT_CLEAR-The subtype field will not be cleared. This is the
         default."""
    ...

@gptooldoc("Analyze_management", None)
def Analyze(
    in_dataset=..., components=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Analyze_management(in_dataset, components;components...)

       Updates database statistics of business tables, feature tables, and
       delta tables, along with the statistics of those tables' indexes.

    INPUTS:
     in_dataset (Layer / Table View / Dataset):
         The table or feature class to be analyzed.
     components (String):
         The component type to be analyzed.

         * BUSINESS-Updates business rules statistics.

         * FEATURE-Updates feature statistics.

         * RASTER-Updates statistics on raster tables.

         * ADDS-Updates statistics on added datasets.

         * DELETES-Updates statistics on deleted datasets."""
    ...

@gptooldoc("CopyRows_management", None)
def CopyRows(
    in_rows=..., out_table=..., config_keyword=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CopyRows_management(in_rows, out_table, {config_keyword})

       Copies the rows of a table to a different table.

    INPUTS:
     in_rows (Table View / Raster Layer):
         The input rows to be copied to a new table.
     config_keyword {String}:
         The default storage parameters for an enterprise geodatabase.

    OUTPUTS:
     out_table (Table):
         The table that will be created and to which rows from the input will
         be copied.If the output table is in a folder, include an extension
         such as .csv,
         .txt, or .dbf to make the table the specified format. If the output
         table is in a geodatabase, do not specify an extension."""
    ...

@gptooldoc("CreateTable_management", None)
def CreateTable(
    out_path=..., out_name=..., template=..., config_keyword=..., out_alias=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateTable_management(out_path, out_name, {template;template...}, {config_keyword}, {out_alias})

       Creates a geodatabase table or a dBASE table.

    INPUTS:
     out_path (Workspace):
         The workspace in which the output table will be created.
     out_name (String):
         The name of the table to be created.
     template {Table View}:
         A table with an attribute schema that is used to define the output
         table. Fields in the template table will be added to the output table.
     config_keyword {String}:
         The configuration keyword that determines the storage parameters of
         the table in an enterprise geodatabase.
     out_alias {String}:
         The alternate name of the output table that will be created."""
    ...

@gptooldoc("CreateUnRegisteredTable_management", None)
def CreateUnRegisteredTable(
    out_path=..., out_name=..., template=..., config_keyword=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateUnRegisteredTable_management(out_path, out_name, {template;template...}, {config_keyword})

       Creates an empty table in a database or enterprise geodatabase. The
       table is not registered with the geodatabase.

    INPUTS:
     out_path (Workspace):
         The enterprise geodatabase or database in which the output table will
         be created.
     out_name (String):
         The name of the table to be created.
     template {Table View}:
         An existing table or list of tables with fields and attribute schema
         used to define the fields in the output table.
     config_keyword {String}:
         Specifies the default storage parameters (configurations) for
         geodatabases in a relational database management system (RDBMS). This
         setting is applicable only when using enterprise geodatabase
         tables.Configuration keywords are set by the database administrator."""
    ...

@gptooldoc("DeleteRows_management", None)
def DeleteRows(
    in_rows=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteRows_management(in_rows)

       Deletes all or the selected subset of rows from the input.

    INPUTS:
     in_rows (Table View):
         The feature class, layer, table, or table view whose rows will be
         deleted."""
    ...

@gptooldoc("GetCount_management", None)
def GetCount(
    in_rows: str,
) -> Result:
    """GetCount_management(in_rows)

       Returns the total number of rows for a table.

    INPUTS:
     in_rows (Table View / Raster Layer):
         The input table view or raster layer. If a selection is defined on the
         input, the count of the selected rows will be returned."""
    ...

@gptooldoc("PivotTable_management", None)
def PivotTable(
    in_table=..., fields=..., pivot_field=..., value_field=..., out_table=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PivotTable_management(in_table, fields;fields..., pivot_field, value_field, out_table)

       Creates a table from the input table by reducing redundancy in records
       and flattening one-to-many relationships.

    INPUTS:
     in_table (Table View):
         The table containing the records that will be pivoted.
     fields (Field):
         The fields that define the records that will be included in the output
         table.
     pivot_field (Field):
         The field whose record values will be used to generate the field names
         in the output table.
     value_field (Field):
         The field whose values will populate the pivoted fields in the output
         table.

    OUTPUTS:
     out_table (Table):
         The table that will be created containing the pivoted records."""
    ...

@gptooldoc("TruncateTable_management", None)
def TruncateTable(
    in_table=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TruncateTable_management(in_table)

       Removes all rows from a database table or feature class using truncate
       procedures in the database.

    INPUTS:
     in_table (Table View):
         The input database table or feature class that will be truncated."""
    ...

@gptooldoc("ExportTileCache_management", None)
def ExportTileCache(
    in_cache_source=...,
    in_target_cache_folder=...,
    in_target_cache_name=...,
    export_cache_type=...,
    storage_format_type=...,
    scales=...,
    area_of_interest=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportTileCache_management(in_cache_source, in_target_cache_folder, in_target_cache_name, {export_cache_type}, {storage_format_type}, {scales;scales...}, {area_of_interest})

       Exports tiles from an existing tile cache to a new tile cache or a
       tile package. The tiles can be either independently imported into
       other caches or accessed from ArcGIS Pro or mobile devices.

    INPUTS:
     in_cache_source (Raster Layer / Raster Dataset):
         An existing tile cache to be exported.
     in_target_cache_folder (Folder):
         The output folder into which the tile cache or tile package will be
         exported.
     in_target_cache_name (String):
         The name of the exported tile cache or tile package.
     export_cache_type {String}:
         Specifies whether the cache will be exported as a tile cache or a tile
         package. Tile packages are suitable for ArcGIS Runtime and ArcGIS
         Mobile deployments.

         * TILE_CACHE-The cache will be exported as a stand-alone cache raster
         dataset. This is the default.

         * TILE_PACKAGE-The cache will be exported as a single compressed file
         (.tpk) in which the cache dataset is added as a layer and consolidated
         so that it can be shared easily. This type can be used in ArcMap as
         well as in ArcGIS Runtime and ArcGIS Mobile applications.

         * TILE_PACKAGE_TPKX-The cache will be exported using Compact_v2
         storage format (.tpkx), which provides better performance on network
         shares and cloud storage directories. This improved and simplified
         package structure type is supported by newer versions of the ArcGIS
         platform such as ArcGIS Online, ArcGIS Pro 2.3, ArcGIS Enterprise
         10.7, and ArcGIS Runtime 100.5.
     storage_format_type {String}:
         Determines the storage format of tiles.

         * COMPACT-Group tiles into large files called bundles. This storage
         format is more efficient in terms of storage and mobility.

         * COMPACT_V2-Tiles are grouped in bundle files only. This format
         provides better performance on network shares and cloudstore
         directories. If the Export cache type parameter is set to Tile package
         (tpkx) then the extension of the tile package is (.tpkx), which is
         supported by newer versions of the ArcGIS Platform such as ArcGIS
         Online, ArcGIS Enterprise 11.1 and ArcGIS Runtime 100.5.This is the
         default.

         * EXPLODED-Each tile is stored as an individual file. Note that this
         format cannot be used with tile packages.
     scales {Double}:
         A list of scale levels at which tiles will be exported.
     area_of_interest {Feature Set}:
         An area of interest that spatially constrains where tiles will be
         exported from the cache.The area of interest can be a feature class or
         a feature that you draw
         on the map.This parameter is useful if you want to export irregularly
         shaped
         areas, as the tool clips the cache dataset at pixel resolution."""
    ...

@gptooldoc("GenerateTileCacheTilingScheme_management", None)
def GenerateTileCacheTilingScheme(
    in_dataset=...,
    out_tiling_scheme=...,
    tiling_scheme_generation_method=...,
    number_of_scales=...,
    predefined_tiling_scheme=...,
    scales=...,
    scales_type=...,
    tile_origin=...,
    dpi=...,
    tile_size=...,
    tile_format=...,
    tile_compression_quality=...,
    storage_format=...,
    lerc_error=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateTileCacheTilingScheme_management(in_dataset, out_tiling_scheme, tiling_scheme_generation_method, number_of_scales, {predefined_tiling_scheme}, {scales;scales...}, {scales_type}, {tile_origin}, {dpi}, {tile_size}, {tile_format}, {tile_compression_quality}, {storage_format}, {lerc_error})

       Creates a tiling scheme file based on the information from the source
       dataset. The tiling scheme file will then be used in the Manage Tile
       Cache tool when creating cache tiles.

    INPUTS:
     in_dataset (Raster Layer / Mosaic Layer / Map):
         The source to be used to generate the tiling scheme. It can be a
         raster dataset, a mosaic dataset, or a map.
     tiling_scheme_generation_method (String):
         Choose to use a new or predefined tiling scheme. You can define a new
         tiling scheme with this tool or browse to a predefined tiling scheme
         file (.xml).

         * NEW-Define a new tiling scheme using other parameters in this tool
         to define scale levels, image format, storage format, and so on. This
         is the default.

         * PREDEFINED-Use a tiling scheme .xml file that already exists on
         disk.
     number_of_scales (Long):
         The number of scale levels to be created in the tiling scheme.
     predefined_tiling_scheme {File}:
         Path to a predefined tiling scheme file (usually named conf.xml). This
         parameter is enabled only when the Predefined option is chosen as the
         tiling scheme generation method.
     scales {Value Table}:
         Scale levels to be included in the tiling scheme. By default, these
         are not represented as fractions. Instead, use 500 to represent a
         scale of 1:500, and so on. The value entered in the Number of Scales
         parameter generates a set of default scale levels.
     scales_type {Boolean}:
         Determines the units of the scales parameter.

         * CELL_SIZE-Indicates the values of the scales parameter are pixel
         sizes. This is the default.

         * SCALE-Indicates the values of the scales parameter are scale levels.
     tile_origin {Point}:
         The origin (upper left corner) of the tiling scheme in the coordinates
         of the spatial reference of the source dataset. The extent of the
         source dataset must be within (but does not need to coincide) this
         region.
     dpi {Long}:
         The dots per inch of the intended output device. If a DPI is chosen
         that does not match the resolution of the output device, typically a
         display monitor, the scale of the tile will appear incorrect. The
         default value is 96.
     tile_size {String}:
         The width and height of the cache tiles in pixels. The default is 256
         by 256.For the best balance between performance and manageability,
         avoid
         deviating from widths of 256 or 512.

         * 128 x 128-Tile width and height of 128 pixels.

         * 256 x 256-Tile width and height of 256 pixels.

         * 512 x 512-Tile width and height of 512 pixels.

         * 1024 x 1024-Tile width and height of 1024 pixels.
     tile_format {String}:
         The file format for the tiles in the cache.

         * PNG-Creates PNG format with varying bit depths. The bit depths are
         optimized according to the color variation and transparency values in
         each tile.

         * PNG8-A lossless, 8-bit color, image format that uses an indexed
         color palette and an alpha table. Each pixel stores a value (0 to 255)
         that is used to look up the color in the color palette and the
         transparency in the alpha table. 8-bit PNGs are similar to GIF images
         and provide the best support for a transparent background by most web
         browsers.

         * PNG24-A lossless, three-channel image format that supports large
         color variations (16 million colors) and has limited support for
         transparency. Each pixel contains three 8-bit color channels, and the
         file header contains the single color that represents the transparent
         background. The color representing the transparent background color
         can be set in the application. Versions of Internet Explorer prior to
         version 7 do not support this type of transparency. Caches using PNG24
         are significantly larger than those using PNG8 or JPEG, so will take
         more disk space and require greater bandwidth to serve clients.

         * PNG32-A lossless, four-channel image format that supports large
         color variations (16 million colors) and transparency. Each pixel
         contains three 8-bit color channels and one 8-bit alpha channel that
         represents the level of transparency for each pixel. While the PNG32
         format allows for partially transparent pixels in the range from 0 to
         255, the ArcGIS Server cache generation tool only writes fully
         transparent (0) or fully opaque (255) values in the transparency
         channel. Caches using PNG32 are significantly larger than the other
         supported formats, so will take more disk space and require greater
         bandwidth to serve clients.

         * JPEG-A lossy, three-channel image format that supports large color
         variations (16 million colors) but does not support transparency. Each
         pixel contains three 8-bit color channels. Caches using JPEG provide
         control over output quality and size.

         * MIXED-Creates PNG32 anywhere that transparency is detected (in other
         words, anyplace where the data frame background is visible), but
         creates JPEG for the remaining tiles. This keeps the average file size
         down while providing you with a clean overlay on top of other caches.
         This is the default.

         * LERC-Limited Error Raster Compression (LERC) is an efficient lossy
         compression method recommended for single-band or elevation data with
         a large pixel depth (12 bit to 32 bit). Compresses between 10:1 and
         20:1.
     tile_compression_quality {Long}:
         Enter a value between 1 and 100 for the JPEG or Mixed compression
         quality. The default value is 75.Compression is supported only for
         Mixed and JPEG format. Choosing a
         higher value will result in higher-quality images, but the file sizes
         will be larger. Using a lower value will result in lower-quality
         images with smaller file sizes.
     storage_format {String}:
         Determines the storage format of tiles.

         * COMPACT-Group tiles into large files called bundles. This storage
         format is more efficient in terms of storage and mobility. This is the
         default.

         * EXPLODED-Each tile is stored as an individual file.Note that this
         format cannot be used with tile packages.
     lerc_error {Double}:
         Set the maximum tolerance in pixel values when compressing with LERC.

    OUTPUTS:
     out_tiling_scheme (File):
         The path and file name for the output tiling scheme to be created."""
    ...

@gptooldoc("ImportTileCache_management", None)
def ImportTileCache(
    in_cache_target=...,
    in_cache_source=...,
    scales=...,
    area_of_interest=...,
    overwrite=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportTileCache_management(in_cache_target, in_cache_source, {scales;scales...}, {area_of_interest}, {overwrite})

       Imports tiles from an existing tile cache or a tile package. The
       target cache must have the same tiling scheme, spatial reference, and
       storage format as the source tile cache.

    INPUTS:
     in_cache_target (Raster Layer):
         An existing tile cache to which the tiles will be imported.
     in_cache_source (Raster Layer / File):
         An existing tile cache or a tile package from which the tiles are
         imported.
     scales {Double}:
         A list of scale levels at which tiles will be imported.
     area_of_interest {Feature Set}:
         An area of interest will spatially constrain where tiles are imported
         into the cache.This parameter is useful if you want to import tiles
         for irregularly
         shaped areas.
     overwrite {Boolean}:
         Determines whether the images in the destination cache will be merged
         with the tiles from the originating cache or overwritten by them.

         * MERGE-When the tiles are imported, transparent pixels in the
         originating cache are ignored by default. This results in a merged or
         blended image in the destination cache. This is the default.

         * OVERWRITE-The import replaces all pixels in the area of interest,
         effectively overwriting tiles in the destination cache with tiles from
         the originating cache."""
    ...

@gptooldoc("ManageTileCache_management", None)
def ManageTileCache(
    in_cache_location=...,
    manage_mode=...,
    in_cache_name=...,
    in_datasource=...,
    tiling_scheme=...,
    import_tiling_scheme=...,
    scales=...,
    area_of_interest=...,
    max_cell_size=...,
    min_cached_scale=...,
    max_cached_scale=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ManageTileCache_management(in_cache_location, manage_mode, {in_cache_name}, {in_datasource}, {tiling_scheme}, {import_tiling_scheme}, {scales;scales...}, {area_of_interest}, {max_cell_size}, {min_cached_scale}, {max_cached_scale})

       Creates a tile cache or updates tiles in an existing tile cache. You
       can use this tool to create tiles, replace missing tiles, overwrite
       outdated tiles, and delete tiles.

    INPUTS:
     in_cache_location (Raster Layer / Folder):
         The folder in which the cache dataset is created, the raster layer, or
         the path to an existing tile cache.
     manage_mode (String):
         Specifies the mode that will be used to manage the cache.

         * RECREATE_ALL_TILES-Existing tiles will be replaced and new tiles
         will be added if the extent has changed or if layers have been added
         to a multilayer cache.

         * RECREATE_EMPTY_TILES-Only tiles that are empty will be created.
         Existing tiles will be left unchanged.

         * DELETE_TILES-Tiles will be deleted from the cache. The cache folder
         structure will not be deleted.
     in_cache_name {String}:
         The name of the cache dataset to be created in the cache location.
     in_datasource {Mosaic Layer / Raster Layer / Map}:
         A raster dataset, mosaic dataset, or map file.This parameter is not
         required when the manage_mode parameter is set
         to DELETE_TILES.A map file (.mapx) cannot contain a map service or
         image service.
     tiling_scheme {String}:
         Specifies the tiling scheme that will be used.

         * ARCGISONLINE_SCHEME-The default ArcGIS Online tiling scheme will be
         used.

         * IMPORT_SCHEME-An existing tiling scheme will be imported and used.

         * ARCGISONLINE_ELEVATION_SCHEME-The elevation services tiling scheme
         will be used.

         * WGS84_V2_SCHEME-The WGS84 version 2 tiling scheme will be used.

         * WGS84_V2_ELEVATION_SCHEME-The WGS84 version 2 tiling scheme will be
         used to build a tile cache for elevation data.
     import_tiling_scheme {Image Service / Map Server / File}:
         The path to an existing scheme file (.xml) or to a tiling scheme
         imported from an existing image service or map service.
     scales {Double}:
         The scale levels at which tiles will be created or deleted , depending
         on the value of the manage_mode parameter. The pixel size is based on
         the spatial reference of the tiling scheme.

         * By default, only the values for min_cached_scale and
         max_cached_scale will be used when generating cache.

         * Altering the value of either the min_cached_scale or the
         max_cached_scale parameter will change which scales will be used when
         generating cache.

         * Scales that exist but are not within the range of the
         min_cached_scale or max_cached_scale parameter values will be ignored
         when generating the cache.
     area_of_interest {Feature Set}:
         Defines an area of interest to constrain where tiles will be created
         or deleted.It can be a feature class, or it can be a feature set that
         you
         interactively define.This parameter is useful if you want to manage
         tiles for irregularly
         shaped areas. It's also useful when you want to precache some areas
         and leave less-visited areas uncached.
     max_cell_size {Double}:
         The value that defines the visibility of the data source for which the
         cache will be generated. By default, the value is empty.If the value
         is empty, the following apply:

         * For levels of cache that lie within the visibility ranges of the
         data source, the cache will be generated from the data source.

         * For levels of cache that fall outside the visibility of the data
         source, the cache will be generated from the previous level of cache.
         If the value is greater than zero, the following apply:

         * For levels with cell sizes smaller than or equal to the Maximum
         Source Cell Size (max_cell_size) value, the cache will be generated
         from the data source.

         * For levels with cell sizes greater than the Maximum Source Cell Size
         (max_cell_size) value, the cache will be generated from the previous
         level of cache.
         The unit of the Maximum Source Cell Size value should be the same as
         the unit of the cell size of the source dataset.
     min_cached_scale {Double}:
         The minimum scale at which tiles will be created. This value does not
         have to be the smallest scale in the tiling scheme. The minimum cache
         scale will determine which scales are used when generating cache.
     max_cached_scale {Double}:
         The maximum scale at which tiles will be created. This does not have
         to be the largest scale in the tiling scheme. The maximum cache scale
         will determine which scales are used when generating cache."""
    ...

@gptooldoc("AnalyzeToolboxForVersion_management", None)
def AnalyzeToolboxForVersion(
    in_toolbox=..., version=..., report=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AnalyzeToolboxForVersion_management(in_toolbox, version, {report})

       Analyzes the contents of a toolbox and identifies compatibility issues
       with previous versions of ArcGIS software.

    INPUTS:
     in_toolbox (Toolbox):
         The input toolbox (.tbx or .atbx file) that will be analyzed.The
         Python toolbox format (.pyt file) is not supported as an input.
     version (String):
         Specifies the software version that will be used for toolbox
         compatibility analysis.

         * 10.6.0-ArcGIS Desktop 10.6.0 will be used for toolbox compatibility
         issue analysis.

         * 10.7.0-ArcGIS Desktop 10.7.0 will be used for toolbox compatibility
         issue analysis.

         * 10.8.0-ArcGIS Desktop 10.8.0 will be used for toolbox compatibility
         issue analysis.

         * 10.8.2-ArcGIS Desktop 10.8.2 will be used for toolbox compatibility
         issue analysis.

         * 2.2-ArcGIS Pro 2.2 will be used for toolbox compatibility issue
         analysis.

         * 2.3-ArcGIS Pro 2.3 will be used for toolbox compatibility issue
         analysis.

         * 2.4-ArcGIS Pro 2.4 will be used for toolbox compatibility issue
         analysis.

         * 2.5-ArcGIS Pro 2.5 will be used for toolbox compatibility issue
         analysis.

         * 2.6-ArcGIS Pro 2.6 will be used for toolbox compatibility issue
         analysis.

         * 2.7-ArcGIS Pro 2.7 will be used for toolbox compatibility issue
         analysis.

         * 2.8-ArcGIS Pro 2.8 will be used for toolbox compatibility issue
         analysis.

         * 2.9-ArcGIS Pro 2.9 will be used for toolbox compatibility issue
         analysis.

         * 3.0-ArcGIS Pro 3.0 will be used for toolbox compatibility issue
         analysis.

    OUTPUTS:
     report {File}:
         The text file that will be created containing the compatibility issues
         identified by the analyzers."""
    ...

@gptooldoc("AnalyzeToolsForPro_management", None)
def AnalyzeToolsForPro(
    input=..., report=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AnalyzeToolsForPro_management(input, {report})

       Analyzes Python scripts and custom geoprocessing tools and toolboxes
       for functionality that is not supported in ArcGIS Pro.

    INPUTS:
     input (Toolbox / String / File):
         The input can be a geoprocessing toolbox, Python file, or a tool
         name.If a tool name is specified, the tool will need to be loaded
         first
         using the arcpy.ImportToolbox function to be recognized. Tool names
         should include the toolbox alias.

    OUTPUTS:
     report {File}:
         An output text file that includes all issues."""
    ...

@gptooldoc("SaveToolboxToVersion_management", None)
def SaveToolboxToVersion(
    in_toolbox=...,
    version=...,
    out_toolbox=...,
    missing_tool=...,
    missing_param=...,
    invalid_param_value=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SaveToolboxToVersion_management(in_toolbox, version, out_toolbox, {missing_tool}, {missing_param}, {invalid_param_value})

       Analyzes and saves a toolbox for use with a specific version of ArcGIS
       software.

    INPUTS:
     in_toolbox (Toolbox):
         The input toolbox (.tbx or .atbx file) that will be analyzed and
         saved. The file will not be modified.The Python toolbox format (.pyt
         file) is not supported as an input.
     version (String):
         Specifies the software version that will be used for toolbox
         compatibility issue analysis.

         * 10.6.0-ArcGIS Desktop 10.6.0 will be used for toolbox compatibility
         issue analysis. The output toolbox will be saved to this version.

         * 10.7.0-ArcGIS Desktop 10.7.0 will be used for toolbox compatibility
         issue analysis. The output toolbox will be saved to this version.

         * 10.8.0-ArcGIS Desktop 10.8.0 will be used for toolbox compatibility
         issue analysis. The output toolbox will be saved to this version.

         * 10.8.2-ArcGIS Desktop 10.8.2 will be used for toolbox compatibility
         issue analysis. The output toolbox will be saved to this version.

         * 2.2-ArcGIS Pro 2.2 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.3-ArcGIS Pro 2.3 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.4-ArcGIS Pro 2.4 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.5-ArcGIS Pro 2.5 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.6-ArcGIS Pro 2.6 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.7-ArcGIS Pro 2.7 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.8-ArcGIS Pro 2.8 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 2.9-ArcGIS Pro 2.9 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.

         * 3.0-ArcGIS Pro 3.0 will be used for toolbox compatibility issue
         analysis. The output toolbox will be saved to this version.
     missing_tool {Boolean}:
         Specifies whether an error will be produced if a tool is encountered
         that is not present at the target version.

         * ERROR_ON_MISSING_TOOL-An error will be produced and the output
         toolbox will not be created. This is the default.

         * WARN_ON_MISSING_TOOL-A warning message will be produced and the
         output toolbox will be created. For model tools, the problematic tool
         will be removed from the model, which will require manual editing.
     missing_param {Boolean}:
         Specifies whether an error will be produced if a parameter is
         encountered that is not present at the target version and that
         parameter has a value that is not its default value.

         * ERROR_ON_MISSING_REQUIRED_PARAM-An error will be produced and the
         output toolbox will not be created. This is the default.

         * WARN_ON_MISSING_REQUIRED_PARAM-A warning message will be produced,
         the parameter will be removed from the model, and the output toolbox
         will be created.
     invalid_param_value {Boolean}:
         Specifies whether an error will be produced if a parameter value is
         encountered that is not present in its parameter filter at the target
         version.

         * ERROR_ON_INVALID_PARAM_VALUE-An error will be produced and the
         output toolbox will not be created. This is the default.

         * WARN_ON_INVALID_PARAM_VALUE-A warning message will be produced and
         the output toolbox will be created. The output toolbox will produce an
         error with a value that is not in domain or is invalid.

    OUTPUTS:
     out_toolbox (Toolbox):
         The toolbox that will be created for use with ArcGIS software of the
         specified version parameter value."""
    ...

@gptooldoc("AddFeatureClassToTopology_management", None)
def AddFeatureClassToTopology(
    in_topology=..., in_featureclass=..., xy_rank=..., z_rank=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddFeatureClassToTopology_management(in_topology, in_featureclass, xy_rank, z_rank)

       Adds a feature class to a topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology to which the feature class will participate.
     in_featureclass (Feature Layer):
         The feature class to add to the topology. The feature class must be in
         the same feature dataset as the topology.
     xy_rank (Long):
         The relative degree of positional accuracy associated with vertices of
         features in the feature class versus those in other feature classes
         participating in the topology. The feature class with the highest
         accuracy should get a higher rank (lower number, for example, 1) than
         a feature class which is known to be less accurate.
     z_rank (Long):
         Feature classes that are z-aware have elevation values embedded in
         their geometry for each vertex. By setting a z rank, you can influence
         how vertices with accurate z-values are snapped or clustered with
         vertices that contain less accurate z measurements."""
    ...

@gptooldoc("AddRuleToTopology_management", None)
def AddRuleToTopology(
    in_topology=...,
    rule_type=...,
    in_featureclass=...,
    subtype=...,
    in_featureclass2=...,
    subtype2=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddRuleToTopology_management(in_topology, rule_type, in_featureclass, {subtype}, {in_featureclass2}, {subtype2})

       Adds a rule to a topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology to which the new rule will be added.
     rule_type (String):
         Specifies the topology rule that will be added.

         * Must Not Have Gaps (Area)-There must be no voids within a single
         polygon or between adjacent polygons. All polygons must form a
         continuous surface. An error will always exist on the perimeter of the
         surface. You can either ignore this error or mark it as an exception.
         Use this rule for data that must completely cover an area. For
         example, soil polygons cannot include gaps or form voids; they must
         cover an entire area.

         * Must Not Overlap (Area)-The interior of polygons must not overlap.
         The polygons can share edges or vertices. This rule is used when an
         area cannot belong to two or more polygons. It is useful for modeling
         administrative boundaries, such as ZIP Codes or voting districts, and
         mutually exclusive area classifications, such as land cover or
         landform type.

         * Must Be Covered By Feature Class Of (Area-Area)-A polygon in one
         feature class (or subtype) must share all of its area with polygons in
         another feature class (or subtype). An area in the first feature class
         that is not covered by polygons from the other feature class is an
         error. This rule is used when an area of one type, such as a state,
         should be completely covered by areas of another type, such as
         counties.

         * Must Cover Each Other (Area-Area)-The polygons of one feature class
         (or subtype) must share all of their area with the polygons of another
         feature class (or subtype). Polygons may share edges or vertices. Any
         area defined in either feature class that is not shared with the other
         is an error. This rule is used when two systems of classification are
         used for the same geographic area, and any given point defined in one
         system must also be defined in the other. One such case occurs with
         nested hierarchical datasets, such as census blocks and block groups
         or small watersheds and large drainage basins. The rule can also be
         applied to nonhierarchically related polygon feature classes, such as
         soil type and slope class.

         * Must Be Covered By (Area-Area)-The polygons of one feature class (or
         subtype) must be contained within polygons of another feature class
         (or subtype). Polygons may share edges or vertices. Any area defined
         in the contained feature class must be covered by an area in the
         covering feature class. This rule is used when area features of a
         given type must be located within features of another type. This rule
         is useful when modeling areas that are subsets of a larger surrounding
         area, such as management units within forests or blocks within block
         groups.

         * Must Not Overlap With (Area-Area)-The interior of polygons in one
         feature class (or subtype) must not overlap with the interior of
         polygons in another feature class (or subtype). Polygons of the two
         feature classes can share edges or vertices or be completely
         disjointed. This rule is used when an area cannot belong to two
         separate feature classes. It is useful for combining two mutually
         exclusive systems of area classification, such as zoning and water
         body type, in which areas defined within the zoning class cannot also
         be defined in the water body class and vice versa.

         * Must Be Covered By Boundary Of (Line-Area)-Lines must be covered by
         the boundaries of area features. This is useful for modeling lines,
         such as lot lines, that must coincide with the edge of polygon
         features, such as lots.

         * Must Be Covered By Boundary Of (Point-Area)-Points must fall on the
         boundaries of area features. This is useful when the point features
         help support the boundary system, such as boundary markers, which must
         be found on the edges of certain areas.

         * Must Be Properly Inside (Point-Area)-Points must fall within area
         features. This is useful when the point features are related to
         polygons, such as wells and well pads or address points and parcels.

         * Must Not Overlap (Line)-Lines must not overlap with lines in the
         same feature class (or subtype). This rule is used when line segments
         should not be duplicated, for example, in a stream feature class.
         Lines can cross or intersect but cannot share segments.

         * Must Not Intersect (Line)-Line features from the same feature class
         (or subtype) must not cross or overlap each other. Lines can share
         endpoints. This rule is used for contour lines that should never cross
         each other or when the intersection of lines should only occur at
         endpoints, such as street segments and intersections.

         * Must Not Have Dangles (Line)-A line feature must touch lines from
         the same feature class (or subtype) at both endpoints. An endpoint
         that is not connected to another line is called a dangle. This rule is
         used when line features must form closed loops, such as when they are
         defining the boundaries of polygon features. It can also be used when
         lines typically connect to other lines, as with streets. In this case,
         exceptions can be used when the rule is occasionally violated, as with
         cul-de-sac or dead-end street segments.

         * Must Not Have Pseudo-Nodes (Line)-A line must connect to at least
         two other lines at each endpoint. Lines that connect to one other line
         (or to themselves) are considered to have pseudo nodes. This rule is
         used when line features must form closed loops, such as when they
         define the boundaries of polygons or when line features logically must
         connect to two other line features at each end, as with segments in a
         stream network, with exceptions being marked for the originating ends
         of first-order streams.

         * Must Be Covered By Feature Class Of (Line-Line)-Lines from one
         feature class (or subtype) must be covered by the lines in another
         feature class (or subtype). This is useful for modeling logically
         different but spatially coincident lines, such as routes and streets.
         For example, a bus route feature class must not depart from the
         streets defined in the street feature class.

         * Must Not Overlap With (Line-Line)-A line from one feature class (or
         subtype) must not overlap with line features in another feature class
         (or subtype). This rule is used when line features cannot share the
         same space. For example, roads must not overlap with railroads, or
         depression subtypes of contour lines cannot overlap with other contour
         lines.

         * Must Be Covered By (Point-Line)-Points in one feature class must be
         covered by lines in another feature class. It does not constrain the
         covering portion of the line to be an endpoint. This rule is useful
         for points that fall along a set of lines, such as highway signs along
         highways.

         * Must Be Covered By Endpoint Of (Point-Line)-Points in one feature
         class must be covered by the endpoints of lines in another feature
         class. This rule is similar to the Endpoint Must Be Covered By line
         rule except that, in cases where the rule is violated, it is the point
         feature that is marked as an error rather than the line. Boundary
         corner markers may be constrained to be covered by the endpoints of
         boundary lines.

         * Boundary Must Be Covered By (Area-Line)-Boundaries of polygon
         features must be covered by lines in another feature class. This rule
         is used when area features must have line features that mark the
         boundaries of the areas. This is usually when the areas have one set
         of attributes and their boundaries have other attributes. For example,
         parcels are stored in the geodatabase along with their boundaries.
         Each parcel is defined by one or more line features that store
         information about their length or the date surveyed, and every parcel
         must exactly match its boundaries.

         * Boundary Must Be Covered By Boundary Of (Area-Area)-Boundaries of
         polygon features in one feature class (or subtype) must be covered by
         boundaries of polygon features in another feature class (or subtype).
         This is useful when polygon features in one feature class, such as
         subdivisions, are composed of multiple polygons in another class, such
         as parcels, and the shared boundaries must be aligned.

         * Must Not Self-Overlap (Line)-Line features must not overlap
         themselves. They can cross or touch but must not have coincident
         segments. This rule is useful for features, such as streets, in which
         segments might touch in a loop but the same street must not follow the
         same course twice.

         * Must Not Self-Intersect (Line)-Line features must not cross or
         overlap themselves. This rule is useful for lines, such as contour
         lines, that cannot cross themselves.

         * Must Not Intersect Or Touch Interior (Line)-A line in one feature
         class (or subtype) must only touch other lines of the same feature
         class (or subtype) at endpoints. Any line segment in which features
         overlap or any intersection that is not at an endpoint is an error.
         This rule is useful when lines must only be connected at endpoints,
         such as lot lines, which must split (only connect to the endpoints of)
         back lot lines and cannot overlap each other.

         * Endpoint Must Be Covered By (Line-Point)-The endpoints of line
         features must be covered by point features in another feature class.
         This is useful for modeling cases in which a fitting must connect two
         pipes, or a street intersection must be found at the junction of two
         streets.

         * Contains Point (Area-Point)-A polygon in one feature class must
         contain at least one point from another feature class. Points must be
         within the polygon, not on the boundary. This is useful when every
         polygon must have at least one associated point, such as when parcels
         must have an address point.

         * Must Be Single Part (Line)-Lines must have only one part. This rule
         is useful when line features, such as highways, may not have multiple
         parts.

         * Must Coincide With (Point-Point)-Points in one feature class (or
         subtype) must be coincident with points in another feature class (or
         subtype). This is useful when points must be covered by other points,
         such as transformers that must coincide with power poles in electric
         distribution networks and observation points that must coincide with
         stations.

         * Must Be Disjoint (Point)-Points must be separated spatially from
         other points in the same feature class (or subtype). Any points that
         overlap are errors. This is useful for ensuring that points are not
         coincident or duplicated in the same feature class, such as in layers
         of cities, parcel lot ID points, wells, or street lamp poles.

         * Must Not Intersect With (Line-Line)-Line features from one feature
         class (or subtype) must not cross or overlap lines from another
         feature class (or subtype). Lines can share endpoints. This rule is
         used when lines from two layers should never cross each other or when
         the intersection of lines should only occur at endpoints, such as
         streets and railroads.

         * Must Not Intersect or Touch Interior With (Line-Line)-A line in one
         feature class (or subtype) must only touch other lines of another
         feature class (or subtype) at endpoints. Any line segment in which
         features overlap or any intersection that is not at an endpoint is an
         error. This rule is useful when lines from two layers must only be
         connected at endpoints.

         * Must Be Inside (Line-Area)-A line must be contained within the
         boundary of an area feature. This is useful when lines may partially
         or totally coincide with area boundaries but cannot extend beyond
         polygons, such as state highways that must be inside state borders and
         rivers that must be within watersheds.

         * Contains One Point (Area-Point)-Each polygon must contain one point
         feature and each point feature must fall within a single polygon. This
         is used when there must be a one-to-one correspondence between
         features of a polygon feature class and features of a point feature
         class, such as administrative boundaries and their capital cities.
         Each point must be properly inside exactly one polygon and each
         polygon must properly contain exactly one point. Points must be within
         the polygon, not on the boundary.
     in_featureclass (Feature Layer):
         The input or origin feature class.
     subtype {String}:
         The subtype for the input or origin feature class. Provide the subtype
         description (not the code). If subtypes do not exist on the input
         feature class, or you want the rule to be applied to all subtypes in
         the feature class, leave this parameter blank.
     in_featureclass2 {Feature Layer}:
         The destination feature class for the topology rule.
     subtype2 {String}:
         The subtype for the destination feature class. Provide the subtype
         description (not the code). If subtypes do not exist on the origin
         feature class, or you want the rule to be applied to all subtypes in
         the feature class, leave this parameter blank."""
    ...

@gptooldoc("CreateTopology_management", None)
def CreateTopology(
    in_dataset=..., out_name=..., in_cluster_tolerance=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateTopology_management(in_dataset, out_name, {in_cluster_tolerance})

       Creates a topology. The topology will not contain any feature classes
       or rules.

    INPUTS:
     in_dataset (Feature Dataset):
         The feature dataset in which the topology will be created.
     out_name (String):
         The name of the topology to be created. This name must be unique
         across the entire geodatabase.
     in_cluster_tolerance {Double}:
         The cluster tolerance to be set on the topology. The larger the value,
         the more likely vertices will be to cluster together."""
    ...

@gptooldoc("ExportTopologyErrors_management", None)
def ExportTopologyErrors(
    in_topology=..., out_path=..., out_basename=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportTopologyErrors_management(in_topology, out_path, out_basename)

       Exports the errors and exceptions from a geodatabase topology to the
       target geodatabase. All information associated with the errors and
       exceptions, such as the features referenced by the error or exception,
       is exported. Once the errors and exceptions are exported, the feature
       classes can be accessed using any license level of ArcGIS. The feature
       classes can be used with the Select Layer By Location tool and can be
       shared with other users who do not have access to the topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology from which the errors will be exported.
     out_path (Workspace / Feature Dataset):
         The output workspace in which the feature classes will be created.
     out_basename (String):
         The name to prefix to each output feature class. This allows you to
         specify unique output names when running multiple exports to the same
         workspace. The default is the topology name."""
    ...

@gptooldoc("RemoveFeatureClassFromTopology_management", None)
def RemoveFeatureClassFromTopology(
    in_topology=..., in_featureclass=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveFeatureClassFromTopology_management(in_topology, in_featureclass)

       Removes a feature class from a topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology from which to remove the feature class.
     in_featureclass (String):
         The feature class to remove from the topology."""
    ...

@gptooldoc("RemoveRuleFromTopology_management", None)
def RemoveRuleFromTopology(
    in_topology=..., in_rule=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveRuleFromTopology_management(in_topology, in_rule)

       Removes a rule from a topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology from which to remove a rule.
     in_rule (String):
         The topology rule to remove from the topology."""
    ...

@gptooldoc("SetClusterTolerance_management", None)
def SetClusterTolerance(
    in_topology=..., cluster_tolerance=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetClusterTolerance_management(in_topology, cluster_tolerance)

       Sets the cluster tolerance of a topology.

    INPUTS:
     in_topology (Topology Layer):
         The topology for which you want to change the cluster tolerance.
     cluster_tolerance (Double):
         The value to be set as the cluster tolerance property of the selected
         topology. If you enter a value of zero, the default or minimum cluster
         tolerance will be applied to the topology."""
    ...

@gptooldoc("ValidateTopology_management", None)
def ValidateTopology(
    in_topology=..., visible_extent=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ValidateTopology_management(in_topology, {visible_extent})

       Validates a geodatabase topology.

    INPUTS:
     in_topology (Topology Layer):
         The geodatabase topology to be validated.
     visible_extent {Boolean}:
         Specifies whether the current visible extent of the map or the full
         extent of the topology will be validated. If the tool is run in the
         Python window or in a Python script, the entire extent of the topology
         will be validated regardless of this parameter setting.

         * Full_Extent-The entire extent of the topology will be validated.
         This is the default.

         * Visible_Extent-Only the current visible extent will be validated."""
    ...

@gptooldoc("AddDataToTrajectoryDataset_management", None)
def AddDataToTrajectoryDataset(
    in_trajectory_dataset=...,
    trajectory_type=...,
    input_path=...,
    filter=...,
    sub_folder=...,
    aux_inputs=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddDataToTrajectoryDataset_management(in_trajectory_dataset, trajectory_type, input_path;input_path..., {filter}, {sub_folder}, {aux_inputs;aux_inputs...})

       Adds trajectory data to an existing trajectory dataset.

    INPUTS:
     in_trajectory_dataset (Trajectory Layer):
         The trajectory dataset to which the data will be added.
     trajectory_type (Raster Type):
         Specifies the data type that will be added.

         * Cryosat-2-Cryosat-2 data will be added.

         * ICESat-2-ICESat-2 data will be added.

         * Sentinel-3 SRAL-Sentinel-3 SRAL data will be added.

         * Sentinel-6-Sentinel-6 data will be added.
     input_path (Workspace / File / WCS Coverage / Image Service / Map Server / WMS Map / Raster Catalog / Table View / Raster Layer / Mosaic Layer / Terrain Layer / LAS Dataset Layer / Layer File / WMTS Layer):
         The input files or folder. The inputs can be netCDF or HDF (.nc or
         .hdf files).
     filter {String}:
         Specifies the filter for the input data. The default will be
         determined by the trajectory_type parameter value. Custom filter
         criteria can also be provided. For example, a value of STD_ will
         filter files that start with STD_ in the file name.
     sub_folder {Boolean}:
         Specifies whether data in the input_path subfolders will be searched
         and added.

         * SUBFOLDERS-All subfolders will be searched and the data added. This
         is the default.

         * NO_SUBFOLDERS-Only the top-level folder will be searched and the
         data added.
     aux_inputs {Value Table}:
         The properties that are determined by the trajectory_type parameter
         value. Supported property names are ProductFilter, Frequency,
         PredefinedVariables, and Variables. For a list of supported values
         associated with each property name, see Trajectory type properties."""
    ...

@gptooldoc("CreateTrajectoryDataset_management", None)
def CreateTrajectoryDataset(
    in_workspace=..., in_dataset_name=..., coordinate_system=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateTrajectoryDataset_management(in_workspace, in_dataset_name, coordinate_system)

       Creates an empty trajectory dataset in a geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The geodatabase where the trajectory dataset will be stored.
     in_dataset_name (String):
         The name of the trajectory dataset that will be created.
     coordinate_system (Coordinate System):
         The spatial reference of the trajectory dataset that will be created."""
    ...

@gptooldoc("RepairTrajectoryDatasetPaths_management", None)
def RepairTrajectoryDatasetPaths(
    in_trajectory_dataset=..., paths_list=..., where_clause=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RepairTrajectoryDatasetPaths_management(in_trajectory_dataset, paths_list;paths_list..., {where_clause})

       Repairs paths to source data for a trajectory dataset.

    INPUTS:
     in_trajectory_dataset (Trajectory Layer):
         The input trajectory dataset.
     paths_list (Value Table):
         A list of paths to remap.
     where_clause {SQL Expression}:
         An SQL expression that will limit the repairs to selected items in the
         trajectory dataset."""
    ...

@gptooldoc("AddFieldConflictFilter_management", None)
def AddFieldConflictFilter(
    table=..., fields=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AddFieldConflictFilter_management(table, fields;fields...)

       Adds a field conflict filter for a given field in a geodatabase table
       or feature class.

    INPUTS:
     table (Table View):
         Table or feature class containing the field or fields to which
         conflict filters will be applied.
     fields (Field):
         Field or list of fields that will have conflict filters applied."""
    ...

@gptooldoc("AlterVersion_management", None)
def AlterVersion(
    in_workspace=...,
    in_version=...,
    name=...,
    description=...,
    access=...,
    target_owner=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AlterVersion_management(in_workspace, in_version, {name}, {description}, {access}, {target_owner})

       Alters the properties of a geodatabase version.

    INPUTS:
     in_workspace (Workspace):
         The database connection file to the enterprise, workgroup, or desktop
         geodatabase where the version to be altered is located. The default is
         to use the workspace defined in the Current Workspace environment.For
         branch versioning, use a feature service URL (that is, https://mys
         ite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or
         the feature layer portal item.
     in_version (String):
         The name of the version to be altered. If altering a branch version
         from a database connection connected as the geodatabase administrator,
         the version name must also include the service name, for example,
         myservice.versionowner.versionname.
     name {String}:
         The new name of the version.
     description {String}:
         The new description of the version.
     access {String}:
         Specifies the access permission for the version. If no value is
         specified, the access permission will not be updated.

         * PRIVATE-Only the owner can view the version and modify available
         feature classes.

         * PUBLIC-Any user can view the version and modify available feature
         classes.

         * PROTECTED-Any user can view the version, but only the owner can
         modify available feature classes.
     target_owner {String}:
         The name of the portal user to which the version ownership will be
         transferred. Ensure that the target owner user exists; the tool does
         not check the validity of the owner name specified. This parameter is
         only applicable for branch versions."""
    ...

@gptooldoc("ChangeVersion_management", None)
def ChangeVersion(
    in_features=...,
    version_type=...,
    version_name=...,
    date=...,
    include_participating=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ChangeVersion_management(in_features, version_type, {version_name}, {date}, {include_participating})

       Modifies the workspace of a layer or table view to connect to the
       specified version.

    INPUTS:
     in_features (Feature Layer / Table View / Topology Layer / Parcel Layer / Utility Network Layer / Trace Network Layer):
         The layer or table view that will connect to the specified version.The
         sublayers of a topology layer, parcel layer, utility network
         layer, or trace network layer are not valid inputs.
     version_type (String):
         Specifies the type of version to which the input feature layer will
         connect.

         * TRANSACTIONAL-Connect to a defined state of the database
         (traditional version).

         * HISTORICAL-Connect to a version representing a defined moment in
         time, often specified by a time or historical marker.

         * BRANCH-Connect to a branch version.
     version_name {String}:
         The name of the version to which the input feature layer will connect.
         This parameter is optional if you're using a historical version.
     date {Date}:
         The date of the historical version to which the input feature layer
         will connect.
     include_participating {Boolean}:
         Specifies whether the workspace of participating classes will also
         change.The parameter is only applicable when the input layer is a
         topology
         layer, parcel layer, utility network layer, or trace network layer.

         * INCLUDE-The version of the participating classes of the controller
         dataset will change if they are from the same workspace as the
         controller dataset.

         * EXCLUDE-Only the version of the controller dataset will change."""
    ...

@gptooldoc("CreateVersion_management", None)
def CreateVersion(
    in_workspace=..., parent_version=..., version_name=..., access_permission=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateVersion_management(in_workspace, parent_version, version_name, {access_permission})

       Creates a new version in the specified geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The enterprise geodatabase that contains the parent version and will
         contain the new version.For branch versioning, use a feature service
         URL (that is, https://mys
         ite.mydomain/server/rest/services/ElectricNetwork/FeatureServer).
     parent_version (String):
         The geodatabase, or version of a geodatabase, on which the new version
         will be based.
     version_name (String):
         The name of the version that will be created.
     access_permission {String}:
         Specifies the permission access level for the version to protect it
         from being edited or viewed by users other than the owner.

         * PRIVATE-Only the owner or the geodatabase administrator will be able
         to view and modify the version or versioned data.

         * PUBLIC-Any user will be able to view the version. Any user who has
         been granted read/write (update, insert, and delete) permissions on
         datasets will be able to modify datasets in the version.

         * PROTECTED-Any user will be able to view the version, but only the
         owner or the geodatabase administrator will be able to edit the
         version or datasets in the version."""
    ...

@gptooldoc("CreateVersionedView_management", None)
def CreateVersionedView(
    in_dataset=..., in_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateVersionedView_management(in_dataset, {in_name})

       Creates a versioned view on a table or feature class.

    INPUTS:
     in_dataset (Table View):
         Input table or feature class for which a versioned view will be
         created.
     in_name {String}:
         Name for the versioned view that is created. If nothing is specified
         the output versioned view name is the name of the table or feature
         class with _evw appended to the end."""
    ...

@gptooldoc("DeleteVersion_management", None)
def DeleteVersion(
    in_workspace=..., version_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DeleteVersion_management(in_workspace, version_name)

       Deletes the specified version from the input enterprise, workgroup, or
       desktop geodatabase.

    INPUTS:
     in_workspace (Workspace):
         The database connection file to the enterprise, workgroup, or desktop
         geodatabase containing the version to be deleted. The default is the
         workspace defined in the Current Workspace environment.For branch
         versioning, you can use a feature service URL (that is, htt
         ps://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServe
         r) or a feature layer portal item. You can also delete a branch
         version using a database connection file (connected to a branch
         versioned workspace) when connected as the geodatabase admin user
         (sde).
     version_name (String):
         The name of the version to be deleted.For branch versioning, if the
         input workspace is a database connection
         file, the name of the branch version to delete should be fully
         qualified (for example servicename.portaluser.versionname). If the
         input workspace is a feature service URL, the name of the branch
         version to delete should not include the service name (for example.
         portaluser.versionname)."""
    ...

@gptooldoc("ReconcileVersion_management", None)
def ReconcileVersion(
    in_workspace=...,
    version_name=...,
    target_name=...,
    conflict_definition=...,
    conflict_resolution=...,
    aquired_locks=...,
    abort_if_conflicts=...,
    post=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReconcileVersion_management(in_workspace, version_name, target_name, {conflict_definition}, {conflict_resolution}, {aquired_locks}, {abort_if_conflicts}, {post})

       Reconciles a version against another version in its lineage.

    INPUTS:
     in_workspace (Workspace):
         The enterprise geodatabase containing the reconcilable version. The
         default is to use the workspace defined in the environment.
     version_name (String):
         Name of the Edit Version to be reconciled with the Target Version.
     target_name (String):
         Name of any version in the direct ancestry of the Edit version, such
         as the parent version or the default version.
     conflict_definition {String}:
         Describes the conditions required for a conflict to occur:

         * BY_OBJECT-Any changes to the same row or feature in the parent and
         child versions will conflict during reconcile. This is the default.

         * BY_ATTRIBUTE-Only changes to the same attribute of the same row or
         feature in the parent and child versions will be flagged as a conflict
         during reconcile. Changes to different attributes will not be
         considered a conflict during reconcile.
     conflict_resolution {String}:
         Describes the behavior if a conflict is detected:

         * FAVOR_TARGET_VERSION-For all conflicts, resolves in favor of the
         target version. This is the default.

         * FAVOR_EDIT_VERSION-For all conflicts, resolves in favor of the edit
         version.
     aquired_locks {Boolean}:
         Determines whether feature locks will be acquired.

         * LOCK_ACQUIRED-Acquires locks when there is no intention of posting
         the edit session. This is the default.

         * NO_LOCK_ACQUIRED-No locks are acquired and the edit session will be
         posted to the target version.
     abort_if_conflicts {Boolean}:
         Determines if the reconcile process should be aborted if conflicts are
         found between the target version and the edit version.

         * NO_ABORT-Does not abort the reconcile if conflicts are found. This
         is the default.

         * ABORT_CONFLICTS-Aborts the reconcile if conflicts are found.
     post {Boolean}:
         Posts the current edit session to the reconciled target version.

         * NO_POST-Current edits will not be posted to the target version after
         the reconcile. This is the default.

         * POST-Current edits will be posted to the target version after the
         reconcile."""
    ...

@gptooldoc("ReconcileVersions_management", None)
def ReconcileVersions(
    input_database=...,
    reconcile_mode=...,
    target_version=...,
    edit_versions=...,
    acquire_locks=...,
    abort_if_conflicts=...,
    conflict_definition=...,
    conflict_resolution=...,
    with_post=...,
    with_delete=...,
    out_log=...,
    proceed_if_conflicts_not_reviewed=...,
    reconcile_checkout_versions=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReconcileVersions_management(input_database, reconcile_mode, {target_version}, {edit_versions;edit_versions...}, {acquire_locks}, {abort_if_conflicts}, {conflict_definition}, {conflict_resolution}, {with_post}, {with_delete}, {out_log}, {proceed_if_conflicts_not_reviewed}, {reconcile_checkout_versions})

       Reconciles a version or multiple versions with a target version.

    INPUTS:
     input_database (Workspace):
         The enterprise geodatabase that contains the versions to be
         reconciled. The default is to use the geoprocessing workspace
         environment.For branch versioning, this will be the feature service
         URL (that is,
         https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureSe
         rver).
     reconcile_mode (String):
         Specifies the versions that will be reconciled when the tool is
         executed.If the input is a branch workspace, the only valid option for
         this
         parameter is to reconcile all versions.

         * ALL_VERSIONS-Edit versions will be reconciled with the target
         version. This is the default.

         * BLOCKING_VERSIONS-Versions that are blocking the target version from
         compressing will be reconciled. This option uses the recommended
         reconcile order.
     target_version {String}:
         The name of any version in the direct ancestry of the edit version,
         such as the parent version or the default version.It typically
         contains edits from other versions that you want included
         in the edit version.If the input is a branch workspace, the only valid
         option for this
         parameter is to reconcile with the default version.
     edit_versions {String}:
         The name of the edit version or versions to be reconciled with the
         selected target version. This can be an individual version name or a
         list of version names.
     acquire_locks {Boolean}:
         Specifies whether feature locks will be acquired.

         * LOCK_ACQUIRED-Locks will be acquired during the reconcile process.
         Use this option when the intention is to post edits. It ensures that
         the target version is not modified in the time between the reconcile
         and post operations. This is the default.

         * NO_LOCK_ACQUIRED-Locks will not be acquired during the reconcile
         process. This allows multiple users to reconcile in parallel. Use this
         option when the edit version will not be posted to the target version
         because there is a possibility that the target version may be modified
         in the time between the reconcile and post operations.
     abort_if_conflicts {Boolean}:
         Specifies whether the reconcile process will be aborted if conflicts
         are found between the target version and the edit version during the
         reconcile process.

         * NO_ABORT-The reconcile will not be aborted if conflicts are found.
         This is the default.

         * ABORT_CONFLICTS-The reconcile will be aborted if conflicts are
         found.
     conflict_definition {String}:
         Specifies whether the conditions required for a conflict to occur will
         be defined by object (row) or by attribute (column).

         * BY_OBJECT-Conflicts will be defined by object. Any changes to the
         same row or feature in the parent and child versions will conflict
         during reconcile. This is the default.

         * BY_ATTRIBUTE-Conflicts will be defined by attribute. Only changes to
         the same attribute (column) of the same row or feature in the parent
         and child versions will be flagged as a conflict during reconcile.
         Changes to different attributes will not be considered a conflict
         during reconcile.
     conflict_resolution {String}:
         Specifies the resolution that will be used if a conflict is
         detected.If the input is a branch workspace, the default is to favor
         the edit
         version.

         * FAVOR_TARGET_VERSION-All conflicts will be resolved in favor of the
         target version. This is the default for traditional versioning.

         * FAVOR_EDIT_VERSION-All conflicts will be resolved in favor of the
         edit version. This is the default for branch versioning.
     with_post {Boolean}:
         Specifies whether the current edit session will be posted to the
         reconciled target version.

         * NO_POST-The current edit version will not be posted to the target
         version after the reconcile. This is the default.

         * POST-The current edit version will be posted to the target version
         after the reconcile.
     with_delete {Boolean}:
         Specifies whether the reconciled edit version will be deleted after
         posting. This parameter only applies if the with_post parameter is set
         to POST.

         * DELETE_VERSION-The current edit version that was reconciled will be
         deleted after being posted to the target version.

         * KEEP_VERSION-The current edit version that was reconciled will not
         be deleted. This is the default.
     proceed_if_conflicts_not_reviewed {Boolean}:
         Specifies whether the reconcile will proceed if existing unreviewed
         conflicts are detected before the reconcile process starts. If you
         proceed, existing conflicts from previous sessions will be lost upon
         tool execution. This parameter is only applicable to branch
         versioning.

         * PROCEED-The reconcile process will proceed if existing unreviewed
         conflicts are detected. This is the default.

         * NOT_PROCEED-The reconcile process will not proceed if existing
         unreviewed conflicts are detected.
     reconcile_checkout_versions {Boolean}:
         Specifies whether the reconcile process will include checkout replica
         versions. If you are creating a checkout replica as part of a
         geodatabase replication workflow, an associated version is created in
         the geodatabase. This option allows you to include or remove these
         types of versions from the list of versions to be reconciled. This
         parameter is not applicable for branch versioning.

         * RECONCILE-The reconcile process will include checkout replica
         versions. This is the default.

         * DO_NOT_RECONCILE-The reconcile process will not include checkout
         replica versions.

    OUTPUTS:
     out_log {File}:
         The name and location where the log file will be written. The log file
         is an ASCII file containing the contents of the geoprocessing
         messages."""
    ...

@gptooldoc("RegisterAsVersioned_management", None)
def RegisterAsVersioned(
    in_dataset=..., edit_to_base=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RegisterAsVersioned_management(in_dataset, {edit_to_base})

       Registers an enterprise geodatabase dataset as versioned.

    INPUTS:
     in_dataset (Table View / Feature Dataset):
         The dataset to be registered as versioned.
     edit_to_base {Boolean}:
         Specifies whether edits made to the default version will be moved to
         the base tables. This parameter is not applicable for branch
         versioning.

         * NO_EDITS_TO_BASE-The dataset will not be versioned with the option
         of moving edits to base. This is the default.

         * EDITS_TO_BASE-The dataset will be versioned with the option of
         moving edits to base."""
    ...

@gptooldoc("RemoveFieldConflictFilter_management", None)
def RemoveFieldConflictFilter(
    table=..., fields=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveFieldConflictFilter_management(table, fields;fields...)

       Removes a field conflict filter for a given field in a geodatabase
       table or feature class.

    INPUTS:
     table (Table View):
         Table or feature class containing the field or fields to be removed as
         conflict filters.
     fields (Field):
         Field or list of fields to be removed as conflict filters."""
    ...

@gptooldoc("UnregisterAsVersioned_management", None)
def UnregisterAsVersioned(
    in_dataset=..., keep_edit=..., compress_default=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UnregisterAsVersioned_management(in_dataset, {keep_edit}, {compress_default})

       Unregisters an enterprise geodatabase dataset as versioned.

    INPUTS:
     in_dataset (Table View / Feature Dataset):
         The name of the dataset to be unregistered as versioned.
     keep_edit {Boolean}:
         Specifies whether edits made to the versioned data will be maintained.

         * KEEP_EDIT-If there are existing edits in the delta tables, the tool
         will fail with an error message. Do not use this option if you intend
         to compress your edits from the Default version in the
         compress_default parameter. This is the default.

         * NO_KEEP_EDIT-If there are existing edits in the delta tables, the
         tool will allow deletion of these edits. Use this option if you do
         intend to compress your edits from the Default version in the
         compress_default parameter.
     compress_default {Boolean}:
         Specifies whether edits will be compressed and unused data will be
         removed. This option is ignored if the KEEP_EDIT option is used in the
         keep_edit parameter. This option is only applicable for traditional
         versioned datasets.

         * COMPRESS_DEFAULT-Edits in the Default version will be compressed to
         the base table.

         * NO_COMPRESS_DEFAULT-Any edits remaining in the delta tables will not
         be compressed. This is the default."""
    ...

@gptooldoc("ClearWorkspaceCache_management", None)
def ClearWorkspaceCache(
    in_data=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ClearWorkspaceCache_management({in_data})

       Clears any enterprise geodatabase workspaces from the enterprise
       geodatabase workspace cache.

    INPUTS:
     in_data {Data Element / Layer}:
         The enterprise geodatabase database connection file representing the
         enterprise geodatabase workspace to be removed from the cache. Specify
         the path to the enterprise geodatabase connection file that was used
         in running your geoprocessing tools in order to remove the specific
         enterprise geodatabase workspace from the cache. Passing no input
         parameter will clear all enterprise geodatabase workspaces from the
         cache."""
    ...

@gptooldoc("CreateCloudStorageConnectionFile_management", None)
def CreateCloudStorageConnectionFile(
    out_folder_path=...,
    out_name=...,
    service_provider=...,
    bucket_name=...,
    access_key_id=...,
    secret_access_key=...,
    region=...,
    end_point=...,
    config_options=...,
    folder=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateCloudStorageConnectionFile_management(out_folder_path, out_name, service_provider, bucket_name, {access_key_id}, {secret_access_key}, {region}, {end_point}, {config_options;config_options...}, {folder})

       Creates a connection file for ArcGIS-supported cloud storage. This
       tool allows existing raster geoprocessing tools to write cloud raster
       format (CRF) datasets into the cloud storage bucket or read raster
       datasets (not limited to CRF) stored in the cloud storage as input.

    INPUTS:
     out_folder_path (Folder):
         The folder path where the connection file will be created.
     out_name (String):
         The name of the cloud storage connection file.
     service_provider (String):
         Specifies the cloud storage service provider that will be used.

         * AZURE-The service provider will be Microsoft Azure.

         * AMAZON-The service provider will be Amazon S3.

         * GOOGLE-The service provider will be Google Cloud Storage.

         * ALIBABA-The service provider will be Alibaba Cloud Storage.

         * WEBHDFS-The service provider will be WebHDFS.

         * MINIO-The service provider will be MinIO.

         * AZUREDATALAKE-The service provider will be Microsoft Azure Data
         Lake.

         * OZONE-The service provider will be Ozone.
     bucket_name (String):
         The name of the cloud storage container where the raster dataset will
         be stored. Many cloud providers also call it a bucket.
     access_key_id {String}:
         The access key ID string for the specific cloud storage type. It can
         also be the account name, as is the case with Azure.
     secret_access_key {Encrypted String}:
         The secret access key string to authenticate the connection to cloud
         storage.
     region {String}:
         The region string for the cloud storage. If provided, the value must
         use the format defined by the cloud storage choice. The default is the
         selected cloud provider's default account.
     end_point {String}:
         The service endpoint (URI) of the cloud storage, such as oss-us-
         west-1.aliyuncs.com. If a value is not provided, the default endpoint
         for the selected cloud storage type will be used. The CNAME redirected
         endpoint can also be used if needed.
     config_options {Value Table}:
         The configuration options pertaining to the specific type of cloud
         service. Some services offer options, some do not. You only need to
         set this parameter if you want to turn on the options.

         * Azure and Microsoft Azure Data Lake

         * AZURE_STORAGE_SAS_TOKEN-Specify a shared access signature. Ensure
         that its value is URL encoded and does not contain leading '?' or '&'
         characters. When using this option, the Secret Access Key (Account
         Key) parameter must be empty.

         * AZURE_NO_SIGN_REQUEST-Anonymously connect to buckets (containers)
         that don't require authenticated access. When using this option, the
         Secret Access Key (Account Key) parameter must be empty. The default
         value is False

         * AZURE_STORAGE_CONNECTION_STRING-Specify an Azure Storage connection
         string. This string embeds the account name, key, and endpoint. When
         using this option, the Access Key ID (Account Name) and Secret Access
         Key (Account Key) parameters must be empty.

         * CPL_AZURE_USE_HTTPS-Set to False to use HTTP requests. Some servers
         may be configured to only support HTTPS requests. The default value is
         True.

         * Amazon and MinIO

         * AWS_NO_SIGN_REQUEST-Anonymously connect to buckets (containers) that
         don't require authenticated access. The default value is False.

         * AWS_SESSION_TOKEN-Specify temporary credentials.

         * AWS_DEFAULT_PROFILE-AWS credential profiles are automatically used
         when the access key or ID is missing. This option can be used to
         specify the profile to use.

         * AWS_REQUEST_PAYER-Requester Pays buckets can be accessed by setting
         this option to requester.

         * AWS_Virtual_Hosting-If you use Amazon S3 or S3-compatible cloud
         providers that support only path-style requests, set this option to
         True. It is recommended that you use virtual hosting if it's
         supported. The default value is True.

         * CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE-Some older S3-compatible
         implementations do not support the Bulk Delete operation. Set this
         option to False for these providers. The default value is True.

         * AWS_HTTPS-Set to False to use HTTP requests. Some servers may be
         configured to only support HTTPS requests. The default value is True.

         * Google

         * GS_NO_SIGN_REQUEST-Anonymously connect to buckets (containers) that
         do not require authenticated access. The default value is Tue

         * GS_USER_PROJECT-Requester Pays buckets can be accessed by setting
         OAuth2 keys and a project for billing. Set the project using this
         option and set OAuth2 keys using other options and not HMAC keys as a
         secret access key or ID.

         * GS_OAUTH2_REFRESH_TOKEN-Specify OAuth2 Refresh Access Token. Set
         OAuth2 client credentials using GS_OAUTH2_CLIENT_ID and
         GS_OAUTH2_CLIENT_SECRET.

         * GOOGLE_APPLICATION_CREDENTIALS-Specify Service Account OAuth2
         credentials using a .json file containing a private key and client
         email address.

         * GS_OAUTH2_ PRIVATE_KEY-Specify Service Account OAuth2 credentials
         using a private key string. GS_AUTH2_CLIENT_EMAIL must be set.

         * GS_OAUTH2_ PRIVATE_KEY_FILE-Specify Service Account OAuth2
         credentials using a private key from a file. GS_AUTH2_CLIENT_EMAIL
         must be set.

         * GS_AUTH2_CLIENT_EMAIL-Specify Service Account OAuth2 credentials
         using a client email address.

         * GS_AUTH2_SCOPE-Specify Service Account OAuth2 scope. Valid values
         are https://www.googleapis.com/auth/devstorage.read_write (the
         default) and https://www.googleapis.com/auth/devstorage.read_only.

         * GDAL_HTTP_HEADER_FILE-Specify bearer authentication credentials
         stored in an external file.

         * Alibaba

         * OSS_Virtual_Hosting-If you use Alibaba or S3-compatible cloud
         providers that support only path-style requests, set this option to
         True. It is recommended that you use virtual hosting if it's
         supported. The default value is True.

         * OSS_HTTPS-Set to False to use HTTP requests. Some servers may be
         configured to only support HTTPS requests. The default value is True.

         * WebHDFS

         * WEBHDFS_REPLICATION (integer)-The replication value is used when
         creating a file

         * WEBHDFS_PERMISSION (decimal)-A permission mask is used when creating
         a file.

         If multiple authentication parameters are provided, precedence is as
         follows:

         * Azure-AZURE_STORAGE_CONNECTION_STRING, account name or key,
         AZURE_STORAGE_SAS_TOKEN, AZURE_NO_SIGN_REQUEST, or RBAC.

         * Amazon-AWS_NO_SIGN_REQUEST, access ID or key or AWS_SESSION_TOKEN,
         AWS Credential Profile, or IAM Role.

         * Google-GS_NO_SIGN_REQUEST, access ID or key, GDAL_HTTP_HEADER_FILE,
         (GS_OAUTH2_REFRESH_TOKEN or GS_OAUTH2_CLIENT_ID and
         GS_OAUTH2_CLIENT_SECRET), GOOGLE_APPLICATION_CREDENTIALS,
         (GS_OAUTH2_PRIVATE_KEY or GS_OAUTH2_CLIENT_EMAIL),
         (GS_OAUTH2_PRIVATE_KEY_FILE or GS_OAUTH2_CLIENT_EMAIL), or IAM Role.

         * Ozone

         * AWS_DEFAULT_PROFILE-AWS credential profiles are automatically used
         when the access key or ID is missing. This option can be used to
         specify the profile to use.

         * AWS_Virtual_Hosting-If you use Amazon S3 or S3-compatible cloud
         providers that support only path-style requests, set this option to
         True. It is recommended that you use virtual hosting if it's
         supported. The default value is True.

         * AWS_HTTPS-Set to False to use HTTP requests. Some servers may be
         configured to only support HTTPS requests. The default value is True.

         * CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE-Some older S3-compatible
         implementations do not support the Bulk Delete operation. Set this
         option to False for these providers. The default value is True.

         * x-amz-storage-class-Specify REDUCED_REDUNDANCY for writing to a
         single container ozone as it has a single data node.

         In addition to the provider options listed above, the ARC_DEEP_CRAWL
         option can be used with all the service providers. If True, it is used
         to identify CRFs with no extension and cloud-enabled raster products
         in the cloud. This is operation intensive and it is recommended that
         you set this option to False for faster catalog browsing and crawling.
         The default value is True.Custom token vending services-such as
         Planetary Computer's data
         collection, for example-can be authenticated using the
         ARC_TOKEN_SERVICE_API (URL of the token vendor) and
         ARC_TOKEN_OPTION_NAME (type of token from the service provider)
         provider options.The GDAL_DISABLE_READDIR_ON_OPEN option is available
         with all the
         service providers. To improve the performance of loading cloud-based
         rasters, this option is set to NO by default. If the raster resides in
         a folder that contains more than 30,000 items, set this option to YES.
     folder {String}:
         The folder in the bucket_name parameter value where the raster dataset
         will be stored."""
    ...

@gptooldoc("CreateDatabaseConnection_management", None)
def CreateDatabaseConnection(
    out_folder_path=...,
    out_name=...,
    database_platform=...,
    instance=...,
    account_authentication=...,
    username=...,
    password=...,
    save_user_pass=...,
    database=...,
    schema=...,
    version_type=...,
    version=...,
    date=...,
    auth_type=...,
    project_id=...,
    default_dataset=...,
    refresh_token=...,
    key_file=...,
    role=...,
    warehouse=...,
    advanced_options=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDatabaseConnection_management(out_folder_path, out_name, database_platform, {instance}, {account_authentication}, {username}, {password}, {save_user_pass}, {database}, {schema}, {version_type}, {version}, {date}, {auth_type}, {project_id}, {default_dataset}, {refresh_token}, {key_file}, {role}, {warehouse}, {advanced_options})

       Creates a file that ArcGIS uses to connect to a database or an
       enterprise geodatabase.

    INPUTS:
     out_folder_path (Folder):
         The folder path where the database connection file (.sde) will be
         stored.
     out_name (String):
         The name of the database connection file. The output file will have
         the extension .sde.
     database_platform (String):
         Specifies the database management system platform to which the
         connection will be made. The following are valid options:

         * BIGQUERY-The connection will be made to Google BigQuery.

         * DAMENG-The connection will be made to Dameng.

         * DB2-The connection will be made to IBM Db2 for Linux, UNIX, or
         Windows.

         * ORACLE-The connection will be made to Oracle, Amazon Relational
         Database Service (RDS) for Oracle, or Autonomous Transaction
         Processing.

         * POSTGRESQL-The connection will be made to PostgreSQL, Amazon Aurora
         (PostgreSQL-compatible edition), Amazon Relational Database Service
         (RDS) for PostgreSQL, Google Cloud SQL for PostgreSQL, Microsoft Azure
         Database for PostgreSQL, or Microsoft Azure Cosmos DB for PostgreSQL.

         * REDSHIFT-The connection will be made to Amazon Redshift.

         * SAP HANA-The connection will be made to SAP HANA or SAP HANA Cloud.

         * SNOWFLAKE-The connection will be made to Snowflake.

         * SQL_SERVER-The connection will be made to Microsoft SQL Server,
         Microsoft Azure SQL Database, Microsoft Azure SQL Managed Instance, or
         Amazon Relational Database Service (RDS) for SQL Server, or Google
         Cloud SQL for SQL Server.

         * TERADATA-The connection will be made to Teradata Vantage.
     instance {String}:
         The database server or instance to which the connection will be
         made.The value you specify for the database_platform parameter
         indicates
         the type of database or cloud data warehouse to which the connection
         will be made. The information you provide for the instance parameter
         will vary, depending on the platform you specified.See below for
         information about what to provide for each platform.

         * Dameng-The name of the server where the Dameng database is installed

         * Db2-The name of the cataloged Db2 database

         * Oracle-Either the TNS name or the Oracle Easy Connection string to
         connect to the Oracle database or database service

         * PostgreSQL-The name of the server where PostgreSQL is installed or
         the name of the PostgreSQL database service instance

         * Redshift-The URL for the Redshift server

         * SAP HANA-The Open Database Connectivity (ODBC) data source name for
         the SAP HANA database or database service

         * Snowflake-The URL of the Snowflake server

         * SQL Server-The name of the SQL Server database instance or the name
         of the database service instance

         * Teradata-The ODBC data source name for the Teradata database
     account_authentication {Boolean}:
         Specifies the type of authentication that will be used.

         * DATABASE_AUTH-Database authentication will be used. An
         internal database username and a password will be used to connect to
         the database. You aren't required to type your username and password
         to create a connection; however, if you don't, you will be prompted to
         enter them when a connection is established. If the
         connection file you are creating will provide ArcGIS services
         with access to the database or geodatabase, or if you want to use the
         Catalog search to locate data accessed through this connection file,
         you must include a username and password.

         * OPERATING_SYSTEM_AUTH-Operating system authentication will be used.
         You do not need to type a username and password. The connection will
         be made with the username and password that were used to log in to the
         operating system. If the login used for the operating system is not a
         valid database login, the connection will fail.
     username {String}:
         The database username that will be used for database authentication.
     password {Encrypted String}:
         The database user password that will be used for database
         authentication.
     save_user_pass {Boolean}:
         Specifies whether the username and password will be saved.

         * SAVE_USERNAME-The username and password will be saved in the
         connection file. This is the default. If the connection file you are
         creating will provide ArcGIS services with access to the database,
         geodatabase, or cloud data warehouse, or if you want to use the
         Catalog search to locate data accessed through this connection file,
         you must save the username and password.

         * DO_NOT_SAVE_USERNAME-The username and password will not be saved in
         the connection file. Every time you attempt to connect using the file,
         you will be prompted for the username and password.
     database {String}:
         The name of the database to which the connection will be made. This
         parameter applies to PostgreSQL, Redshift, Snowflake, and SQL Server
         platforms.
     schema {String}:
         The user schema geodatabase to which the connection will be made. This
         option only applies to Oracle databases that contain at least one
         user-schema geodatabase. The default value for this parameter is to
         use the sde schema geodatabase.
     version_type {String}:
         Specifies the type of version to which the connection will be made.

         * TRANSACTIONAL-The connection will be made to a traditional
         transactional version. This option does not apply to
         geodatabases in SAP HANA.

         * HISTORICAL-The connection will be made to an historical marker.

         * POINT_IN_TIME-The connection will be made to a specific point in
         time. If POINT_IN_TIME is used, the version parameter will be ignored.

         * BRANCH-The connection will be made to the default branch version.
         If TRANSACTIONAL or HISTORICAL is used, the date parameter will be
         ignored. If HISTORICAL is used and a name is not provided for the
         version parameter, the default transactional version will be used. If
         POINT_IN_TIME is used and a date is not provided for the date
         parameter, the default transactional version will be used.
     version {String}:
         The geodatabase transactional version or historical marker to which
         the connection will be made. The default option uses the default
         transactional version.If you choose a branch version type, the
         connection is always to the
         default branch version.
     date {Date}:
         The value representing the date and time that will be used to connect
         to the database when working with archive-enabled data. Dates
         can be entered in the following formats:

         * 6/9/2011 4:20:15 PM

         * 6/9/2011 16:20:15

         * 6/9/2011

         * 4:20:15 PM

         * 16:20:15

         * If a time is entered without a date, the default date of December
         30, 1899, will be used.

         * If a date is entered without a time, the default time of 12:00:00 AM
         will be used.
     auth_type {String}:
         Specifies the advanced authentication type that will be used when
         connecting to a cloud data warehouse, Microsoft Azure SQL Database, or
         Azure SQL Managed Instance.

         * AZURE_ACTIVE_DIRECTORY_UNIVERSAL_WITH_MFA-The Azure Active Directory
         username authentication type will be used, but not the password. When
         you connect, a code is sent to you in a text message, email, or MFA
         device, or it can use a fingerprint scan for authentication. This
         second part of the authentication process varies depending on how your
         network and authentication protocols are configured.This option is
         supported for Azure SQL Database and Azure SQL Managed Instance only.

         * AZURE_ACTIVE_DIRECTORY_PASSWORD-The Azure Active Directory username
         and password authentication type using the username and password
         parameters will be used. Usernames can be a maximum of 30
         characters.This option is supported for Azure SQL Database and Azure
         SQL Managed Instance only.

         * SERVICE_AUTHENTICATION-The service authentication type when
         connecting to Google BigQuery will be used. See Google BigQuery
         documentation about authentication for information.

         * STANDARD-The standard authentication type when connecting to Amazon
         Redshift will be used. See the Amazon Redshift ODBC Data Connector
         Installation and Configuration Guide for information about standard
         authentication.

         * USER-An authentication method that requires a username and password
         when connecting to Snowflake will be used.

         * USER_AUTHENTICATION-The user authentication type when connecting to
         Google BigQuery will be used. See Google BigQuery documentation about
         authentication for information.
     project_id {String}:
         The project ID for the Google BigQuery connection.
     default_dataset {String}:
         The default dataset for the Google BigQuery connection.
     refresh_token {Encrypted String}:
         The refresh token value.This parameter is only applicable for Google
         BigQuery connections when
         the advanced authentication type is user authentication.
     key_file {File}:
         The key file value.This parameter is only applicable for Google
         BigQuery connections when
         the advanced authentication type is server authentication.
     role {String}:
         The role value for a cloud data warehouse connection.This parameter is
         only applicable for connections to Snowflake.
     warehouse {String}:
         The warehouse value for the connection.This parameter is only
         applicable for connections to Snowflake.
     advanced_options {String}:
         The advanced options for the connection. This is optional connection
         information that is specific to the cloud data warehouse platform
         (Google BigQuery, Amazon Redshift, or Snowflake) to which you connect.
         Provide advanced options using Option=<value> separated by semicolons.
         For example, option1=value1;option2=value2;. Consult the cloud data
         warehouse documentation for information about optional connection
         options."""
    ...

@gptooldoc("CreateDatabaseConnectionString_management", None)
def CreateDatabaseConnectionString(
    database_platform=...,
    instance=...,
    account_authentication=...,
    username=...,
    password=...,
    database=...,
    object_name=...,
    data_type=...,
    feature_dataset=...,
    schema=...,
    version_type=...,
    version=...,
    date=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateDatabaseConnectionString_management(database_platform, instance, {account_authentication}, {username}, {password}, {database}, {object_name}, {data_type}, {feature_dataset}, {schema}, {version_type}, {version}, {date})

       Creates a connection string that geoprocessing tools can use to
       connect to a database or an enterprise geodatabase.

    INPUTS:
     database_platform (String):
         Specifies the database platform to which the connection will be made.

         * SQL_SERVER-Connect to Microsoft SQL Server or Microsoft Azure SQL
         Database.

         * ORACLE-Connect to Oracle.

         * DB2-Connect to IBM DB2 for Linux, UNIX, or Windows.

         * POSTGRESQL-Connect to PostgreSQL.

         * TERADATA-Connect to Teradata Data Warehouse Appliance.

         * SAP HANA-Connect to SAP HANA.

         * DAMENG-Connect to Dameng.
     instance (String):
         The database server or instance to which the connection will be
         made.This parameter value depends on the Database Platform parameter
         value
         chosen.
     account_authentication {Boolean}:
         Specifies the type of authentication that will be used.

         * DATABASE_AUTH-Database authentication will be used. An
         internal database user name and password are used to connect to the
         database. You aren't required to type your user name and password to
         create a connection; however, if you don't, you will be prompted to
         enter them when a connection is established. This is the default.
         If the connection file you are creating will provide ArcGIS services
         with access to the database or geodatabase, or if you want to use the
         Catalog search to locate data accessed through this connection file,
         you must type a user name and password.

         * OPERATING_SYSTEM_AUTH-Operating system authentication will be used.
         You do not need to type a user name and password. The connection will
         be made with the user name and password that were used to log in to
         the operating system. If the login used for the operating system is
         not a valid geodatabase login, the connection will fail.
     username {String}:
         The database user name that will be used when using database
         authentication.
     password {Encrypted String}:
         The database user password that will be used when using database
         authentication.
     database {String}:
         The name of the database to which you will connect. This parameter
         only applies to PostgreSQL and SQL Server platforms.
     object_name {String}:
         The name of the dataset or object in the database to which the
         connection string will point. This connection string can be used as a
         path to the specified dataset.
     data_type {String}:
         The type of dataset or object referred to in the dataset object name.
         If there are multiple objects with the same name in the database, you
         may need to specify the data type of the object for which you want to
         make a connection string.
     feature_dataset {String}:
         The name of the feature dataset containing the dataset or object for
         which you want to make a connection string. If the dataset is not in a
         feature dataset (for example, if it's at the root of the database), do
         not specify a target feature dataset.
     schema {String}:
         The user schema geodatabase to which you will connect. This option
         only applies to Oracle databases that contain at least one user-schema
         geodatabase. The default value for this parameter is to use the sde
         schema (master) geodatabase.
     version_type {String}:
         Specifies the type of version to which you will connect. This
         parameter only applies when connecting to a geodatabase.

         * TRANSACTIONAL-Connect to a transactional version. If Transactional
         is selected, the The following version will be used parameter will be
         populated with a list of transactional versions, and the Date and Time
         parameter will be inactive. This is the Default.

         * HISTORICAL-Connect to an historical marker. If Historical is
         selected, the The following version will be used parameter will be
         populated with a list of historical markers, and the Date and Time
         parameter will be inactive.

         * POINT_IN_TIME-Connect to a specific point in time. If Point in time
         is selected, the The following version will be used parameter will be
         inactive, and the Date and Time parameter will become active.

         * BRANCH-Connect to the default branch version.
         If Historical is selected and a name is not provided, the default
         transactional version is used. If Point in time is selected and a date
         is not provided in the Date and Time parameter, the default
         transactional version is used.
     version {String}:
         The geodatabase transactional version or historical marker to connect
         to. The default option uses the default transactional version.If you
         choose a branch version type, the connection is always to the
         default branch version.
     date {Date}:
         The value representing the date and time that will be used to connect
         to the database when working with archive-enabled data. Dates
         can be entered in the following formats:

         * 6/9/2011 4:20:15 PM

         * 6/9/2011 16:20:15

         * 6/9/2011

         * 4:20:15 PM

         * 16:20:15

         * If a time is entered without a date, the default date of December
         30, 1899, will be used.

         * If a date is entered without a time, the default time of 12:00:00 AM
         will be used."""
    ...

@gptooldoc("CreateFeatureDataset_management", None)
def CreateFeatureDataset(
    out_dataset_path=..., out_name=..., spatial_reference=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFeatureDataset_management(out_dataset_path, out_name, {spatial_reference})

       Creates a feature dataset in the output location-an existing
       enterprise, file, or mobile geodatabase.

    INPUTS:
     out_dataset_path (Workspace):
         The enterprise, file, or mobile geodatabase in which the output
         feature dataset will be created.
     out_name (String):
         The name of the feature dataset to be created.
     spatial_reference {Spatial Reference}:
         The spatial reference of the output feature dataset. You can
         specify the spatial reference in the following ways:

         * Enter the path to a .prj file, such as C:/workspace/watershed.prj.

         * Reference a feature class or feature dataset whose spatial reference
         you want to apply, such as
         C:/workspace/myproject.gdb/landuse/grassland.

         * Define a spatial reference object prior to using this tool, such as
         sr = arcpy.SpatialReference("C:/data/Africa/Carthage.prj"), which you
         then use as the spatial reference parameter."""
    ...

@gptooldoc("CreateFileGDB_management", None)
def CreateFileGDB(
    out_folder_path=..., out_name=..., out_version=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFileGDB_management(out_folder_path, out_name, {out_version})

       Creates a file geodatabase in a folder.

    INPUTS:
     out_folder_path (Folder):
         The folder where the file geodatabase will be created.
     out_name (String):
         The name of the file geodatabase to be created.
     out_version {String}:
         Specifies the ArcGIS version for the new geodatabase.

         * CURRENT-A geodatabase compatible with the currently installed
         version of ArcGIS will be created. This is the default.

         * 10.0-A geodatabase compatible with ArcGIS version 10 will be
         created.

         * 9.3-A geodatabase compatible with ArcGIS version 9.3 will be
         created.

         * 9.2-A geodatabase compatible with ArcGIS version 9.2 will be
         created."""
    ...

@gptooldoc("CreateFolder_management", None)
def CreateFolder(
    out_folder_path=..., out_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateFolder_management(out_folder_path, out_name)

       Creates a folder in the specified location.

    INPUTS:
     out_folder_path (Folder):
         The disk location where the folder is created.
     out_name (String):
         The folder to be created."""
    ...

@gptooldoc("CreateMobileGDB_management", None)
def CreateMobileGDB(
    out_folder_path=..., out_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateMobileGDB_management(out_folder_path, out_name)

       Creates a mobile geodatabase.

    INPUTS:
     out_folder_path (Folder):
         The folder where the mobile geodatabase will be created.
     out_name (String):
         The name of the mobile geodatabase to be created."""
    ...

@gptooldoc("CreateSQLiteDatabase_management", None)
def CreateSQLiteDatabase(
    out_database_name=..., spatial_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateSQLiteDatabase_management(out_database_name, {spatial_type})

       Creates a GeoPackage or an SQLite database that contains the
       ST_Geometry or SpatiaLite spatial type.

    INPUTS:
     spatial_type {String}:
         Specifies the spatial type that will be installed with the new SQLite
         database or the GeoPackage version that will be created.

         * ST_GEOMETRY-The Esri spatial storage type will be installed. This is
         the default.

         * SPATIALITE-SpatiaLite spatial storage type will be installed.

         * GEOPACKAGE-An OGC GeoPackage 1.3 dataset is created.

         * GEOPACKAGE_1.0-An OGC GeoPackage 1.0 dataset will be created.

         * GEOPACKAGE_1.1-An OGC GeoPackage 1.1 dataset will be created.

         * GEOPACKAGE_1.2-An OGC GeoPackage 1.2.1 dataset will be created.

         * GEOPACKAGE_1.3-An OGC GeoPackage 1.3 dataset will be created.

    OUTPUTS:
     out_database_name (File):
         The location of the SQLite database or GeoPackage to be created and
         the name of the file. The .sqlite extension will be automatically
         assigned if the spatial_type parameter value is ST_GEOMETRY or
         SPATIALITE. If the spatial_type parameter value is GEOPACKAGE, the
         .gpkg extension will be automatically assigned."""
    ...

@gptooldoc("CreateSpatialType_management", None)
def CreateSpatialType(
    input_database=...,
    sde_user_password=...,
    tablespace_name=...,
    st_shape_library_path=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateSpatialType_management(input_database, sde_user_password, {tablespace_name}, {st_shape_library_path})

       Adds the ST_Geometry SQL type, subtypes, and functions to an Oracle or
       a PostgreSQL database. This allows you to use the ST_Geometry SQL type
       to store geometries in a database that does not contain a geodatabase.
       You can also use this tool to upgrade the existing ST_Geometry type,
       subtypes, and functions in an Oracle or a PostgreSQL database.

    INPUTS:
     input_database (Workspace):
         The input_database is the database connection file (.sde) that
         connects to the Oracle or PostgreSQL database. You must connect as a
         database administrator user; in Oracle, you must connect as the sys
         user.
     sde_user_password (Encrypted String):
         The password for the sde database user. If the sde user does not exist
         in the database, it will be created and will use the password you
         provide. The password policy of the underlying database will be
         enforced. If the sde user does exist in the database or database
         cluster, this password must match the existing password.
     tablespace_name {String}:
         The name of a tablespace that will be set as the default tablespace
         for the sde user in Oracle. If the tablespace name does not exist, it
         will be created in the Oracle default storage location. If a
         tablespace with the specified name does exist, it will be set as the
         sde user's default.
     st_shape_library_path {File}:
         The location on the Oracle server where the st_shape library resides."""
    ...

@gptooldoc("ExportXMLWorkspaceDocument_management", None)
def ExportXMLWorkspaceDocument(
    in_data=..., out_file=..., export_type=..., storage_type=..., export_metadata=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportXMLWorkspaceDocument_management(in_data;in_data..., out_file, {export_type}, {storage_type}, {export_metadata})

       Creates a readable XML document of the geodatabase contents.

    INPUTS:
     in_data (Table / Feature Class / Feature Dataset / Raster Dataset / Workspace):
         The input datasets that will be exported and represented in an XML
         workspace document. The input data can be a geodatabase, feature
         dataset, feature class, table, raster, or raster catalog. If there are
         multiple inputs, the inputs must be from the same workspace. Multiple
         input workspaces are not supported.
     export_type {String}:
         Specifies whether the output XML workspace document will contain all
         of the data from the input (table and feature class records, including
         geometry) or only the schema.

         * DATA-The schema and the data will be exported. This is the default.

         * SCHEMA_ONLY-Only the schema will be exported.
     storage_type {String}:
         Specifies how feature geometry will be stored when data is exported
         from a feature class.

         * BINARY-The geometry will be stored in a compressed base64 binary
         format. This binary format will produce a smaller XML workspace
         document. Use this option when the XML workspace document will be read
         by a custom program that uses ArcObjects. This is the default.

         * NORMALIZED-The geometry will be stored in an uncompressed format.
         Using this option will result in a larger file. Use this option when
         the XML workspace document will be read by a custom program that does
         not use ArcObjects.
     export_metadata {Boolean}:
         Specifies whether the metadata will be exported.

         * METADATA-If the input contains metadata, it will be exported. This
         is the default.

         * NO_METADATA-Metadata will not be exported.

    OUTPUTS:
     out_file (File):
         The XML workspace document file that will be created. The output can
         be XML (with an .xml file extension) or compressed XML (with a .zip or
         .z file extension)."""
    ...

@gptooldoc("ImportXMLWorkspaceDocument_management", None)
def ImportXMLWorkspaceDocument(
    target_geodatabase=..., in_file=..., import_type=..., config_keyword=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ImportXMLWorkspaceDocument_management(target_geodatabase, in_file, {import_type}, {config_keyword})

       Imports the contents of an XML workspace document into an existing
       geodatabase.

    INPUTS:
     target_geodatabase (Workspace):
         The existing geodatabase where the contents of the XML workspace
         document will be imported.
     in_file (File):
         The input XML workspace document file containing geodatabase contents
         to be imported. The file can be an .xml file or a compressed .zip or
         .z file containing the .xml file.
     import_type {String}:
         Specifies whether both data (feature class and table records,
         including geometry) and schema will be imported, or only the schema
         will be imported.

         * DATA-Data and schema will be imported. This is the default.

         * SCHEMA_ONLY-Only the schema will be imported.
     config_keyword {String}:
         The geodatabase configuration keyword to be applied if the Target
         Geodatabase parameter value is an enterprise or file geodatabase."""
    ...

@gptooldoc("RefreshExcel_management", None)
def RefreshExcel(
    in_excel_file=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RefreshExcel_management(in_excel_file)

       Refreshes a Microsoft Excel file in ArcGIS Pro.

    INPUTS:
     in_excel_file (File):
         The Excel file that will be refreshed."""
    ...

@gptooldoc("UpdateGeodatabaseConnectionPropertiesToBranch_management", None)
def UpdateGeodatabaseConnectionPropertiesToBranch(
    input_database=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpdateGeodatabaseConnectionPropertiesToBranch_management(input_database)

       Updates an enterprise geodatabase connection to work with branch
       versioning.

    INPUTS:
     input_database (Workspace):
         The input enterprise geodatabase connection to update."""
    ...
