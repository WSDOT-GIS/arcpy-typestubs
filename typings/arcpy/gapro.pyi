"""
This type stub file was generated by pyright.
"""

from arcpy.geoprocessing._base import gptooldoc

r"""GeoAnalytics Desktop tools provide a parallel processing framework for
analysis on a desktop machine using Apache Spark. Through aggregation,
regression, detection, and clustering, you can visualize, understand,
and interact with big data. These tools work with big datasets and
allow you to gain insight into your data through patterns, trends, and
anomalies. The tools are integrated and run in ArcGIS Pro in the same
way as other desktop geoprocessing tools."""
__all__ = [
    "AggregatePoints",
    "CalculateDensity",
    "CalculateField",
    "CalculateMotionStatistics",
    "ClipLayer",
    "CopyDatasetFromBDC",
    "CreateBDC",
    "CreateBuffers",
    "DescribeDataset",
    "DetectIncidents",
    "DissolveBoundaries",
    "DuplicateDatasetFromBDC",
    "FindDwellLocations",
    "FindHotSpots",
    "FindPointClusters",
    "FindSimilarLocations",
    "Forest",
    "GeneralizedLinearRegression",
    "GroupByProximity",
    "JoinFeatures",
    "OverlayLayers",
    "PreviewDatasetFromBDC",
    "ReconstructTracks",
    "RefreshBDC",
    "RemoveDatasetFromBDC",
    "SnapTracks",
    "SummarizeAttributes",
    "SummarizeCenterAndDispersion",
    "SummarizeWithin",
    "TraceProximityEvents",
    "UpdateBDCDatasetProperties",
]
__alias__ = ...

@gptooldoc("CalculateDensity_gapro", None)
def CalculateDensity(
    input_layer=...,
    out_feature_class=...,
    bin_type=...,
    bin_size=...,
    weight=...,
    neighborhood_size=...,
    fields=...,
    area_unit_scale_factor=...,
    time_step_interval=...,
    time_step_repeat=...,
    time_step_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateDensity_gapro(input_layer, out_feature_class, bin_type, bin_size, weight, neighborhood_size, {fields;fields...}, {area_unit_scale_factor}, {time_step_interval}, {time_step_repeat}, {time_step_reference})

       Calculates a magnitude-per-unit area from point features that fall
       within a neighborhood around each cell.

    INPUTS:
     input_layer (Feature Layer):
         The points that will be used to calculate the density.
     bin_type (String):
         Specifies the bin shape that will be used in the analysis.

         * SQUARE-The bin shape will be square. This is the default.

         * HEXAGON-The bin shape will be hexagonal.
     bin_size (Linear Unit):
         The size of the bins used to aggregate input features. When generating
         bins for squares, the number and units specified determine the height
         and length of the square. For hexagons, the number and units specified
         determine the distance between parallel sides.
     weight (String):
         Specifies the weighting that will be applied to the density function.

         * UNIFORM-A magnitude-per-area calculation in which each bin is
         equally weighted will be used. This is the default.

         * KERNEL-A magnitude-per-area calculation with a smoothing algorithm
         applied (kernel) that weights bins closer to the points more heavily
         will be used.
     neighborhood_size (Linear Unit):
         The search radius that will be applied to density calculations.
     fields {Field}:
         One or more fields denoting population values for each feature. The
         population field is the count or quantity to be spread across the
         landscape to create a continuous surface.Values in the population
         field must be numeric. By default, the
         density of the count of input points will always be calculated.
     area_unit_scale_factor {String}:
         Specifies the areal units that will be used for the output density
         values. The default unit is based on the units of the output spatial
         reference.

         * ACRES-The areal units will be international acres.

         * HECTARES-The areal units will be hectares.

         * SQUARE_MILES-The areal units will be square statute miles.

         * SQUARE_KILOMETERS-The areal units will be square kilometers.

         * SQUARE_METERS-The areal units will be square meters.

         * SQUARE_FEET-The areal units will be square feet.

         * SQUARE_YARDS-The areal units will be square yards.

         * SQUARE_MILES_US-The areal units will be square US survey miles.

         * SQUARE_FEET_US-The areal units will be square US survey feet.

         * SQUARE_YARDS_US-The areal units will be square US survey yards.

         * ACRES_US-The areal units will be US survey acres.
     time_step_interval {Time Unit}:
         A value that specifies the duration of the time step. This parameter
         is only available if the input points are time enabled and represent
         an instant in time.Time stepping can only be applied if time is
         enabled on the input.
     time_step_repeat {Time Unit}:
         A value that specifies how often the time-step interval occurs. This
         parameter is only available if the input points are time enabled and
         represent an instant in time.
     time_step_reference {Date}:
         A date that specifies the reference time with which to align the time
         steps. The default is January 1, 1970, at 12:00 a.m. This parameter is
         only available if the input points are time enabled and represent an
         instant in time.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with calculated densities."""
    ...

@gptooldoc("FindHotSpots_gapro", None)
def FindHotSpots(
    point_layer=...,
    out_feature_class=...,
    bin_size=...,
    neighborhood_size=...,
    time_step_interval=...,
    time_step_alignment=...,
    time_step_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FindHotSpots_gapro(point_layer, out_feature_class, {bin_size}, {neighborhood_size}, {time_step_interval}, {time_step_alignment}, {time_step_reference})

       Given a set of features, identifies statistically significant hot
       spots and cold spots using the Getis-Ord Gi* statistic.

    INPUTS:
     point_layer (Feature Layer):
         The point feature class for which hot spot analysis will be performed.
     bin_size {Linear Unit}:
         The distance interval that represents the bin size and units into
         which the point_layer will be aggregated. The distance interval must
         be a linear unit.
     neighborhood_size {Linear Unit}:
         The spatial extent of the analysis neighborhood. This value determines
         which features are analyzed together to assess local clustering.
     time_step_interval {Time Unit}:
         The interval that will be used for the time step. This parameter is
         only used if time is enabled for point_layer.
     time_step_alignment {String}:
         Specifies how time steps will be aligned. This parameter is only
         available if the input points are time enabled and represent an
         instant in time.

         * END_TIME-Time steps will align to the last time event and aggregate
         back in time.

         * START_TIME-Time steps will align to the first time event and
         aggregate forward in time. This is the default.

         * REFERENCE_TIME-Time steps will align to a specified date or time. If
         all points in the input features have a time stamp larger than the
         specified reference time (or it falls exactly on the start time of the
         input features), the time-step interval will begin with that reference
         time and aggregate forward in time (as occurs with the Start time
         alignment). If all points in the input features have a time stamp
         smaller than the specified reference time (or it falls exactly on the
         end time of the input features), the time-step interval will end with
         that reference time and aggregate backward in time (as occurs with the
         End time alignment). If the specified reference time is in the middle
         of the time extent of the data, a time-step interval will be created
         ending with the reference time provided (as occurs with the End time
         alignment); additional intervals will be created both before and after
         the reference time until the full time extent of the data is covered.
     time_step_reference {Date}:
         The time that will be used to align the time steps and time intervals.
         This parameter is only used if time is enabled for point_layer.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class with the z-score and p-value results."""
    ...

@gptooldoc("FindPointClusters_gapro", None)
def FindPointClusters(
    input_points=...,
    out_feature_class=...,
    clustering_method=...,
    minimum_points=...,
    search_distance=...,
    use_time=...,
    search_duration=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FindPointClusters_gapro(input_points, out_feature_class, clustering_method, minimum_points, {search_distance}, {use_time}, {search_duration})

       Finds clusters of point features in surrounding noise based on their
       spatial or spatiotemporal distribution.

    INPUTS:
     input_points (Feature Layer):
         The point feature class containing the point clusters.
     clustering_method (String):
         Specifies the method that will be used to define clusters.

         * DBSCAN-A defined distance will be used to separate dense clusters
         from sparser noise. DBSCAN is the fastest of the clustering methods
         but is only appropriate when there is a clear distance that works well
         to define all clusters that may be present. This results in clusters
         that have similar densities. This is the default.

         * HDBSCAN-Varying distances will be used to separate clusters of
         varying densities from sparser noise. HDBSCAN is the most data-driven
         of the clustering methods and requires the least user input.
     minimum_points (Long):
         This parameter is used differently depending on the clustering method
         chosen as follows:

         * Defined distance (DBSCAN)-Specifies the number of features that must
         be found within a certain distance of a point for that point to start
         to form a cluster. The distance is defined using the Search Distance
         parameter.

         * Self-adjusting (HDBSCAN)-Specifies the number of features
         neighboring each point (including the point) that will be considered
         when estimating density. This number is also the minimum cluster size
         allowed when extracting clusters.
     search_distance {Linear Unit}:
         The maximum distance that will be considered.The Minimum Features per
         Cluster value specified must be found within
         this distance for cluster membership. Individual clusters will be
         separated by at least this distance. If a feature is located farther
         than this distance from the next closest feature in the cluster, it
         will not be included in the cluster.
     use_time {Boolean}:
         Specifies whether or not time will be used to discover clusters with
         DBSCAN.

         * TIME-Spatiotemporal clusters will be found using both a search
         distance and a search duration.

         * NO_TIME-Spatial clusters will be found using a search distance and
         time will be ignored. This is the default.
     search_duration {Time Unit}:
         When searching for cluster members, the specified minimum number of
         points must be found within this time duration to form a cluster.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with the resulting point clusters."""
    ...

@gptooldoc("Forest_gapro", None)
def Forest(
    prediction_type=...,
    in_features=...,
    output_trained_features=...,
    variable_predict=...,
    treat_variable_as_categorical=...,
    explanatory_variables=...,
    features_to_predict=...,
    variable_of_importance=...,
    output_predicted=...,
    explanatory_variable_matching=...,
    number_of_trees=...,
    minimum_leaf_size=...,
    maximum_tree_depth=...,
    sample_size=...,
    random_variables=...,
    percentage_for_validation=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Forest_gapro(prediction_type, in_features, output_trained_features, variable_predict, {treat_variable_as_categorical}, {explanatory_variables;explanatory_variables...}, {features_to_predict}, {variable_of_importance}, {output_predicted}, {explanatory_variable_matching;explanatory_variable_matching...}, {number_of_trees}, {minimum_leaf_size}, {maximum_tree_depth}, {sample_size}, {random_variables}, {percentage_for_validation})

       Creates models and generates predictions using an adaptation of the
       random forest algorithm, which is a supervised machine learning method
       developed by Leo Breiman and Adele Cutler. Predictions can be
       performed for both categorical variables (classification) and
       continuous variables (regression). Explanatory variables can take the
       form of fields in the attribute table of the training features. In
       addition to validation of model performance based on the training
       data, predictions can be made to features.

    INPUTS:
     prediction_type (String):
         Specifies the operation mode of the tool. The tool can be run to train
         a model to only assess performance, predict features, or create a
         prediction surface.

         * TRAIN-A model will be trained, but no predictions will be generated.
         Use this option to assess the accuracy of your model before generating
         predictions. This option will output model diagnostics in the messages
         window and a chart of variable importance. This is the default

         * TRAIN_AND_PREDICT-Predictions or classifications will be generated
         for features. Explanatory variables must be provided for both the
         training features and the features to be predicted. The output of this
         option will be a feature class, model diagnostics in the messages
         window, and an optional table of variable importance.
     in_features (Table View):
         The feature class containing the variable_predict parameter and the
         explanatory training variables fields.
     variable_predict (Field):
         The variable from the in_features parameter containing the values to
         be used to train the model. This field contains known (training)
         values of the variable that will be used to predict at unknown
         locations.
     treat_variable_as_categorical {Boolean}:
         * CATEGORICAL-variable_predict is a categorical variable and the tool
         will perform classification.

         * NUMERIC-variable_predict is continuous and the tool will perform
         regression. This is the default.
     explanatory_variables {Value Table}:
         A list of fields representing the explanatory variables that help
         predict the value or category of variable_predict. Use the
         treat_variable_as_categorical parameter for any variables that
         represent classes or categories (such as land cover or presence or
         absence). Specify the variable as true for any that represent classes
         or categories such as land cover or presence or absence and false if
         the variable is continuous.
     features_to_predict {Table View}:
         A feature layer representing locations where predictions will be made.
         This feature layer must also contain any explanatory variables
         provided as fields that correspond to those used from the training
         data.
     explanatory_variable_matching {Value Table}:
         A list of explanatory_variables specified from in_features on the
         right and their corresponding fields from features_to_predict on the
         left, for example, [["LandCover2000", "LandCover2010"], ["Income",
         "PerCapitaIncome"]].
     number_of_trees {Long}:
         The number of trees to create in the forest model. More trees will
         generally result in more accurate model prediction, but the model will
         take longer to calculate. The default number of trees is 100.
     minimum_leaf_size {Long}:
         The minimum number of observations required to keep a leaf (that is,
         the terminal node on a tree without further splits). The default
         minimum for regression is 5, and the default for classification is 1.
         For very large data, increasing these numbers will decrease the run
         time of the tool.
     maximum_tree_depth {Long}:
         The maximum number of splits that will be made down a tree. Using a
         large maximum depth, more splits will be created, which may increase
         the chances of overfitting the model. The default is data driven and
         depends on the number of trees created and the number of variables
         included.
     sample_size {Long}:
         The percentage of in_features used for each decision tree. The default
         is 100 percent of the data. Samples for each tree are taken randomly
         from two-thirds of the data specified.Each decision tree in the forest
         is created using a random sample or
         subset (approximately two-thirds) of the training data available.
         Using a lower percentage of the input data for each decision tree
         increases the speed of the tool for very large datasets.
     random_variables {Long}:
         The number of explanatory variables used to create each decision
         tree.Each decision tree in the forest is created using a random subset
         of
         the explanatory variables specified. Increasing the number of
         variables used in each decision tree will increase the chances of
         overfitting your model, particularly if there is one or more dominant
         variables. A common practice is to use the square root of the total
         number of explanatory variables if variable_predict is numeric, or
         divide the total number of explanatory variables by 3 if
         variable_predict is categorical.
     percentage_for_validation {Long}:
         The percentage (between 10 percent and 50 percent) of in_features to
         reserve as the test dataset for validation. The model will be trained
         without this random subset of data, and the observed values for those
         features will be compared to the predicted value. The default is 10
         percent.

    OUTPUTS:
     output_trained_features (Feature Class / Table):
         The output feature layer name.
     variable_of_importance {Table}:
         A table containing information describing the importance of each
         explanatory variable to be used in the created model.
     output_predicted {Feature Class / Table}:
         The output feature class that will receive the results of the
         prediction results."""
    ...

@gptooldoc("GeneralizedLinearRegression_gapro", None)
def GeneralizedLinearRegression(
    input_features=...,
    dependent_variable=...,
    model_type=...,
    explanatory_variables=...,
    output_features=...,
    input_features_to_predict=...,
    explanatory_variables_to_match=...,
    dependent_variable_mapping=...,
    output_predicted_features=...,
    coefficient_table=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeneralizedLinearRegression_gapro(input_features, dependent_variable, model_type, explanatory_variables;explanatory_variables..., output_features, {input_features_to_predict}, {explanatory_variables_to_match;explanatory_variables_to_match...}, {dependent_variable_mapping;dependent_variable_mapping...}, {output_predicted_features}, {coefficient_table})

       Performs generalized linear regression (GLR) to generate predictions
       or to model a dependent variable in terms of its relationship to a set
       of explanatory variables. This tool can be used to fit continuous
       (OLS), binary (logistic), and count (Poisson) models.

    INPUTS:
     input_features (Table View):
         The layer containing the dependent and independent variables.
     dependent_variable (Field):
         The numeric field containing the observed values to be modeled.
     model_type (String):
         Specifies the type of data that will be modeled.

         * CONTINUOUS-The dependent_variable value is continuous. The Gaussian
         model will be used, and the tool will perform ordinary least squares
         regression. This is the default.

         * BINARY-The dependent_variable value represents presence or absence.
         This can be either conventional ones and zeroes, or string values
         mapped to zero or ones in the Match Explanatory Variables parameter.
         The Logistic regression model will be used.

         * COUNT-The dependent_variable value is discrete and represents
         events, for example, crime counts, disease incidents, or traffic
         accidents. The Poisson regression model will be used.
     explanatory_variables (Field):
         A list of fields representing independent explanatory variables in the
         regression model.
     input_features_to_predict {Table View}:
         A layer containing features representing locations where estimates
         will be computed. Each feature in this dataset should contain values
         for all the explanatory variables specified. The dependent variable
         for these features will be estimated using the model calibrated for
         the input layer data.
     explanatory_variables_to_match {Value Table}:
         Matches the explanatory variables in the input_features_to_predict
         parameter to corresponding explanatory variables from the
         input_features parameter-for example, [["LandCover2000",
         "LandCover2010"], ["Income", "PerCapitaIncome"]].
     dependent_variable_mapping {Value Table}:
         Two strings representing the values used to map to 0 (absence) and 1
         (presence) for binary regression. By default, 0 and 1 will be used.
         For example, to predict an arrest with field values of Arrest and No
         Arrest, enter No Arrest for False Value (0) and Arrest for True Value
         (1).

    OUTPUTS:
     output_features (Table / Feature Class):
         The name of the feature class that will be created containing the
         dependent variable estimates and residuals.
     output_predicted_features {Table / Feature Class}:
         The output feature class with the dependent variable estimates for
         each input_features_to_predict value.The output feature class with the
         dependent variable estimates for
         each Input Prediction Features value.
     coefficient_table {Table}:
         An output table containing the coefficients from the model fit."""
    ...

@gptooldoc("CalculateMotionStatistics_gapro", None)
def CalculateMotionStatistics(
    input_layer=...,
    out_feature_class=...,
    track_fields=...,
    track_history_window=...,
    motion_statistics=...,
    distance_method=...,
    idle_dist_tolerance=...,
    idle_time_tolerance=...,
    time_boundary_split=...,
    time_boundary_reference=...,
    distance_unit=...,
    duration_unit=...,
    speed_unit=...,
    acceleration_unit=...,
    elevation_unit=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateMotionStatistics_gapro(input_layer, out_feature_class, track_fields;track_fields..., track_history_window, {motion_statistics;motion_statistics...}, {distance_method}, {idle_dist_tolerance}, {idle_time_tolerance}, {time_boundary_split}, {time_boundary_reference}, {distance_unit}, {duration_unit}, {speed_unit}, {acceleration_unit}, {elevation_unit})

       Calculates motion statistics for points in a time-enabled feature
       class.

    INPUTS:
     input_layer (Feature Layer):
         The time-enabled point features on which motion statistics will be
         calculated.
     track_fields (Field):
         One or more fields that will be used to identify distinct entities.
     track_history_window (Long):
         The number of observations (including the current observation) that
         will be used for summary statistics. The default value is 3, which
         means that the summary statistics will be calculated at each point in
         a track using the current observation and the previous two
         observations. This parameter does not affect instantaneous statistics
         or idle classification.
     motion_statistics {String}:
         Specifies the group containing the statistics that will be calculated
         and written to the result. If no value is specified, all statistics
         from all groups will be calculated.

         * DISTANCE-The distance between the current and previous observation
         and the maximum, minimum, average, and total distance in the track
         history window will be calculated.

         * DURATION-The duration between the current and previous observation
         and the maximum, minimum, average, and total duration in the track
         history window will be calculated.

         * SPEED-The speed of travel between the current and previous
         observation and the maximum, minimum, and average speed in the track
         history window will be calculated.

         * ACCELERATION-The acceleration between the current speed and the
         previous speed and the maximum, minimum, and average acceleration in
         the track history window will be calculated.

         * ELEVATION-The current elevation, the elevation change between the
         current and previous observation, and the maximum, minimum, average,
         and total elevation change in the track history window will be
         calculated.

         * SLOPE-The slope between the current and previous observation and the
         maximum, minimum, and average slope in the track history window will
         be calculated.

         * IDLE-A determination as to whether an entity is currently idling
         will be made and the percentage of idle time and total idle time in
         the track history window will be calculated.

         * BEARING-The angle of travel between the previous observation and the
         current observation will be calculated.
     distance_method {String}:
         Specifies the distance measurement method that will be used when
         calculating motion statistics.

         * GEODESIC-Geodesic distance will be used.

         * PLANAR-Planar distance will be used. This is the default.
     idle_dist_tolerance {Linear Unit}:
         The maximum distance that two sequential points in a track can be
         apart and still be considered idle. This parameter is used with the
         idle_time_tolerance parameter to determine whether an entity is
         idling. The idle_dist_tolerance parameter is required if the IDLE
         statistic group is specified for the motion_statistics parameter or if
         statistics in all the groups will be calculated.
     idle_time_tolerance {Time Unit}:
         The minimum duration that two sequential points in a track must be
         near each other to be considered idle. This parameter is used with the
         idle_dist_tolerance parameter to determine whether an entity is
         idling. The idle_time_tolerance parameter is required if the IDLE
         statistic group is specified for the motion_statistics parameter or if
         statistics in all the groups will be calculated.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, starting on January 1, 1980,
         tracks will be split at the beginning of every day. This parameter is
         only available with ArcGIS Enterprise 10.7 and later.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.
     distance_unit {String}:
         Specifies the unit of measure that will be used for distance values in
         the output feature class.

         * METERS-The unit of measure will be meters. This is the default.

         * KILOMETERS-The unit of measure will be kilometers.

         * MILES-The unit of measure will be US survey miles.

         * NAUTICAL_MILES-The unit of measure will be US survey nautical miles.

         * YARDS-The unit of measure will be US survey yards.

         * FEET-The unit of measure will be US survey feet.

         * MILES_INT-The unit of measure will be statute miles.

         * NAUTICAL_MILES_INT-The unit of measure will be international
         nautical miles.

         * YARDS_INT-The unit of measure will be international yards.

         * FEET_INT-The unit of measure will be international feet.
     duration_unit {String}:
         Specifies the unit of measure that will be used for duration values in
         the output feature class.

         * YEARS-The unit of measure will be years.

         * MONTHS-The unit of measure will be months.

         * WEEKS-The unit of measure will be weeks.

         * DAYS-The unit of measure will be days.

         * HOURS-The unit of measure will be hours.

         * MINUTES-The unit of measure will be minutes.

         * SECONDS-The unit of measure will be seconds. This is the default.

         * MILLISECONDS-The unit of measure will be milliseconds.
     speed_unit {String}:
         Specifies the unit of measure that will be used for speed values in
         the output feature class.

         * METERS_PER_SECOND-The unit of measure will be meters per second.
         This is the default.

         * MILES_PER_HOUR-The unit of measure will be miles per hour.

         * KILOMETERS_PER_HOUR-The unit of measure will be kilometers per hour.

         * FEET_PER_SECOND-The unit of measure will be feet per second.

         * NAUTICAL_MILES_PER_HOUR-The unit of measure will be nautical miles
         per hour.
     acceleration_unit {String}:
         Specifies the unit of measure that will be used for acceleration
         values in the output feature class.

         * METERS_PER_SECOND_SQUARED-The unit of measure will be meters per
         second squared. This is the default.

         * FEET_PER_SECOND_SQUARED-The unit of measure will be feet per second
         squared.
     elevation_unit {String}:
         Specifies the unit of measure that will be used for elevation values
         in the output feature class.

         * METERS-The unit of measure will be meters. This is the default.

         * KILOMETERS-The unit of measure will be US survey kilometers.

         * MILES-The unit of measure will be US survey miles.

         * YARDS-The unit of measure will be US survey yards.

         * FEET-The unit of measure will be US survey feet.

         * MILES_INT-The unit of measure will be statute miles.

         * YARDS_INT-The unit of measure will be international yards.

         * FEET_INT-The unit of measure will be international feet.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class or layer containing the points with new
         fields for each motion statistic that was calculated."""
    ...

@gptooldoc("DetectIncidents_gapro", None)
def DetectIncidents(
    input_layer=...,
    output=...,
    track_fields=...,
    start_condition=...,
    end_condition=...,
    output_mode=...,
    time_boundary_split=...,
    time_boundary_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DetectIncidents_gapro(input_layer, output, track_fields;track_fields..., start_condition, {end_condition}, {output_mode}, {time_boundary_split}, {time_boundary_reference})

       Creates a layer that detects features that meet a given condition.

    INPUTS:
     input_layer (Table View):
         The input features that contain potential incidents.
     track_fields (Field):
         A field or fields that will be used to identify unique tracks.
     start_condition (Calculator Expression):
         The condition that will be used to identify incidents. Expressions are
         written in Arcade and can include [+ - * / ] operators and multiple
         fields.
     end_condition {Calculator Expression}:
         The condition that will be used to end incidents. If no end condition
         is specified, incidents will end when the start condition is no longer
         true.
     output_mode {String}:
         Specifies the features that will be returned.

         * ALL_FEATURES-All the input features will be returned. This is the
         default.

         * INCIDENTS-Only features that were found to be incidents will be
         returned.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, and set the time boundary
         reference to January 1, 1980, tracks will be split at the beginning of
         every day.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.

    OUTPUTS:
     output (Feature Class / Table):
         A new output dataset that contains incidents."""
    ...

@gptooldoc("FindDwellLocations_gapro", None)
def FindDwellLocations(
    input_features=...,
    output=...,
    track_fields=...,
    distance_method=...,
    distance_tolerance=...,
    time_tolerance=...,
    output_type=...,
    summary_statistics=...,
    time_boundary_split=...,
    time_boundary_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FindDwellLocations_gapro(input_features, output, track_fields;track_fields..., distance_method, distance_tolerance, time_tolerance, output_type, {summary_statistics;summary_statistics...}, {time_boundary_split}, {time_boundary_reference})

       Finds locations where moving objects have stopped, or dwelled, using
       given time and distance thresholds.

    INPUTS:
     input_features (Feature Layer):
         The point tracks in which dwells will be found. The input must be a
         time-enabled layer with features that represent instants in time.
     track_fields (Field):
         One or more fields that will be used to identify unique tracks.
     distance_method (String):
         Specifies how the distances between dwell features will be calculated.

         * GEODESIC-If the spatial reference can be panned, tracks will cross
         the international date line when appropriate. If the spatial reference
         cannot be panned, tracks will be limited to the coordinate system
         extent and may not wrap.

         * PLANAR-Planar distances will be used.
     distance_tolerance (Linear Unit):
         The maximum distance between points to be considered a single dwell
         location.
     time_tolerance (Time Unit):
         The minimum time duration to be considered a single dwell
         location.Both time and distance are considered when finding dwells.
         The
         Distance Tolerance parameter specifies distance.
     output_type (String):
         Specifies how the dwell features will be output.

         * DWELL_FEATURES-All of the input point features that are part of a
         dwell will be returned.

         * DWELL_MEAN_CENTERS-Points representing the mean centers of each
         dwell group will be returned. This is the default.

         * DWELL_CONVEX_HULLS-Polygons representing the convex hull of each
         dwell group will be returned.

         * ALL_FEATURES-All of the input point features will be returned.
     summary_statistics {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.

         * FIRST-The first value of a specified field in a track.

         * LAST-The last value of a specified field in a track.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, and set the time boundary
         reference to January 1, 1980, tracks will be split at the beginning of
         every day.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.

    OUTPUTS:
     output (Feature Class):
         The output feature class with the resulting dwells."""
    ...

@gptooldoc("FindSimilarLocations_gapro", None)
def FindSimilarLocations(
    input_layer=...,
    search_layer=...,
    output=...,
    analysis_fields=...,
    most_or_least_similar=...,
    match_method=...,
    number_of_results=...,
    append_fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """FindSimilarLocations_gapro(input_layer, search_layer, output, analysis_fields;analysis_fields..., {most_or_least_similar}, {match_method}, {number_of_results}, {append_fields;append_fields...})

       Identifies the candidate features that are most similar or dissimilar
       to one or more input features based on feature attributes.

    INPUTS:
     input_layer (Table View):
         The reference layer (or a selection on a layer) containing the
         features to be matched. The tool searches for other features similar
         to these features. When more than one feature is provided, matching is
         based on attribute averages.
     search_layer (Table View):
         The candidate layer (or a selection on a layer) containing candidate-
         matching features. The tool searches for features most similar (or
         dissimilar) to the input_layer parameter among these candidates.
     analysis_fields (String):
         A list of numeric attributes representing the matching criteria.
     most_or_least_similar {String}:
         Specifies whether the features to be found are most similar or least
         similar to the input_layer parameter.

         * MOST_SIMILAR-Finds the features that are most similar.

         * LEAST_SIMILAR-Finds the features that are least similar.

         * BOTH-Finds the features that are most similar and the features that
         are least similar.
     match_method {String}:
         Specifies whether matches will be based on values or cosine
         relationships.

         * ATTRIBUTE_VALUES-Similarity or dissimilarity will be based on the
         sum of squared standardized attribute value differences for all the
         analysis_fields attributes.

         * ATTRIBUTE_PROFILES-Similarity or dissimilarity will be computed as a
         function of cosine similarity for all the analysis_fields attributes.
     number_of_results {Long}:
         The number of solution matches to be found. Entering zero or a number
         larger than the total number of search_layer features will return
         rankings for all the candidate features, with a maximum of 10,000.
     append_fields {Field}:
         An optional list of attributes to include with the output. You can
         include a name identifier, categorical field, or date field for
         example. These fields are not used to determine similarity; they are
         only included in the output parameter attributes for your reference.
         By default, all fields are added.

    OUTPUTS:
     output (Feature Class / Table):
         The output dataset contains a record for each of the input_layer
         parameter values and for all the solution-matching features found."""
    ...

@gptooldoc("CalculateField_gapro", None)
def CalculateField(
    input_layer=...,
    output=...,
    field_to_calculate=...,
    field_name=...,
    existing_field=...,
    field_type=...,
    expression=...,
    track_aware=...,
    track_fields=...,
    time_boundary_split=...,
    time_boundary_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateField_gapro(input_layer, output, field_to_calculate, {field_name}, {existing_field}, {field_type}, expression, {track_aware}, {track_fields;track_fields...}, {time_boundary_split}, {time_boundary_reference})

       Creates a layer with calculated field values.

    INPUTS:
     input_layer (Table View):
         The input features that will have a field calculated.
     field_to_calculate (String):
         Specifies whether values will be calculated for a newly created field
         or an existing field.

         * NEW_FIELD-Values will be calculated for a newly created field.

         * EXISTING_FIELD-Values will be calculated for an existing field.
     field_name {String}:
         The new field that will have values calculated.
     existing_field {Field}:
         The existing field that will have values calculated.
     field_type {String}:
         Specifies the field type for the calculated field.

         * STRING-The new field will be of type text.

         * INTEGER-The new field will be of type integer.

         * FLOAT-The new field will be of type float.

         * DATE-The new field will be of type date.
     expression (Calculator Expression):
         Calculates values in the field. Expressions are written in Arcade and
         can include operators and multiple fields. Calculated values are
         applied in the units of the spatial reference of the input unless you
         are using a geographic coordinate system, in which case they will be
         in meters.
     track_aware {Boolean}:
         Specifies whether the expression will use a track-aware expression.

         * TRACK_AWARE-The expression will use a track-aware expression, and a
         track field must be specified.

         * NOT_TRACK_AWARE-The expression will not use a track-aware
         expression. This is the default.
     track_fields {Field}:
         One or more fields that will be used to identify unique tracks.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, and set the time boundary
         reference to January 1, 1980, tracks will be split at the beginning of
         every day.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.

    OUTPUTS:
     output (Feature Class / Table):
         A new dataset with calculated fields."""
    ...

@gptooldoc("ClipLayer_gapro", None)
def ClipLayer(
    input_layer=..., clip_layer=..., out_feature_class=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ClipLayer_gapro(input_layer, clip_layer, out_feature_class)

       Extracts input features from within specified polygons.

    INPUTS:
     input_layer (Feature Layer):
         The dataset containing the point, line, or polygon features to be
         clipped.
     clip_layer (Feature Layer):
         The dataset containing the polygon features used to clip the input
         features.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class with clipped features."""
    ...

@gptooldoc("DissolveBoundaries_gapro", None)
def DissolveBoundaries(
    input_layer=...,
    out_feature_class=...,
    multipart=...,
    dissolve_fields=...,
    fields=...,
    summary_fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DissolveBoundaries_gapro(input_layer, out_feature_class, {multipart}, {dissolve_fields}, {fields;fields...}, {summary_fields;summary_fields...})

       Finds polygons that intersect or have the same field values and merges
       them to form a single polygon.

    INPUTS:
     input_layer (Feature Layer):
         The layer containing the polygon features that will be dissolved.
     multipart {Boolean}:
         Specifies whether multipart features will be created in the output
         feature class.

         * MULTI_PART-Multipart features will be created.

         * SINGLE_PART-Multipart features will not be created. Individual
         features will be created for each part instead. This is the default.
     dissolve_fields {Boolean}:
         Specifies whether features with the same field values will be
         dissolved.

         * NO_DISSOLVE_FIELDS-Polygons that share a common border (that is,
         they are adjacent) or polygons that overlap will be dissolved into one
         polygon. This is the default.

         * DISSOLVE_FIELDS-Polygons that have the same field value or values
         will be dissolved.
     fields {Field}:
         The field or fields that will be used to dissolve like features.
         Features with the same value for each field will be dissolved.
     summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * Count-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * Sum-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * Mean-The mean of numeric values. The mean of [0, 2, null] is 1.

         * Min-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * Max-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * Standard Deviation-The standard deviation of a numeric field. The
         standard deviation of [1] is null. The standard deviation of [null,
         1,1,1] is null.

         * Variance-The variance of a numeric field in a track. The variance of
         [1] is null. The variance of [null, 1, 1, 1] is null.

         * Range-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * Any-A sample string from a field of type string.
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with dissolved boundaries."""
    ...

@gptooldoc("OverlayLayers_gapro", None)
def OverlayLayers(
    input_layer=..., overlay_layer=..., out_feature_class=..., overlay_type=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """OverlayLayers_gapro(input_layer, overlay_layer, out_feature_class, overlay_type)

       Overlays the geometries from multiple layers into a single layer.
       Overlay can be used to combine, erase, modify, or update spatial
       features.

    INPUTS:
     input_layer (Feature Layer):
         The point, line, or polygon features that will be overlaid with the
         overlay layer.
     overlay_layer (Feature Layer):
         The features that will be overlaid with the input layer features.
     overlay_type (String):
         Specifies the type of overlay to be performed.

         * INTERSECT-A geometric intersection of the input layers will be
         computed. Features or portions of features that overlap in both the
         input layer and overlay layer will be written to the output layer.
         This is the default.

         * ERASE-Only those features or portions of features in the input layer
         that do not overlap the features in the overlay layer will be written
         to the output.

         * UNION-A geometric union of the input layer and overlay layer will be
         computed. All features and their attributes will be written to the
         layer.

         * IDENTITY-A geometric intersection of the input features and identity
         features will be computed. Features or portions of features that
         overlap in both the input layer and the overlay layer will be written
         to the output layer.

         * SYMMETRICAL_DIFFERENCE-Features or portions of features in the
         input layer and overlay layer that do not overlap will be written to
         the output layer.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with overlaid features."""
    ...

@gptooldoc("AggregatePoints_gapro", None)
def AggregatePoints(
    point_layer=...,
    out_feature_class=...,
    polygon_or_bin=...,
    polygon_layer=...,
    bin_type=...,
    bin_size=...,
    time_step_interval=...,
    time_step_repeat=...,
    time_step_reference=...,
    summary_fields=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AggregatePoints_gapro(point_layer, out_feature_class, polygon_or_bin, {polygon_layer}, {bin_type}, {bin_size}, {time_step_interval}, {time_step_repeat}, {time_step_reference}, {summary_fields;summary_fields...})

       Aggregates points into polygon features or bins. A polygon is returned
       with a count of points as well as optional statistics at all locations
       where points exist.

    INPUTS:
     point_layer (Feature Layer):
         The point features to be aggregated into polygons or bins.
     polygon_or_bin (String):
         Specifies how the point_layer will be aggregated.

         * POLYGON-The point layer will be aggregated into a polygon dataset.

         * BIN-The point layer will be aggregated into square or hexagonal bins
         that are generated when the tool is run.
     polygon_layer {Feature Layer}:
         The polygon features into which the input points will be aggregated.
     bin_type {String}:
         Specifies the bin shape that will be generated to hold the aggregated
         points.

         * SQUARE-Square bins will be generated. in which bin_size represents
         the height of a square. This is the default.

         * HEXAGON-Hexagonal bins will be generated, in which bin_size
         represents the height between two parallel sides.
     bin_size {Linear Unit}:
         The distance interval that represents the bin size and units into
         which the point_layer will be aggregated. The distance interval must
         be a linear unit.
     time_step_interval {Time Unit}:
         A value that specifies the duration of the time step. This parameter
         is only available if the input points are time enabled and represent
         an instant in time.Time stepping can only be applied if time is
         enabled on the input.
     time_step_repeat {Time Unit}:
         A value that specifies how often the time-step interval occurs. This
         parameter is only available if the input points are time enabled and
         represent an instant in time.
     time_step_reference {Date}:
         A date that specifies the reference time with which to align the time
         steps. The default is January 1, 1970, at 12:00 a.m. This parameter is
         only available if the input points are time enabled and represent an
         instant in time.
     summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with the aggregated polygon results."""
    ...

@gptooldoc("DescribeDataset_gapro", None)
def DescribeDataset(
    input_layer=..., output=..., sample_features=..., sample_layer=..., extent_layer=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DescribeDataset_gapro(input_layer, output, {sample_features}, {sample_layer}, {extent_layer})

       Summarizes features into calculated field statistics, sample features,
       and extent boundaries.

    INPUTS:
     input_layer (Table View):
         The point, line, polygon, or tabular features to be described.
     sample_features {Long}:
         The number of features that will be included in the output sample
         layer. No sample is returned if you select 0 features or don't provide
         a number. By default, no sample layer is returned.

    OUTPUTS:
     output (Table):
         A new table with the summary information.
     sample_layer {Table / Feature Class}:
         A new feature class with a sample of the input data.
     extent_layer {Feature Class}:
         A new feature class with the spatial and temporal extent of the input
         data."""
    ...

@gptooldoc("JoinFeatures_gapro", None)
def JoinFeatures(
    target_layer=...,
    join_layer=...,
    output=...,
    join_operation=...,
    spatial_relationship=...,
    spatial_near_distance=...,
    temporal_relationship=...,
    temporal_near_distance=...,
    attribute_relationship=...,
    summary_fields=...,
    join_condition=...,
    keep_all_target_features=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """JoinFeatures_gapro(target_layer, join_layer, output, join_operation, {spatial_relationship}, {spatial_near_distance}, {temporal_relationship}, {temporal_near_distance}, {attribute_relationship;attribute_relationship...}, {summary_fields;summary_fields...}, {join_condition}, {keep_all_target_features})

       Joins attributes from one layer to another based on spatial, temporal,
       or attribute relationships, or a combination of those relationships.

    INPUTS:
     target_layer (Table View):
         Contains the target features. The attributes from the target features
         and the attributes from the joined features will be transferred to the
         output.
     join_layer (Table View):
         Contains the join features. The attributes from the join features will
         be joined to the attributes of the target features. See the
         explanation of the Join Operation (join_operation in Python) parameter
         for details about how the aggregation of joined attributes are
         affected by the type of join operation.
     join_operation (String):
         Specifies how joins between the target_layer values and join_layer
         values will be handled in the output feature if multiple join features
         are found that have the same spatial relationship with a single target
         feature.

         * JOIN_ONE_TO_ONE-The attributes from the multiple join features will
         be aggregated. For example, if a point target feature is found in two
         separate polygon join features, the attributes from the two polygons
         will be aggregated before being transferred to the output point
         feature class. If one polygon has an attribute value of 3 and the
         other has a value of 7, and the summary statistic sum is specified for
         that field, the aggregated value in the output feature class will be
         10. This is the default, and only the count statistic is returned.

         * JOIN_ONE_TO_MANY-The output feature class will contain multiple
         copies (records) of the target feature. For example, if a single-point
         target feature is found in two separate polygon join features, the
         output feature class will contain two copies of the target feature:
         one record with the attributes of one polygon and another record with
         the attributes of the other polygon. There are no summary statistics
         available with this method.
     spatial_relationship {String}:
         Specifies the criteria that will be used to spatially join features.

         * INTERSECTS-The features in the join features will be matched if they
         intersect a target feature. This is the default.

         * EQUALS-The features in the join features will be matched if they
         are the same geometry as a target feature.

         * NEAR-The features in the join features will be matched if they are
         within a specified distance of a target feature. The distance is
         measured using planar distance. Specify a distance in the
         spatial_near_distance parameter.

         * NEAR_GEODESIC-The features in the join features will be matched if
         they are within a specified distance of a target feature. The distance
         is measured geodesically. Specify a distance in the
         spatial_near_distance parameter.

         * CONTAINS-The features in the join features will be matched if a
         target feature contains them. The target features must be polygons or
         polylines. The join features can only be polygons when the target
         features are also polygons. A polygon can contain any feature type. A
         polyline can contain only polylines and points. A point cannot contain
         any feature, not even a point. If the join feature is entirely on the
         boundary of the target feature (no part is properly inside or
         outside), the feature will not be matched.

         * WITHIN-The features in the join features will be matched if a target
         feature is within them. It is the opposite of the contains
         relationship. For this option, the target features can only be
         polygons when the join features are also polygons. A point can be a
         join feature only if a point is also a target feature. If the entirety
         of the feature in the join features is on the boundary of the target
         feature, the feature will not be matched.

         * TOUCHES-The features in the join features will be matched if they
         have a boundary that touches a target feature. When the target and
         join features are lines or polygons, the boundary of the join feature
         can only touch the boundary of the target feature, and no part of the
         join feature can cross the boundary of the target feature.

         * CROSSES-The features in the join features will be matched if a
         target feature is crossed by their outline. The join and target
         features must be lines or polygons. If polygons are used for the join
         or target features, the polygon's boundary (line) will be used. Lines
         that cross at a point will be matched, but lines that share a line
         segment will not.

         * OVERLAPS-The features in the join features will be matched if they
         overlap a target feature.
     spatial_near_distance {Linear Unit}:
         The distance from a target feature within which join features will be
         considered for the spatial join. A search radius is only valid when
         the spatial_relationship parameter value is NEAR or NEAR_GEODESIC.
     temporal_relationship {String}:
         Specifies the time criteria that will be used to match features.

         * MEETS-When a target time interval end is equal to the join time
         interval start, the target time meets the join time.

         * MET_BY-When a target time interval start is equal to the join time
         interval end, the target time is met by the join time.

         * OVERLAPS-When a target time interval starts and ends before the
         start and end of the join time interval, the target time overlaps the
         join time.

         * OVERLAPPED_BY-When a target time interval starts and ends after the
         start and end time of the join time interval, the target time is
         overlapped by the join time.

         * DURING-When a target time occurs between the start and end of the
         join time interval, the target time is during the join time.

         * CONTAINS-When a join feature time occurs between the start and end
         of the target time interval, the target time contains the join time.

         * EQUALS-Two times are considered equal if their instants or intervals
         are identical.

         * FINISHES-When a target time ends at the same time as a join time,
         and the target time started after the join time, the target time
         finishes the join time.

         * FINISHED_BY-When a join feature time ends at the same time as a
         target time, and the join time started after the target time, the
         target time is finished by the join time.

         * STARTS-When a target time starts at the same time as the join time
         interval and ends before the join time interval ends, the target time
         starts the join time.

         * STARTED_BY-When a target interval time starts at the same time as
         the join time and ends after the join time, the target time is started
         by the join time.

         * INTERSECTS-When any part of a target time occurs at the same time as
         the join time, the target time intersects the join time.

         * NEAR-When a target time is within a specified range of time from the
         join time, the target time is near the join time.

         * NEAR_BEFORE-When a target time is before the join time but within a
         specified range of time from the join time, the target time is near
         before the join time.

         * NEAR_AFTER-When a target time is after the join time but within a
         specified range of time from the join time, the target time is near
         after the join time.
     temporal_near_distance {Time Unit}:
         The distance in time from a target feature within which join features
         will be considered for the spatial join. A time is only valid when the
         temporal_relationship parameter value is NEAR, NEAR_BEFORE, or
         NEAR_AFTER and both feature are time enabled.
     attribute_relationship {Value Table}:
         Joins features based on values in an attribute field. Specify
         the attribute field from the target layer that matches an attribute
         field from the join layer.

         * Target Field-An attribute field from the target layer containing
         values to match.

         * Join Field-An attribute field from the join layer containing values
         to match.
     summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.
     join_condition {String}:
         Applies a condition to specified fields. Only features with fields
         that meet these conditions will be joined.For example, you could apply
         a join condition to features in which the
         HealthSpending attribute in the join layer is more than 20 percent of
         the Income attribute in the target layer. Use an Arcade expression
         such as $join["HealthSpending"] > $target["Income"] * .2.
     keep_all_target_features {Boolean}:
         Specifies whether all target features will be maintained in the output
         feature class (known as a left outer join) or only those that have the
         specified relationships with the join features (inner join).

         * KEEP_ALL-All target features will be maintained in the output (left
         outer join).

         * KEEP_COMMON-Only those target features that have the specified
         relationships will be maintained in the output feature class (inner
         join). This is the default.

    OUTPUTS:
     output (Feature Class / Table):
         The new feature class containing the target layer features with joined
         features."""
    ...

@gptooldoc("ReconstructTracks_gapro", None)
def ReconstructTracks(
    input_layer=...,
    out_feature_class=...,
    track_fields=...,
    method=...,
    buffer_type=...,
    buffer_field=...,
    buffer_expression=...,
    time_split=...,
    distance_split=...,
    time_boundary_split=...,
    time_boundary_reference=...,
    summary_fields=...,
    split_expression=...,
    split_type=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ReconstructTracks_gapro(input_layer, out_feature_class, track_fields;track_fields..., method, {buffer_type}, {buffer_field}, {buffer_expression}, {time_split}, {distance_split}, {time_boundary_split}, {time_boundary_reference}, {summary_fields;summary_fields...}, {split_expression}, {split_type})

       Creates line or polygon tracks from time-enabled input data.

    INPUTS:
     input_layer (Feature Layer):
         The points or polygons to be reconstructed into tracks. The input must
         be a time-enabled layer that represents an instant in time.
     track_fields (Field):
         One or more fields that will be used to identify unique tracks.
     method (String):
         Specifies the criteria that will be used to reconstruct tracks. If a
         buffer is used, the method parameter determines the type of buffer.

         * GEODESIC-If the spatial reference can be panned, tracks will cross
         the date line when appropriate. If the spatial reference cannot be
         panned, tracks will be limited to the coordinate system extent and may
         not wrap.

         * PLANAR-The tracks will not cross the date line.
     buffer_type {String}:
         Specifies how the buffer distance will be defined.

         * FIELD-A single field will be used to define the buffer distance.

         * EXPRESSION-An equation using fields and mathematical operators will
         be used to define the buffer distance.
     buffer_field {Field}:
         The field that will be used to buffer the input features. Field values
         are applied in the units of the spatial reference of the input unless
         you are using a geographic coordinate system, in which case they will
         be in meters.
     buffer_expression {Calculator Expression}:
         The expression that will be used to buffer input features. Fields must
         be numeric, and the expression can include [+ - * / ] operators and
         multiple fields. Calculated values are applied in the units of the
         spatial reference of the input unless you are using a geographic
         coordinate system, in which case they will be in meters.Use Arcade
         expressions such as as_kilometers($feature.distance) * 2 +
         as_meters(15).
     time_split {Time Unit}:
         Features that are farther apart in time than the time-split duration
         will be split into separate tracks.
     distance_split {Linear Unit}:
         Features that are farther apart in distance than the distance split
         value will be split into separate tracks.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, and set the time boundary
         reference to January 1, 1980, tracks will be split at the beginning of
         every day.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.
     summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.

         * FIRST-The first value of a specified field in a track.

         * LAST-The last value of a specified field in a track.
     split_expression {Calculator Expression}:
         An expression that splits tracks based on values, geometry, or time
         values. Expressions that validate to true will be split.
     split_type {String}:
         Specifies how the track segment between two features is created when a
         track is split. The split type is applied to split expressions,
         distance splits, and time splits.

         * GAP-No segment is created between the two features. This is the
         default.

         * FINISH_LAST-A segment is created between the two features that ends
         after the split.

         * START_NEXT-A segment is created between the two features that ends
         before the split.

    OUTPUTS:
     out_feature_class (Feature Class):
         A new feature class with the resulting tracks."""
    ...

@gptooldoc("SummarizeAttributes_gapro", None)
def SummarizeAttributes(
    input_layer=...,
    out_table=...,
    fields=...,
    summary_fields=...,
    time_step_interval=...,
    time_step_repeat=...,
    time_step_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SummarizeAttributes_gapro(input_layer, out_table, {fields;fields...}, {summary_fields;summary_fields...}, {time_step_interval}, {time_step_repeat}, {time_step_reference})

       Calculates summary statistics for fields in a feature class.

    INPUTS:
     input_layer (Table View):
         The point, polyline, or polygon layer to be summarized.
     fields {Field}:
         A field or fields used to summarize similar features. For example, if
         you choose a single field called PropertyType with the values of
         commercial and residential, all of the fields with the value
         residential fields will be summarized together, with summary
         statistics calculated, and all of the fields with the value commercial
         will be summarized together. This example will results in two rows in
         the output, one for commercial, and one for residential summary
         values.You can optionally select no fields and summarize all features
         in a
         single summary result.
     summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.
     time_step_interval {Time Unit}:
         A value that specifies the duration of the time step. This parameter
         is only available if the input points are time enabled and represent
         an instant in time.Time stepping can only be applied if time is
         enabled on the input.
     time_step_repeat {Time Unit}:
         A value that specifies how often the time-step interval occurs. This
         parameter is only available if the input points are time enabled and
         represent an instant in time.
     time_step_reference {Date}:
         A date that specifies the reference time with which to align the time
         steps. The default is January 1, 1970, at 12:00 a.m. This parameter is
         only available if the input points are time enabled and represent an
         instant in time.

    OUTPUTS:
     out_table (Table):
         A new table with the summarized attributes."""
    ...

@gptooldoc("SummarizeCenterAndDispersion_gapro", None)
def SummarizeCenterAndDispersion(
    input_layer=...,
    out_central_feature=...,
    out_mean_center=...,
    out_median_center=...,
    out_ellipse=...,
    ellipse_size=...,
    weight_field=...,
    group_by_field=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SummarizeCenterAndDispersion_gapro(input_layer, {out_central_feature}, {out_mean_center}, {out_median_center}, {out_ellipse}, {ellipse_size}, {weight_field}, {group_by_field})

       Finds central features and directional distributions and calculates
       mean and median locations from the input.

    INPUTS:
     input_layer (Feature Layer):
         The point, line, or polygon layer to be summarized.
     ellipse_size {String}:
         Specifies the size of output ellipses in standard deviations.

         * 1_STANDARD_DEVIATION-Output ellipses will cover one standard
         deviation of the input features. This is the default.

         * 2_STANDARD_DEVIATIONS-Output ellipses will cover two standard
         deviations of the input features.

         * 3_STANDARD_DEVIATIONS-Output ellipses will cover three standard
         deviations of the input features.
     weight_field {Field}:
         A numeric field used to weight locations according to their relative
         importance. This applies to all summary types.
     group_by_field {Field}:
         The field used to group similar features. This applies to all summary
         types. For example, if you choose a field named PlantType that
         contains values of tree, bush, and grass, all of the features with the
         value tree will be analyzed for their own center or dispersion. This
         example will result in three features, one for each group of tree,
         bush, and grass.

    OUTPUTS:
     out_central_feature {Feature Class}:
         The output feature class that will contain the most centrally located
         feature in the input layer.
     out_mean_center {Feature Class}:
         The output point feature class that will contain features representing
         the mean centers of the input layer.
     out_median_center {Feature Class}:
         The output point feature class that will contain features representing
         the median centers of the input layer.
     out_ellipse {Feature Class}:
         The output polygon feature class that will contain the directional
         ellipse representation of the input layer."""
    ...

@gptooldoc("SummarizeWithin_gapro", None)
def SummarizeWithin(
    summarized_layer=...,
    out_feature_class=...,
    polygon_or_bin=...,
    bin_type=...,
    bin_size=...,
    summary_polygons=...,
    sum_shape=...,
    shape_units=...,
    standard_summary_fields=...,
    weighted_summary_fields=...,
    group_by_field=...,
    add_minority_majority=...,
    add_percentages=...,
    group_by_summary=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SummarizeWithin_gapro(summarized_layer, out_feature_class, {polygon_or_bin}, {bin_type}, {bin_size}, {summary_polygons}, {sum_shape}, {shape_units}, {standard_summary_fields;standard_summary_fields...}, {weighted_summary_fields;weighted_summary_fields...}, {group_by_field}, {add_minority_majority}, {add_percentages}, {group_by_summary})

       Overlays a polygon layer with another layer to summarize the number of
       points, length of the lines, or area of the polygons within each
       polygon and calculates attribute field statistics for those features
       within the polygons.

    INPUTS:
     summarized_layer (Feature Layer):
         The point, line, or polygon features that will be summarized by either
         polygons or bins.
     polygon_or_bin {String}:
         Specifies whether the summarized_layer value will be summarized by
         polygons or bins.

         * POLYGON-The summarized layer will be aggregated into a polygon
         dataset.

         * BIN-The summarized layer will be aggregated into square or hexagonal
         bins.
     bin_type {String}:
         Specifies the bin shape that will be generated to summarize features.

         * SQUARE-The bin_size value represents the height of a square. This is
         the default.

         * HEXAGON-The bin_size value represents the height between two
         parallel sides.
     bin_size {Linear Unit}:
         The distance interval that represents the bin size and units by which
         the input features will be summarized.
     summary_polygons {Feature Layer}:
         The polygons that will be used to summarize the features in the input
         summarized layer.
     sum_shape {Boolean}:
         Specifies whether the length of lines or area of polygons within the
         summary layer (polygon or bin) will be calculated. The count of
         points, lines, and polygons intersecting the summary shape will always
         be included.

         * ADD_SUMMARY-Summary shape values will be calculated. This is the
         default.

         * NO_SUMMARY-Summary shape values will not be calculated.
     shape_units {String}:
         Specifies the unit of measurement that will be used to calculate shape
         summary attributes. If the input summarized_layer value is points, no
         shape unit is necessary, since only the count of points within each
         input polygon is added. If the input summary features are lines,
         specify a linear unit. If the input summary features are polygons,
         specify an areal unit.

         * METERS-The shape units will be meters.

         * KILOMETERS-The shape units will be kilometers.

         * FEET-The shape units will be US survey feet.

         * YARDS-The shape units will be US survey yards.

         * MILES-The shape units will be US survey miles.

         * NAUTICAL_MILES-The shape units will be US survey nautical miles.

         * FEET_INT-The shape units will be international feet.

         * YARDS_INT-The shape units will be international yards.

         * MILES_INT-The shape units will be statute miles.

         * NAUTICAL_MILES_INT-The shape units will be international nautical
         miles.

         * ACRES-The shape units will be international acres.

         * HECTARES-The shape units will be hectares.

         * SQUARE_METERS-The shape units will be square meters.

         * SQUARE_KILOMETERS-The shape units will be square kilometers.

         * SQUARE_FEET-The shape units will be square international feet.

         * SQUARE_YARDS-The shape units will be square international yards.

         * SQUARE_MILES-The shape units will be square statute miles.

         * SQUARE_FEET_US-The shape units will be square US survey feet.

         * SQUARE_YARDS_US-The shape units will be square US survey yards.

         * SQUARE_MILES_US-The shape units will be square US survey miles.

         * ACRES_US-The shape units will be US survey acres.
     standard_summary_fields {Value Table}:
         The statistics that will be calculated on specified fields.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.
         Specifies whether a field represents a count or a rate.

         * COUNT-For line and polygon layers, the summarized field values will
         be proportioned by the percentage of the summarized features that
         intersect the summary polygons prior to calculating statistics. Values
         will not be proportioned for point layers.

         * RATE-The summarized field values will never be proportioned. The raw
         field values will be used to calculate statistics.
     weighted_summary_fields {Value Table}:
         Specifies the weighted statistics that will be calculated on specified
         fields.

         * MEAN-The weighted mean of each field will be calculated in which the
         weight applied is the proportion of the summarized layer within the
         polygons.

         * STDDEV-The weighted standard deviation of each field will be
         calculated in which the weight applied is the proportion of the
         summarized layer within the polygons.

         * VAR-The weighted variance of each field will be calculated in which
         the weight applied is the proportion of the summarized layer within
         the polygons.
         Specifies whether a field represents a count or a rate.

         * Count-The summarized field values will be proportioned by the
         percentage of the summarized features that intersect the summary
         polygons prior to calculating statistics.

         * Rate-The summarized field values will never be proportioned. The raw
         field values will be used to calculate statistics.
     group_by_field {Field}:
         A field from the input summary features that will be used to calculate
         statistics for each unique attribute value. For example, the input
         summary features contain point locations of businesses that store
         hazardous materials, and one of the fields is HazardClass, which
         contains codes that describe the type of hazardous material stored. To
         calculate summaries by each unique value of HazardClass, use it as the
         group-by field.
     add_minority_majority {Boolean}:
         Specifies whether minority (least dominant) and majority (most
         dominant) attribute values for each group field within each boundary
         will be added. When this parameter value is ADD_MIN_MAJ, two new
         fields will be added to the output layer prefixed with Minority_ and
         Majority_. This parameter only applies when a value is provided for
         the group_by_field parameter.

         * NO_MIN_MAJ-Minority and majority fields will not be added. This is
         the default.

         * ADD_MIN_MAJ-Minority and majority fields will be added.
     add_percentages {Boolean}:
         Specifies whether percentage fields will be added. When this parameter
         value is ADD_PERCENT, the percentage of each unique group value will
         be calculated for each input polygon. This parameter only applies when
         a value is provided for the group_by_field parameter and a value is
         specified for the add_minority_majority parameter.

         * NO_PERCENT-Percentage fields will not be added. This is the default.

         * ADD_PERCENT-Percentage fields will be added.

    OUTPUTS:
     out_feature_class (Feature Class):
         The name of the output feature class that will contain the
         intersecting geometries and attributes.
     group_by_summary {Table}:
         The output table that will contain the group by summaries."""
    ...

@gptooldoc("CreateBuffers_gapro", None)
def CreateBuffers(
    input_layer=...,
    out_feature_class=...,
    method=...,
    buffer_type=...,
    buffer_field=...,
    buffer_distance=...,
    buffer_expression=...,
    dissolve_option=...,
    dissolve_fields=...,
    summary_fields=...,
    multipart=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateBuffers_gapro(input_layer, out_feature_class, method, buffer_type, {buffer_field}, {buffer_distance}, {buffer_expression}, {dissolve_option}, {dissolve_fields;dissolve_fields...}, {summary_fields;summary_fields...}, {multipart})

       Creates buffers around input features to a specified distance.

    INPUTS:
     input_layer (Feature Layer):
         The point, polyline, or polygon features that will be buffered.
     method (String):
         Specifies the method that will be used to create the buffers.

         * GEODESIC-Buffers will be created using a shape-preserving geodesic
         buffer method regardless of the input coordinate system. This is the
         default.

         * PLANAR-If the input features are in a projected coordinate system,
         Euclidean buffers will be created. If the input features are in a
         geographic coordinate system, geodesic buffers will be created. The
         Output Coordinate System environment setting can be used to specify a
         coordinate system.
     buffer_type (String):
         Specifies how the buffer distance will be defined.

         * DISTANCE-The same linear distance will be applied to all features.

         * FIELD-A numeric or string field will be selected to represent the
         buffer distance.

         * EXPRESSION-An expression will be built using fields, constants, and
         mathematical operations to represent the buffer distance.
     buffer_field {Field}:
         The field that contains the buffer distance for each feature. If a
         field value is a number, it is assumed that the distance is in the
         linear unit of the input_layer value spatial reference, unless the
         input_layer value is in a geographic coordinate system, in which case,
         the value is assumed to be in meters. If the linear unit specified in
         the field values is invalid or not recognized, the linear unit of the
         input features' spatial reference will be used by default.
     buffer_distance {Linear Unit}:
         The distance around the input features that will be buffered.
     buffer_expression {Calculator Expression}:
         An equation using fields and mathematical operators that will be
         applied as a buffer to each feature. Fields must be numeric, and the
         expression can include [+ - * / ] operators and multiple fields.
         Calculated values will be applied in meters unless otherwise
         specified. For example, apply a buffer that multiples a numeric field
         named distance in kilometers by 2 and adds 15 meters.Use an Arcade
         expression such as as_kilometers($feature["distance"]) *
         2 + as_meters(15).
     dissolve_option {String}:
         Specifies the dissolve option that will be used to remove buffer
         overlap.

         * NONE-An individual buffer for each feature will be maintained
         regardless of overlap. This is the default.

         * ALL-All buffers will be dissolved together into a single feature,
         removing any overlap.

         * LIST-Any buffers sharing attribute values in the listed fields
         (carried over from the input features) will be dissolved.
     dissolve_fields {Field}:
         A list of one or more fields from the input features on which output
         buffers will be dissolved. Any buffers sharing attribute values in the
         listed fields will be dissolved. This parameter is only required when
         dissolve_option is LIST.
     summary_fields {Value Table}:
         Specifies statistics that will be applied to numeric and string
         fields. If left empty, only count will be calculated. These statistics
         are only applied when dissolve_option is LIST or ALL.

         * COUNT-The number of nonnull values. It can be used on numeric fields
         or strings. The count of [null, 0, 2] is 2.

         * SUM-The sum of numeric values in a field. The sum of [null, null, 3]
         is 3.

         * MEAN-The mean of numeric values. The mean of [0,2, null] is 1.

         * MIN-The minimum value of a numeric field. The minimum of [0, 2,
         null] is 0.

         * MAX-The maximum value of a numeric field. The maximum value of [0,
         2, null] is 2.

         * STDDEV-The standard deviation of a numeric field. The standard
         deviation of [1] is null. The standard deviation of [null, 1,1,1] is
         null.

         * VAR-The variance of a numeric field in a track. The variance of [1]
         is null. The variance of [null, 1,1,1] is null.

         * RANGE-The range of a numeric field. This is calculated as the
         minimum value subtracted from the maximum value. The range of [0,
         null, 1] is 1. The range of [null, 4] is 0.

         * ANY-A sample string from a field of type string.
     multipart {Boolean}:
         Specifies whether multipart features will be created.

         * MULTI_PART-Output multipart features will be created where
         appropriate.

         * SINGLE_PART-Multipart features will not be created; individual
         features will be created for each part instead. This is the default.

    OUTPUTS:
     out_feature_class (Feature Class):
         The new feature class of buffered results."""
    ...

@gptooldoc("GroupByProximity_gapro", None)
def GroupByProximity(
    input_layer=...,
    output=...,
    spatial_relationship=...,
    spatial_near_distance=...,
    temporal_relationship=...,
    temporal_near_distance=...,
    attribute_relationship=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GroupByProximity_gapro(input_layer, output, spatial_relationship, {spatial_near_distance}, {temporal_relationship}, {temporal_near_distance}, {attribute_relationship})

       Groups features that are within spatial or spatiotemporal proximity to
       each other.

    INPUTS:
     input_layer (Feature Layer):
         The point, line, or polygon features that will be grouped.
     spatial_relationship (String):
         Specifies the type of relationship that features will be grouped by.

         * INTERSECTS-Features will be grouped when features or portions of
         features overlap. This is the default.

         * TOUCHES-Features will be grouped with another feature if they have
         an intersecting vertex, but the features do not overlap.

         * NEAR_PLANAR-Features will be grouped when a vertex or edge is within
         a given planar distance of another feature.

         * NEAR_GEODESIC-Features will be grouped when a vertex or edge is
         within a given geodesic distance of another feature.
     spatial_near_distance {Linear Unit}:
         The distance that will be used to group near features. This parameter
         is only used when the spatial_relationship parameter value is
         NEAR_PLANAR or NEAR_GEODESIC.
     temporal_relationship {String}:
         Specifies the time criteria that will be used to match features. When
         the parameter is set to INTERSECTS or NEAR, features are grouped when
         both the spatial and time criteria are met. Time must be enabled on
         the input to support this option.

         * INTERSECTS-Features will be grouped when any part of a feature's
         time overlaps another feature. This is the default.

         * NEAR-Features will be grouped when the feature's time is within a
         range of time of another feature.

         * NONE-Time will not be used to group features.
     temporal_near_distance {Time Unit}:
         The temporal distance that will be used to group near features. This
         parameter is only used when the temporal_relationship parameter value
         is Near.
     attribute_relationship {String}:
         An ArcGIS Arcade expression that will be used to group features by.
         For example, $a["Amount"] == $b["Amount"] groups features when the
         Amount field has the same value.

    OUTPUTS:
     output (Feature Class):
         The output feature class with grouped features represented by a new
         field named group_id."""
    ...

@gptooldoc("SnapTracks_gapro", None)
def SnapTracks(
    input_points=...,
    input_lines=...,
    out_feature_class=...,
    track_fields=...,
    search_distance=...,
    connectivity_field_matching=...,
    line_fields_to_include=...,
    distance_method=...,
    direction_value_matching=...,
    output_mode=...,
    time_split=...,
    distance_split=...,
    time_boundary_split=...,
    time_boundary_reference=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SnapTracks_gapro(input_points, input_lines, out_feature_class, track_fields;track_fields..., search_distance, connectivity_field_matching;connectivity_field_matching..., {line_fields_to_include;line_fields_to_include...}, {distance_method}, {direction_value_matching;direction_value_matching...}, {output_mode}, {time_split}, {distance_split}, {time_boundary_split}, {time_boundary_reference})

       Snaps input track points to lines. The time-enabled point data must
       include features that represent an instant in time. Traversable lines
       with fields indicating the from and to nodes are required for
       analysis.

    INPUTS:
     input_points (Feature Layer):
         The points that will be matched to lines. The input must be a time-
         enabled point layer that represents an instant in time and must
         contain at least one field that identifies unique tracks.
     input_lines (Feature Layer):
         The lines to which points will be matched. The input must contain
         fields with values indicating the from and to nodes of the line.
     track_fields (Field):
         One or more fields that will be used to identify unique tracks.
     search_distance (Linear Unit):
         The maximum distance allowed between a point and any line to be
         considered a match. It is recommended that you use values less than or
         equal to 75 meters. Larger distances will result in a longer process
         time and less accurate results.
     connectivity_field_matching (Value Table):
         The line layer fields that will be used to define the connectivity of
         the input line features.

         * Unique ID-The line layer field that contains the unique ID value for
         each line feature

         * From Node-The line layer field that contains the from node values

         * To Node-The line layer field that contains the to node values
     line_fields_to_include {Field}:
         One or more fields from the input line layer that will be included in
         the output result.
     distance_method {String}:
         Specifies the method that will be used to calculate distances between
         points and lines.

         * GEODESIC-Geodesic distances will be calculated.

         * PLANAR-Planar distances will be calculated.
     direction_value_matching {Value Table}:
         The line layer field and attribute values that will be used to
         define the direction of the input line features. For example, a line
         layer has a field named direction with values T (backward), F
         (forward), B (both), and "" (none). If no value is specified, the line
         is assumed to be bidirectional.

         * Direction Field-The field from the line layer that describes the
         direction of travel.

         * Forward Value-The value from the Direction Field that indicates the
         supported direction of travel is forward along a line.

         * Backward Value-The value from the Direction Field that indicates the
         supported direction of travel is backward along a line.

         * Both Value-The value from the Direction Field that indicates both
         forward and backward directions of travel are supported along a line.

         * None Value-The value from the Direction Field that indicates there
         are no supported directions of travel along a line.
     output_mode {String}:
         Specifies whether all input features or only the input features that
         were matched to a line feature will be returned.

         * ALL_FEATURES-All input point features will be returned regardless of
         whether they were matched to a line feature. This is the default.

         * MATCHED_FEATURES-Only input point features that were matched to a
         line feature will be returned.
     time_split {Time Unit}:
         Features that are farther apart in time than the time-split duration
         will be split into separate tracks.
     distance_split {Linear Unit}:
         Features that are farther apart in distance than the distance split
         value will be split into separate tracks.
     time_boundary_split {Time Unit}:
         A time span to split the input data into for analysis. A time boundary
         allows you to analyze values within a defined time span. For example,
         if you use a time boundary of 1 day, and set the time boundary
         reference to January 1, 1980, tracks will be split at the beginning of
         every day.
     time_boundary_reference {Date}:
         The reference time used to split the input data into for analysis.
         Time boundaries will be created for the entire span of the data, and
         the reference time does not need to occur at the start. If no
         reference time is specified, January 1, 1970, is used.

    OUTPUTS:
     out_feature_class (Feature Class):
         The feature class that will contain the matched points."""
    ...

@gptooldoc("TraceProximityEvents_gapro", None)
def TraceProximityEvents(
    in_points=...,
    entity_id_field=...,
    out_feature_class=...,
    distance_method=...,
    spatial_search_distance=...,
    temporal_search_distance=...,
    entities_of_interest_input_type=...,
    entities_interest_ids=...,
    entities_interest_layer=...,
    out_tracks_layer=...,
    max_trace_depth=...,
    attribute_match_criteria=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TraceProximityEvents_gapro(in_points, entity_id_field, out_feature_class, distance_method, spatial_search_distance, temporal_search_distance, entities_of_interest_input_type, {entities_interest_ids;entities_interest_ids...}, {entities_interest_layer}, {out_tracks_layer}, {max_trace_depth}, {attribute_match_criteria;attribute_match_criteria...})

       Traces events near each other in space (location) and time. The time-
       enabled point data must include features that represent an instant in
       time.

    INPUTS:
     in_points (Feature Layer):
         The time-enabled point feature class that will be used to trace
         proximity events.
     entity_id_field (Field):
         The field representing unique IDs for each entity.
     distance_method (String):
         Specifies the distance type that will be used with the Spatial Search
         Distance parameter.

         * PLANAR-Planar distance will be used between features. This is the
         default.

         * GEODESIC-Geodesic distance will be used between features. This line
         type takes into account the curvature of the spheroid and correctly
         deals with data near the dateline and poles.
     spatial_search_distance (Linear Unit):
         The maximum distance between two points to be considered in proximity.
         Features within the spatial search distance and temporal search
         distance criteria are considered to be in proximity of each other.
     temporal_search_distance (Time Unit):
         The maximum duration between two points to be considered in proximity.
         Features within the temporal search distance and that meet the spatial
         search distance criteria are considered to be in proximity of each
         other.
     entities_of_interest_input_type (String):
         Specifies the entities of interest.

         * ID_START_TIME-Entity names and times will be used as the entities of
         interest. This is the default.

         * SELECTED_FEATURE-The selected feature in a specified entity of
         interest layer will be used as the entities of interest.
     entities_interest_ids {Value Table}:
         The entity names and start times for the entities of interest. This
         parameter is supported only when ID_START_TIME is specified for the
         entities_of_interest_input_type parameter.

         * Entity ID-A unique entity name. The names are case sensitive.

         * Starting from-An optional starting time to trace an entity of
         interest. If a time is not specified, January 1, 1970, will be used.
     entities_interest_layer {Table View}:
         The layer or table that contains the entities of interest. This
         parameter is supported only when SELECTED_FEATURE is specified for the
         entities_of_interest_input_type parameter.
     max_trace_depth {Long}:
         The maximum degrees of separation between an entity of interest and an
         entity farther down the trace (downstream).
     attribute_match_criteria {Field}:
         The fields used to constrain the proximity event.

    OUTPUTS:
     out_feature_class (Feature Class):
         The output feature class containing the trace proximity events.
     out_tracks_layer {Feature Class}:
         An output layer containing the first trace event and all subsequent
         features for that specified entity."""
    ...

@gptooldoc("CopyDatasetFromBDC_gapro", None)
def CopyDatasetFromBDC(
    input_layer=..., output=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CopyDatasetFromBDC_gapro(input_layer, output)

       Copies a dataset from a multifile feature connection (MFC) to a
       feature class.

    INPUTS:
     input_layer (Table View):
         The point, line, polygon, or table dataset that will be copied.

    OUTPUTS:
     output (Feature Class / Table):
         The output dataset that will be created when the MFC dataset is
         copied."""
    ...

@gptooldoc("CreateBDC_gapro", None)
def CreateBDC(
    bdc_location=...,
    bdc_name=...,
    connection_type=...,
    data_source_folder=...,
    visible_geometry=...,
    visible_time=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CreateBDC_gapro(bdc_location, bdc_name, connection_type, {data_source_folder}, {visible_geometry}, {visible_time})

       Creates a multifile feature connection file (.mfc) and item. Datasets
       registered in a multifile feature connection (MFC) can be used as
       input to GeoAnalytics Desktop tools and other geoprocessing tools.

    INPUTS:
     bdc_location (Folder):
         The folder where the .mfc file will be created.
     bdc_name (String):
         The name of the .mfc file to be created.
     connection_type (String):
         Specifies the type of connection to be created.

         * FOLDER-Connect to a file system location. This is the default.
     data_source_folder {Folder}:
         The folder containing the datasets to be registered with the MFC.
     visible_geometry {Boolean}:
         Specifies whether the fields used to specify the geometry will be
         visible as fields when the MFC file is used as input to other
         geoprocessing tools. When the geometry fields are not visible,
         geometry is still applied to the dataset. The geometry visibility
         setting can be modified in the MFC.

         * GEOMETRY_VISIBLE-Geometry fields will be included as fields for
         analysis. This is the default.

         * GEOMETRY_NOT_VISIBLE-Geometry fields will not be included as fields
         for analysis.
     visible_time {Boolean}:
         Specifies whether the fields used to specify the time will be visible
         as fields when the MFC file is used as input to other geoprocessing
         tools. When the time fields are not visible, time is still applied to
         the dataset. The time visibility setting can be modified in the MFC.

         * TIME_VISIBLE-Time fields will be included as fields for analysis.
         This is the default.

         * TIME_NOT_VISIBLE-Time fields will not be included as fields for
         analysis."""
    ...

@gptooldoc("DuplicateDatasetFromBDC_gapro", None)
def DuplicateDatasetFromBDC(
    bdc_dataset=..., duplicate_name=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DuplicateDatasetFromBDC_gapro(bdc_dataset, {duplicate_name})

       Creates a duplicate of a multifile feature connection (MFC) dataset.

    INPUTS:
     bdc_dataset (Table View):
         The MFC dataset to be duplicated.
     duplicate_name {String}:
         The name of the output MFC dataset."""
    ...

@gptooldoc("PreviewDatasetFromBDC_gapro", None)
def PreviewDatasetFromBDC(
    bdc_dataset=..., out_preview_file=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PreviewDatasetFromBDC_gapro(bdc_dataset, {out_preview_file})

       Creates a preview of the first ten features in a multifile feature
       connection (MFC) dataset.

    INPUTS:
     bdc_dataset (Table View):
         The dataset to preview from the MFC file.

    OUTPUTS:
     out_preview_file {File}:
         The output .csv file that represents a preview of your MFC dataset."""
    ...

@gptooldoc("RefreshBDC_gapro", None)
def RefreshBDC(
    bdc_file=..., visible_geometry=..., visible_time=...
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RefreshBDC_gapro(bdc_file, {visible_geometry}, {visible_time})

       Refreshes an existing multifile feature connection (MFC) and registers
       any new datasets that have been added to the source location.

    INPUTS:
     bdc_file (File):
         The MFC file to refresh.
     visible_geometry {Boolean}:
         Specifies whether the fields used to identify the geometry will be
         included (visible) as fields for analysis when the MFC file is used in
         other geoprocessing tools. When geometry fields are not visible,
         geometry is still applied to the dataset. The geometry visibility
         setting can be modified in the MFC.

         * GEOMETRY_VISIBLE-Geometry fields will be included as fields for
         analysis. This is the default.

         * GEOMETRY_NOT_VISIBLE-Geometry fields will not be included as fields
         for analysis.
     visible_time {Boolean}:
         Specifies whether the fields used to indicate the time will be
         included (visible) as fields for analysis when the MFC file is used in
         other geoprocessing tools. When time fields are not visible, time is
         still applied to the dataset. The time visibility setting can be
         modified in the MFC.

         * TIME_VISIBLE-Time fields will be included as fields for analysis.
         This is the default.

         * TIME_NOT_VISIBLE-Time fields will not be included as fields for
         analysis."""
    ...

@gptooldoc("RemoveDatasetFromBDC_gapro", None)
def RemoveDatasetFromBDC(
    bdc_datasets=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """RemoveDatasetFromBDC_gapro(bdc_datasets;bdc_datasets...)

       Removes one or more datasets from an existing multifile feature
       connection (MFC). This tool only removes the dataset from the MFC
       file, the source data is not modified.

    INPUTS:
     bdc_datasets (Table View):
         The datasets to remove from the .mfc file."""
    ...

@gptooldoc("UpdateBDCDatasetProperties_gapro", None)
def UpdateBDCDatasetProperties(
    bdc_dataset=...,
    expression=...,
    field_properties=...,
    geometry_type=...,
    spatial_reference=...,
    geometry_format_type=...,
    geometry_field=...,
    x_field=...,
    y_field=...,
    z_field=...,
    time_type=...,
    time_zone=...,
    start_time_format=...,
    end_time_format=...,
    file_extension=...,
    field_delimiter=...,
    record_terminator=...,
    quote_character=...,
    has_header_row=...,
    encoding=...,
):  # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """UpdateBDCDatasetProperties_gapro(bdc_dataset, {expression}, {field_properties;field_properties...}, {geometry_type}, {spatial_reference}, {geometry_format_type}, {geometry_field}, {x_field}, {y_field}, {z_field}, {time_type}, {time_zone}, {start_time_format;start_time_format...}, {end_time_format;end_time_format...}, {file_extension}, {field_delimiter}, {record_terminator}, {quote_character}, {has_header_row}, {encoding})

       Updates the properties of a multifile feature connection (MFC)
       dataset. This tool modifies field, geometry, time, and file settings
       for a specified MFC dataset.

    INPUTS:
     bdc_dataset (Table View):
         The MFC dataset that will be updated. The options for editing will
         differ depending on the source data (shapefile, delimited file, ORC,
         or parquet file).
     expression {SQL Expression}:
         An expression used to limit the features that will be used in the
         analysis.
     field_properties {Value Table}:
         Specifies the field names and properties that will be modified.

         * SHORT-The field will be type short.

         * LONG-The field will be type long

         * DOUBLE-The field will be type double.

         * FLOAT-The field will be type float.

         * STRING-The field will be type string.

         * DATE-The field will be type date.

         * BLOB-The field will be type BLOB.
         Specifies whether fields will be visible or hidden.

         * TRUE-The fields will be visible and available for use in
         geoprocessing tools. This is the default.

         * FALSE-The fields will be hidden and cannot be used as input to
         geoprocessing tools.
     geometry_type {String}:
         Specifies the type of geometry that will be used to spatially
         represent the dataset. The geometry cannot be modified for shapefile-
         sourced datasets.

         * POINT-The geometry type will be point.

         * LINE-The geometry type will be polyline.

         * POLYGON-The geometry type will be polygon.

         * NONE-No geometry type is specified.
     spatial_reference {String}:
         The WKID value or WKT string that will be used for the spatial
         reference of the dataset. The default is WKID 4326 (WGS84). The
         spatial reference cannot be modified for shapefile-sourced data.
     geometry_format_type {String}:
         Specifies how the geometry will be formatted. The geometry cannot be
         modified for shapefile-sourced data.

         * XYZ-Two or more fields will represent x, y, and optionally z.

         * WKT-The geometry will be represented by a single field in a well-
         known text field.

         * WKB-The geometry will be represented by a single field in a well-
         known binary field.

         * GEOJSON-The geometry will be represented by a single field in
         GeoJSON format.

         * ESRIJSON-The geometry will be represented by a single field in
         EsriJSON format.

         * ESRISHAPE-The geometry will be represented by a single field in
         EsriShape format.
     geometry_field {String}:
         A single field used to represent the geometry. This field is used when
         the geometry format is WKT, WKB, GeoJSON, EsriJSON, or EsriShape.
     x_field {String}:
         The field used to represent the x-location. If more than one field
         represents the x-location, modify the .mfc file manually.
     y_field {String}:
         The field used to represent the y-location. If more than one field
         represents the y-location, modify the .mfc file manually.
     z_field {String}:
         The field used to represent the z-location. If more than one field
         represents the z-location, modify the .mfc file manually.
     time_type {String}:
         Specifies the time type that will be used to temporally represent the
         dataset.

         * INTERVAL-The time type will represent a duration of time with a
         start and end time.

         * INSTANT-The time type will represent an instant in time.

         * NONE-Time is not enabled.
     time_zone {String}:
         The time zone of the dataset.
     start_time_format {Value Table}:
         The fields used to define the start time and the time formatting.
     end_time_format {Value Table}:
         The fields used to define the end time and the time formatting.
     file_extension {String}:
         The file extension of the source dataset. The parameter value cannot
         be modified.
     field_delimiter {String}:
         The field delimiter used in the source dataset.
     record_terminator {String}:
         The record terminator used in the source dataset.
     quote_character {String}:
         The quote character used in the source dataset.
     has_header_row {Boolean}:
         Specifies whether the source dataset includes a header row.

         * HAS_HEADER-The source dataset includes a header row.

         * NO_HEADER-The source dataset does not include a header row.
     encoding {String}:
         The type of encoding used by the source dataset. UTF-8 is used by
         default."""
    ...
