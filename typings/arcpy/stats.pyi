"""
This type stub file was generated by pyright.
"""

from arcpy.geoprocessing._base import gptooldoc

r"""The Spatial Statistics toolbox contains statistical tools for
analyzing spatial distributions, patterns, processes, and
relationships. While there may be similarities between spatial and
nonspatial (traditional) statistics in terms of concepts and
objectives, spatial statistics are unique in that they were developed
specifically for use with geographic data. Unlike traditional
nonspatial statistical methods, they incorporate space (proximity,
area, connectivity, and/or other spatial relationships) directly into
their mathematics."""
__all__ = ['AverageNearestNeighbor', 'BuildBalancedZones', 'CalculateAreas', 'CalculateCompositeIndex', 'CalculateDistanceBand', 'CentralFeature', 'ClustersOutliers', 'CollectEvents', 'ColocationAnalysis', 'ConvertSpatialWeightsMatrixtoTable', 'DensityBasedClustering', 'DescribeSSMFile', 'DimensionReduction', 'DirectionalDistribution', 'DirectionalMean', 'ExploratoryRegression', 'ExportXYv', 'Forest', 'GWR', 'GeneralizedLinearRegression', 'GenerateNetworkSWM', 'GenerateNetworkSpatialWeights', 'GenerateSpatialWeightsMatrix', 'GeographicallyWeightedRegression', 'GroupingAnalysis', 'HighLowClustering', 'HotSpotAnalysisComparison', 'HotSpots', 'IncrementalSpatialAutocorrelation', 'LocalBivariateRelationships', 'MGWR', 'MeanCenter', 'MedianCenter', 'MultiDistanceSpatialClustering', 'MultivariateClustering', 'NeighborhoodSummaryStatistics', 'OptimizedHotSpotAnalysis', 'OptimizedOutlierAnalysis', 'OrdinaryLeastSquares', 'PredictUsingSSMFile', 'PresenceOnlyPrediction', 'SetSSMFileProperties', 'SimilaritySearch', 'SpatialAssociationBetweenZones', 'SpatialAutocorrelation', 'SpatialOutlierDetection', 'SpatiallyConstrainedMultivariateClustering', 'StandardDistance', 'TimeSeriesSmoothing']
__alias__ = ...
@gptooldoc('AverageNearestNeighbor_stats', None)
def AverageNearestNeighbor(Input_Feature_Class=..., Distance_Method=..., Generate_Report=..., Area=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """AverageNearestNeighbor_stats(Input_Feature_Class, Distance_Method, {Generate_Report}, {Area})

        Calculates a nearest neighbor index based on the average distance from
        each feature to its nearest neighboring feature.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class, typically a point feature class, for which the
          average nearest neighbor distance will be calculated.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block); calculated by summing the
          (absolute) difference between the x- and y-coordinates
      Generate_Report {Boolean}:
          Specifies whether the tool will create a graphical summary of results.

          * NO_REPORT-No graphical summary will be created. This is the default.

          * GENERATE_REPORT-A graphical summary will be created as an HTML file.
      Area {Double}:
          A numeric value representing the study area size. The default value is
          the area of the minimum enclosing rectangle that would encompass all
          features (or all selected features). Units should match those for the
          Output Coordinate System."""
    ...

@gptooldoc('HighLowClustering_stats', None)
def HighLowClustering(Input_Feature_Class=..., Input_Field=..., Generate_Report=..., Conceptualization_of_Spatial_Relationships=..., Distance_Method=..., Standardization=..., Distance_Band_or_Threshold_Distance=..., Weights_Matrix_File=..., number_of_neighbors=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """HighLowClustering_stats(Input_Feature_Class, Input_Field, {Generate_Report}, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File}, {number_of_neighbors})

        Measures the degree of clustering for either high or low values using
        the Getis-Ord General G statistic.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class for which the General G statistic will be
          calculated.
      Input_Field (Field):
          The numeric field to be evaluated.
      Generate_Report {Boolean}:
          Specifies whether a graphical summary of result will be created as an
          .html file.

          * NO_REPORT-No graphical summary will be created. This is the default.

          * GENERATE_REPORT-A graphical summary will be created.
      Conceptualization_of_Spatial_Relationships (String):
          Specifies how spatial relationships among features are defined.

          * INVERSE_DISTANCE-Nearby neighboring features have a larger influence
          on the computations for a target feature than features that are far
          away.

          * INVERSE_DISTANCE_SQUARED-Same as INVERSE_DISTANCE except that the
          slope is sharper, so influence drops off more quickly, and only a
          target feature's closest neighbors will exert substantial influence on
          computations for that feature.

          * FIXED_DISTANCE_BAND-Each feature is analyzed within the context of
          neighboring features. Neighboring features inside the specified
          critical distance (Distance_Band_or_Threshold) receive a weight of one
          and exert influence on computations for the target feature.
          Neighboring features outside the critical distance receive a weight of
          zero and have no influence on a target feature's computations.

          * ZONE_OF_INDIFFERENCE-Features within the specified critical distance
          (Distance_Band_or_Threshold) of a target feature receive a weight of
          one and influence computations for that feature. Once the critical
          distance is exceeded, weights (and the influence a neighboring feature
          has on target feature computations) diminish with distance.

          * K_NEAREST_NEIGHBORS-The closest k features are included in the
          analysis; k is a specified numeric parameter.

          * CONTIGUITY_EDGES_ONLY-Only neighboring polygon features that share a
          boundary or overlap will influence computations for the target polygon
          feature.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share a boundary,
          share a node, or overlap will influence computations for the target
          polygon feature.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial relationships are defined by a
          specified spatial weights file. The path to the spatial weights file
          is specified by the Weights_Matrix_File parameter.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block); calculated by summing the
          (absolute) difference between the x- and y-coordinates
      Standardization (String):
          Specifies whether standardization of spatial weights will be applied.
          Row standardization is recommended whenever the distribution of your
          features is potentially biased due to sampling design or an imposed
          aggregation scheme.

          * NONE-No standardization of spatial weights is applied.

          * ROW-Spatial weights are standardized; each weight is divided by its
          row sum (the sum of the weights of all neighboring features). This is
          the default.
      Distance_Band_or_Threshold_Distance {Double}:
          Specifies a cutoff distance for the inverse distance and fixed
          distance options. Features outside the specified cutoff for a target
          feature are ignored in analyses for that feature. However, for
          ZONE_OF_INDIFFERENCE, the influence of features outside the given
          distance is reduced with distance, while those inside the distance
          threshold are equally considered. The distance value entered should
          match that of the output coordinate system.For the inverse distance
          conceptualizations of spatial relationships,
          a value of 0 indicates that no threshold distance is applied; when
          this parameter is left blank, a default threshold value is computed
          and applied. This default value is the Euclidean distance that ensures
          that every feature has at least one neighbor.This parameter has no
          effect when polygon contiguity
          (CONTIGUITY_EDGES_ONLY or CONTIGUITY_EDGES_CORNERS) or
          GET_SPATIAL_WEIGHTS_FROM_FILE spatial conceptualizations are selected.
      Weights_Matrix_File {File}:
          The path to a file containing weights that define spatial, and
          potentially temporal, relationships among features.
      number_of_neighbors {Long}:
          An integer specifying the number of neighbors that will be included in
          the analysis."""
    ...

@gptooldoc('IncrementalSpatialAutocorrelation_stats', None)
def IncrementalSpatialAutocorrelation(Input_Features=..., Input_Field=..., Number_of_Distance_Bands=..., Beginning_Distance=..., Distance_Increment=..., Distance_Method=..., Row_Standardization=..., Output_Table=..., Output_Report_File=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """IncrementalSpatialAutocorrelation_stats(Input_Features, Input_Field, Number_of_Distance_Bands, {Beginning_Distance}, {Distance_Increment}, {Distance_Method}, {Row_Standardization}, {Output_Table}, {Output_Report_File})

        Measures spatial autocorrelation for a series of distances and
        optionally creates a line graph of those distances and their
        corresponding z-scores. Z-scores reflect the intensity of spatial
        clustering, and statistically significant peak z-scores indicate
        distances where spatial processes promoting clustering are most
        pronounced. These peak distances are often appropriate values to use
        for tools with a Distance Band or Distance Radius parameter.

     INPUTS:
      Input_Features (Feature Layer):
          The feature class for which spatial autocorrelation will be measured
          over a series of distances.
      Input_Field (Field):
          The numeric field used in assessing spatial autocorrelation.
      Number_of_Distance_Bands (Long):
          The number of times to increment the neighborhood size and analyze the
          dataset for spatial autocorrelation. The starting point and size of
          the increment are specified in the Beginning_Distance and
          Distance_Increment parameters, respectively.
      Beginning_Distance {Double}:
          The distance at which to start the analysis of spatial autocorrelation
          and the distance from which to increment. The value entered for this
          parameter should be in the units of the Output Coordinate System
          environment setting.
      Distance_Increment {Double}:
          The distance to increase after each iteration. The distance used in
          the analysis starts at the Beginning_Distance and increases by the
          amount specified in the Distance_Increment. The value entered for this
          parameter should be in the units of the Output Coordinate System
          environment setting.
      Distance_Method {String}:
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN-The straight-line distance between two points (as the crow
          flies)

          * MANHATTAN-The distance between two points measured along axes at
          right angles (city block); calculated by summing the (absolute)
          difference between the x- and y-coordinates
      Row_Standardization {Boolean}:
          Row standardization is recommended whenever feature distribution is
          potentially biased due to sampling design or to an imposed aggregation
          scheme.

          * ROW_STANDARDIZATION-Spatial weights are standardized by row. Each
          weight is divided by its row sum.

          * NO_STANDARDIZATION-No standardization of spatial weights is applied.

     OUTPUTS:
      Output_Table {Table}:
          The table to be created with each distance band and associated z-score
          result.
      Output_Report_File {File}:
          The PDF file to be created containing a line graph summarizing
          results."""
    ...

@gptooldoc('MultiDistanceSpatialClustering_stats', None)
def MultiDistanceSpatialClustering(Input_Feature_Class=..., Output_Table=..., Number_of_Distance_Bands=..., Compute_Confidence_Envelope=..., Display_Results_Graphically=..., Weight_Field=..., Beginning_Distance=..., Distance_Increment=..., Boundary_Correction_Method=..., Study_Area_Method=..., Study_Area_Feature_Class=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MultiDistanceSpatialClustering_stats(Input_Feature_Class, Output_Table, Number_of_Distance_Bands, {Compute_Confidence_Envelope}, {Display_Results_Graphically}, {Weight_Field}, {Beginning_Distance}, {Distance_Increment}, {Boundary_Correction_Method}, {Study_Area_Method}, {Study_Area_Feature_Class})

        Determines whether features, or the values associated with features,
        exhibit statistically significant clustering or dispersion over a
        range of distances.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class upon which the analysis will be performed.
      Number_of_Distance_Bands (Long):
          The number of times to increment the neighborhood size and analyze the
          dataset for clustering. The starting point and size of the increment
          are specified in the Beginning_Distance and Distance_Increment
          parameters, respectively.
      Compute_Confidence_Envelope {String}:
          The confidence envelope is calculated by randomly placing feature
          points (or feature values) in the study area. The number of
          points/values randomly placed is equal to the number of points in the
          feature class. Each set of random placements is called a permutation
          and the confidence envelope is created from these permutations. This
          parameter allows you to select how many permutations you want to use
          to create the confidence envelope.

          * 0_PERMUTATIONS_-_NO_CONFIDENCE_ENVELOPE-Confidence envelopes are not
          created.

          * 9_PERMUTATIONS-Nine sets of points/values are randomly placed.

          * 99_PERMUTATIONS-99 sets of points/values are randomly placed.

          * 999_PERMUTATIONS-999 sets of points/values are randomly placed.
      Display_Results_Graphically {Boolean}:
          This parameter has no effect; it remains to support backward
          compatibility.

          * NO_DISPLAY-No graphical summary will be created (default).

          * DISPLAY_IT-A graphical summary will be created as a graph layer.
      Weight_Field {Field}:
          A numeric field with weights representing the number of
          features/events at each location.
      Beginning_Distance {Double}:
          The distance at which to start the cluster analysis and the distance
          from which to increment. The value entered for this parameter should
          be in the units of the Output Coordinate System.
      Distance_Increment {Double}:
          The distance to increment during each iteration. The distance used in
          the analysis starts at the Beginning_Distance and increments by the
          amount specified in the Distance_Increment. The value entered for this
          parameter should be in the units of the Output Coordinate System
          environment setting.
      Boundary_Correction_Method {String}:
          Method to use to correct for underestimates in the number of neighbors
          for features near the edges of the study area.

          * NONE-No edge correction is applied. However, if the input feature
          class already has points that fall outside the study area boundaries,
          these will be used in neighborhood counts for features near
          boundaries.

          * SIMULATE_OUTER_BOUNDARY_VALUES-This method simulates points outside
          the study area so that the number of neighbors near edges is not
          underestimated. The simulated points are the "mirrors" of points near
          edges within the study area boundary.

          * REDUCE_ANALYSIS_AREA-This method shrinks the study area such that
          some points are found outside of the study area boundary. Points found
          outside the study area are used to calculate neighbor counts but are
          not used in the cluster analysis itself.

          * RIPLEY_EDGE_CORRECTION_FORMULA-For all the points (j) in the
          neighborhood of point i, this method checks to see if the edge of the
          study area is closer to i, or if j is closer to i. If j is closer,
          extra weight is given to the point j. This edge correction method is
          only appropriate for square or rectangular shaped study areas.
      Study_Area_Method {String}:
          Specifies the region to use for the study area. The K Function is
          sensitive to changes in study area size so careful selection of this
          value is important.

          * MINIMUM_ENCLOSING_RECTANGLE-Indicates that the smallest possible
          rectangle enclosing all of the points will be used.

          * USER_PROVIDED_STUDY_AREA_FEATURE_CLASS-Indicates that a feature
          class defining the study area will be provided in the Study Area
          Feature Class parameter.
      Study_Area_Feature_Class {Feature Layer}:
          Feature class that delineates the area over which the input feature
          class should be analyzed. Only specified if Study_Area_Method =
          "USER_PROVIDED_STUDY_AREA_FEATURE_CLASS".

     OUTPUTS:
      Output_Table (Table):
          The table to which the results of the analysis will be written."""
    ...

@gptooldoc('SpatialAutocorrelation_stats', None)
def SpatialAutocorrelation(Input_Feature_Class=..., Input_Field=..., Generate_Report=..., Conceptualization_of_Spatial_Relationships=..., Distance_Method=..., Standardization=..., Distance_Band_or_Threshold_Distance=..., Weights_Matrix_File=..., number_of_neighbors=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SpatialAutocorrelation_stats(Input_Feature_Class, Input_Field, {Generate_Report}, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File}, {number_of_neighbors})

        Measures spatial autocorrelation based on feature locations and
        attribute values using the Global Moran's I statistic.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class for which spatial autocorrelation will be
          calculated.
      Input_Field (Field):
          The numeric field used in assessing spatial autocorrelation.
      Generate_Report {Boolean}:
          Specifies whether a graphical summary of result will be created as an
          .html file.

          * NO_REPORT-No graphical summary will be created. This is the default.

          * GENERATE_REPORT-A graphical summary will be created.
      Conceptualization_of_Spatial_Relationships (String):
          Specifies how spatial relationships among features are defined.

          * INVERSE_DISTANCE-Nearby neighboring features have a larger influence
          on the computations for a target feature than features that are far
          away.

          * INVERSE_DISTANCE_SQUARED-This is the same as INVERSE_DISTANCE except
          that the slope is sharper, so influence drops off more quickly, and
          only a target feature's closest neighbors will exert substantial
          influence on computations for that feature.

          * FIXED_DISTANCE_BAND-Each feature is analyzed within the context of
          neighboring features. Neighboring features inside the specified
          critical distance (Distance_Band_or_Threshold) receive a weight of one
          and exert influence on computations for the target feature.
          Neighboring features outside the critical distance receive a weight of
          zero and have no influence on a target feature's computations.

          * ZONE_OF_INDIFFERENCE-Features within the specified critical distance
          (Distance_Band_or_Threshold) of a target feature receive a weight of
          one and influence computations for that feature. Once the critical
          distance is exceeded, weights (and the influence a neighboring feature
          has on target feature computations) diminish with distance.

          * K_NEAREST_NEIGHBORS-The closest k features are included in the
          analysis. The number of neighbors (k) to include in the analysis is
          specified by the number_of_neighbors parameter.

          * CONTIGUITY_EDGES_ONLY-Only neighboring polygon features that share a
          boundary or overlap will influence computations for the target polygon
          feature.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share a boundary,
          share a node, or overlap will influence computations for the target
          polygon feature.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial relationships are defined by a
          specified spatial weights file. The path to the spatial weights file
          is specified by the Weights_Matrix_File parameter.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies) will be used.

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block) will be used. This is calculated by
          summing the (absolute) difference between the x- and y-coordinates
      Standardization (String):
          Specifies whether standardization of spatial weights will be applied.
          Row standardization is recommended whenever the distribution of your
          features is potentially biased due to sampling design or an imposed
          aggregation scheme.

          * NONE-No standardization of spatial weights is applied.

          * ROW-Spatial weights are standardized; each weight is divided by its
          row sum (the sum of the weights of all neighboring features). This is
          the default.
      Distance_Band_or_Threshold_Distance {Double}:
          The cutoff distance for the various inverse distance and fixed
          distance options. Features outside the specified cutoff for a target
          feature are ignored in analyses for that feature. However, for
          ZONE_OF_INDIFFERENCE, the influence of features outside the given
          distance is reduced with distance, while those inside the distance
          threshold are equally considered. The distance value entered should
          match that of the output coordinate system.For the inverse distance
          conceptualizations of spatial relationships,
          a value of 0 indicates that no threshold distance is applied; when
          this parameter is left blank, a default threshold value is computed
          and applied. This default value is the Euclidean distance, which
          ensures that every feature has at least one neighbor.This parameter
          has no effect when polygon contiguity
          (CONTIGUITY_EDGES_ONLY or CONTIGUITY_EDGES_CORNERS) or
          GET_SPATIAL_WEIGHTS_FROM_FILE spatial conceptualization is selected.
      Weights_Matrix_File {File}:
          The path to a file containing weights that define spatial, and
          potentially temporal, relationships among features.
      number_of_neighbors {Long}:
          An integer specifying the number of neighbors to include in the
          analysis."""
    ...

@gptooldoc('BuildBalancedZones_stats', None)
def BuildBalancedZones(in_features=..., output_features=..., zone_creation_method=..., number_of_zones=..., zone_building_criteria_target=..., zone_building_criteria=..., spatial_constraints=..., weights_matrix_file=..., zone_characteristics=..., attribute_to_consider=..., distance_to_consider=..., categorial_variable=..., proportion_method=..., population_size=..., number_generations=..., mutation_factor=..., output_convergence_table=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """BuildBalancedZones_stats(in_features, output_features, zone_creation_method, {number_of_zones}, {zone_building_criteria_target;zone_building_criteria_target...}, {zone_building_criteria;zone_building_criteria...}, {spatial_constraints}, {weights_matrix_file}, {zone_characteristics;zone_characteristics...}, {attribute_to_consider;attribute_to_consider...}, {distance_to_consider;distance_to_consider...}, {categorial_variable}, {proportion_method}, {population_size}, {number_generations}, {mutation_factor}, {output_convergence_table})

        Creates spatially contiguous zones in a study area using a genetic
        growth algorithm based on specified criteria.

     INPUTS:
      in_features (Feature Layer):
          The feature class or feature layer that will be aggregated into zones.
      zone_creation_method (String):
          Specifies the method that will be used to create each zone. Zones grow
          until all specified criteria are satisfied.

          * ATTRIBUTE_TARGET-Zones will be created based on target values of one
          or multiple variables. The sum of each attribute must be specified in
          the Zone Building Criteria With Target parameter, and each zone will
          grow until the sum of the attributes exceeds these values. For
          example, you can use this option to create zones that each have at
          least 100,000 residents and 20,000 family homes.

          * NUMBER_ZONES_AND_ATTRIBUTE-A specified number of zones will be
          created while keeping the sum of an attribute approximately equal
          within each zone. The number of zones must be specified in the Target
          Number of Zones parameter. The attribute sum within each zone is equal
          to the sum of the total attribute divided by the number of zones.

          * NUMBER_OF_ZONES-A specified number of zones will be created that are
          each composed of approximately the same number of input features. The
          number of zones must be specified in the Target Number of Zones
          parameter.
      number_of_zones {Long}:
          The number of zones that will be created.
      zone_building_criteria_target {Value Table}:
          Specifies the variables that will be considered, as well as their
          target values and optional weights. The default weight is 1, and each
          variable contributes equally unless they are changed.
      zone_building_criteria {Value Table}:
          Specifies the variables that will be considered and, optionally, their
          weights. The default weight is 1, and each variable contributes
          equally unless changed.
      spatial_constraints {String}:
          Specifies how neighbors will be defined while the zones grow. Zones
          can only grow into new features that are neighbors of at least one of
          the features already in the zone. If the input features are polygons,
          the default spatial constraint is Contiguity edges corners. If the
          input features are points, the default spatial constraint is Trimmed
          Delaunay triangulation.

          * CONTIGUITY_EDGES_ONLY-For zones containing contiguous polygon
          features, only polygons that share an edge will be part of the same
          zone.

          * CONTIGUITY_EDGES_CORNERS-For zones containing contiguous polygon
          features, only polygons that share an edge or a vertex will be part of
          the same zone.

          * TRIMMED_DELAUNAY_TRIANGULATION-Features in the same zone will have
          at least one natural neighbor in common with another feature in the
          zone. Natural neighbor relationships are based on a trimmed Delaunay
          Triangulation. Conceptually, Delaunay Triangulation creates a
          nonoverlapping mesh of triangles from feature centroids. Each feature
          is a triangle node, and nodes that share edges are considered
          neighbors. These triangles are then clipped to a convex hull to ensure
          that features cannot be neighbors with any features outside of the
          convex hull. This is the default.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial, and, optionally, temporal
          relationships will be defined by a specified spatial weights file
          (.swm). Create the spatial weights matrix using the Generate Spatial
          Weights Matrix tool or the Generate Network Spatial Weights tool. The
          path to the spatial weights file is specified by the Spatial Weights
          Matrix File parameter.
      weights_matrix_file {File}:
          The path to a file containing spatial weights that define spatial and,
          optionally, temporal relationships among features.
      zone_characteristics {String}:
          Specifies the characteristics of the zones that will be created.

          * EQUAL_AREA-Zones with total area as similar as possible will be
          created.

          * COMPACTNESS-Zones with more closely-packed (compact) features will
          be created.

          * EQUAL_NUMBER_OF_FEATURES-Zones with an equal number of features will
          be created.
      attribute_to_consider {Value Table}:
          Specifies attributes and statistics to consider in the selection of
          final zones. You can homogenize attributes based on their sum,
          average, median, or variance. For example, if you are creating zones
          based on home values and want to balance the average total income
          within each zone, the solution with the most equal average income
          across zones will be preferred.
      distance_to_consider {Feature Layer}:
          The feature class that will be used to homogenize the total distance
          per zone. The distance is calculated from each of the input features
          to the closest feature provided in this parameter. This distance is
          then used as an additional attribute constraint when selecting the
          final zone solution. For example, you can create police patrol
          districts that are each approximately the same distance from the
          closest police station.
      categorial_variable {Field}:
          The categorical variable to be considered for zone proportions.
      proportion_method {String}:
          Specifies the type of proportion that will be maintained based on the
          chosen categorical variable.

          * MAINTAIN_WITHIN_PROPORTION-Each zone will maintain the same
          proportions as the overall study area for the given categorical
          variable. For example, given a categorical variable that is 60% Type A
          and 40% Type B, this method will prefer zones that are composed of
          approximately 60% Type A features and 40% Type B features.

          * MAINTAIN_OVERALL_PROPORTION-Zones will be created so that the
          overall proportions of category predominance by zone matches the
          proportions of the given categorical variable for the entire dataset.
          For example, given a categorical variable that is 60% Type A and 40%
          Type B, this method will prefer solutions where 60% of the zones are
          predominantly Type A features and 40% of the zones are predominantly
          Type B features.
      population_size {Long}:
          The number of randomly generated initial seeds. For larger datasets,
          increasing this number will increase the search space and the
          probability of finding a better solution. The default is 100.
      number_generations {Long}:
          The number of times the zone search process will be repeated. For
          larger datasets, increasing the number is recommended to find an
          optimal solution. The default is 50 generations.
      mutation_factor {Double}:
          The probability that an individual's seed values will be mutated to a
          new set of seeds. Mutation increases the search space by introducing
          variability of the possible solutions in every generation and allows
          for faster convergence to an optimal solution. The default is 0.1.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class indicating which features are aggregated into
          each zone. The feature class will be symbolized by the ZONE_ID field
          and will contain fields displaying the values of each criteria that
          you specify.
      output_convergence_table {Table}:
          The table containing the total fitness score for the best solution
          found in every generation as well as the fitness score for the
          individual zone constraints."""
    ...

@gptooldoc('CalculateCompositeIndex_stats', None)
def CalculateCompositeIndex(in_table=..., in_variables=..., append_to_input=..., out_table=..., index_preset=..., preprocessing=..., pre_threshold_scaling=..., pre_custom_zscore=..., pre_min_max=..., pre_thresholds=..., index_method=..., index_weights=..., out_index_name=..., out_index_reverse=..., post_min_max=..., post_reclass=..., post_num_classes=..., post_custom_classes=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateCompositeIndex_stats(in_table, in_variables;in_variables..., {append_to_input}, {out_table}, {index_preset}, {preprocessing}, {pre_threshold_scaling}, {pre_custom_zscore;pre_custom_zscore...}, {pre_min_max;pre_min_max...}, {pre_thresholds;pre_thresholds...}, {index_method}, {index_weights;index_weights...}, {out_index_name}, {out_index_reverse}, {post_min_max;post_min_max...}, {post_reclass;post_reclass...}, {post_num_classes}, {post_custom_classes;post_custom_classes...})

        Combines multiple numeric variables to create a single index.

     INPUTS:
      in_table (Feature Layer / Table View):
          The table or features containing the variables that will be combined
          into the index.
      in_variables (Value Table):
          A list of numeric fields representing the variables that will be
          combined as an index. The Reverse Direction column reverses the values
          of the variables. This will mean that the feature or record that
          originally had the highest value will have the lowest value, and vice
          versa. Values will be reversed after scaling.
      append_to_input {Boolean}:
          Specifies whether the results will be appended to the input data or
          provided as an output feature class or table.

          * APPEND_TO_INPUT-The results will be appended to the input data.
          This option modifies the input data.

          * NEW_FEATURES-An output feature class or table will be created
          containing the results. This is the default.
      index_preset {String}:
          Specifies the workflow that will be used when creating the index. The
          options represent common index creation workflows; each option sets
          default values for the preprocessing and index_method parameters.

          * MEAN_SCALED-An index will be created by scaling the input variables
          between 0 and 1 and averaging the scaled values. This method is useful
          for creating an index that is easy to interpret. The shape of the
          distribution and outliers in the input variables will impact the
          index. This is the default.

          * MEAN_PCTL-An index will be created by scaling the ranks of the input
          variables between 0 and 1 and averaging the scaled ranks. This option
          is useful when the rankings of the variable values are more important
          than the differences between values. The shape of the distribution and
          outliers in the input variables will not impact the index.

          * GEOMEAN_SCALED-An index will be created by scaling the input
          variables between 0 and 1 and calculating the geometric average of the
          scaled values. High values will not cancel low values, so this option
          is useful for creating an index in which higher index values will
          occur only when there are high values in multiple variables.

          * SUM_FLAGSPCTL-An index will be created that counts the number of
          input variables with values greater than or equal to the 90th
          percentile. This method is useful for identifying locations that may
          be considered the most extreme or the most in need.

          * CUSTOM-An index will be created using customized variable scaling
          and combination options.
      preprocessing {String}:
          The method that will be used to convert the input variables to a
          common scale.

          * MINMAX-Variables will be scaled between 0 and 1 using the minimum
          and maximum values of each variable. This is the default.

          * CUST_MINMAX-Variables will be scaled between 0 and 1 using the
          possible minimum and possible maximum values for each variable,
          specified by the pre_min_max parameter. This method has many uses,
          including specifying the minimum and maximum based on a benchmark, on
          a reference statistic, or on theoretical values. For example, if ozone
          recordings for a single day range between 5 and 27 parts per million
          (ppm), you can use the theoretical minimum and maximum based on prior
          observation and domain expertise to ensure the index can be compared
          across multiple days

          * PERCENTILE-Variables will be converted to percentiles between 0 and
          1 by calculating the percent of data values less than the data value.
          This option is useful when you want to ignore absolute differences
          between the data values, such as with outliers or skewed
          distributions.

          * RANK-Variables will be ranked. The smallest value is assigned rank
          value 1, the next value is assigned rank value 2, and so on. Ties are
          assigned the average of their ranks.

          * ZSCORE-Each variable will be standardized by subtracting the mean
          value and dividing by the standard deviation (called a z-score). The
          z-score is the number of standard deviations above or below the mean
          value. This option is useful when the means of the variables are
          important comparison points. Values above the mean will receive
          positive z-scores, and values below the mean will receive negative
          z-scores.

          * CUST_ZSCORE-Each variable will be standardized by subtracting a
          custom mean value and dividing by a custom standard deviation. Provide
          the custom values in the pre_custom_zscore parameter. This option is
          useful when the means and standard deviations of the variables are
          known from previous research.

          * BINARY-Variables will be identified when they are above or below a
          defined threshold. The resulting field contains binary (0 or 1) values
          indicating whether the threshold was exceeded. You can also use the
          pre_threshold_scaling parameter to scale the input variable values
          before defining the threshold, and use the pre_thresholds parameter to
          specify the threshold values. This method is useful when the values of
          the variables are less important than whether they exceed a particular
          threshold, such as a safety limit of a pollutant.

          * RAW-The original values of the variables will be used. Use this
          method only when all variables are measured on a comparable scale,
          such as percentages or rates, or when the variables have been
          standardized before using this tool.
      pre_threshold_scaling {String}:
          Specifies the method that will be used to convert the input variables
          to a common scale prior to setting thresholds.

          * THRESHOLD_MINMAX-Variables between 0 and 1 will be scaled using the
          minimum and maximum values of each variable.

          * THRESHOLD_CUST_MINMAX-Variables between 0 and 1 will be scaled using
          the possible minimum and possible maximum values for each variable.

          * THRESHOLD_PERCENTILE-Variables will be converted to percentiles
          between 0 and 1.

          * THRESHOLD_ZSCORE-Each variable will be standardized by subtracting
          the mean value and dividing by the standard deviation.

          * THRESHOLD_CUST_ZSCORE-Each variable will be standardized by
          subtracting a custom mean value and dividing by a custom standard
          deviation.

          * THRESHOLD_RAW-The values of the variable will be used without
          change. This is the default.
      pre_custom_zscore {Value Table}:
          The custom mean value and custom standard deviation that will be used
          when standardizing each input variable. For each variable, provide the
          custom mean in the Mean column and the custom standard deviation in
          the Standard Deviation column.
      pre_min_max {Value Table}:
          The possible minimum and maximum values that will be used in the units
          of the variables. Each variable will be scaled between 0 and 1 based
          on the possible minimum and maximum.
      pre_thresholds {Value Table}:
          The threshold that determines whether a feature will be flagged.
          Specify the value in the units of the scaled variables and specify
          whether values above or below the threshold value will be flagged.
      index_method {String}:
          Specifies the method that will be used to combine the scaled variables
          into a single value.

          * SUM-The values will be added.

          * MEAN-The arithmetic (additive) mean of the values will be
          calculated. This is the default.

          * PRODUCT-The values will be multiplied. All scaled values must be
          greater than or equal to zero.

          * GEOMETRIC_MEAN-The geometric (multiplicative) mean of the values
          will be calculated. All scaled values must be greater than or equal to
          zero.
          You cannot multiply or calculate a geometric mean when any variables
          are scaled using z-scores, because z-scores always contain negative
          values.
      index_weights {Value Table}:
          The weights that will set the relative influence of each input
          variable on the index. Each weight has a default value of 1, meaning
          that each variable has equal contribution. Increase or decrease the
          weights to reflect the relative importance of the variables. For
          example, if a variable is twice as important as another, use a weight
          value of 2. Using weight values larger than one while multiplying to
          combine scaled values can result in indices with very large values.
      out_index_name {String}:
          The name of the index. The value is used in the visualization of the
          outputs, such as field aliases and chart labels. The value is not used
          when the output (or appended input) is a shapefile.
      out_index_reverse {Boolean}:
          Specifies whether the output index values will be reversed in
          direction (for example, to treat high index values as low values).

          * REVERSE-The index values will be reversed in direction.

          * NO_REVERSE-The index values will not be reversed in direction. This
          is the default.
      post_min_max {Value Table}:
          The minimum and maximum of the output index values. This scaling is
          applied after combining the scaled variables. If no values are
          provided, the output index is not scaled.
      post_reclass {String}:
          Specifies the method that will be used to classify the output index.
          An additional output field will be provided for each selected option.

          * EQINTERVAL-Classes will be created by dividing the range of values
          into equally sized intervals

          * QUANTILE-Classes will be created in which each class includes an
          equal number of records.

          * STDDEV-Classes will be created corresponding to the number of
          standard deviations above and below the average of the index. The
          resulting values will be between -3 and 3.

          * CUST-Class breaks and classed values will be specified using the
          post_custom_classes parameter.
      post_num_classes {Long}:
          The number of classes that will be used for the equal interval and
          quantile classification methods.
      post_custom_classes {Value Table}:
          The upper bounds and classed values for the custom classification
          method. For example, you can use this variable to classify an index
          containing values between 0 and 100 into classes representing low,
          medium, and high values based on custom break values.

     OUTPUTS:
      out_table {Feature Class / Table}:
          The output features or table that will include the results."""
    ...

@gptooldoc('ClustersOutliers_stats', None)
def ClustersOutliers(Input_Feature_Class=..., Input_Field=..., Output_Feature_Class=..., Conceptualization_of_Spatial_Relationships=..., Distance_Method=..., Standardization=..., Distance_Band_or_Threshold_Distance=..., Weights_Matrix_File=..., Apply_False_Discovery_Rate__FDR__Correction=..., Number_of_Permutations=..., number_of_neighbors=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ClustersOutliers_stats(Input_Feature_Class, Input_Field, Output_Feature_Class, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Weights_Matrix_File}, {Apply_False_Discovery_Rate__FDR__Correction}, {Number_of_Permutations}, {number_of_neighbors})

        Given a set of weighted features, identifies statistically significant
        hot spots, cold spots, and spatial outliers using the Anselin Local
        Moran's I statistic.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class for which cluster and outlier analysis will be
          performed.
      Input_Field (Field):
          The numeric field to be evaluated.
      Conceptualization_of_Spatial_Relationships (String):
          Specifies how spatial relationships among features are defined.

          * INVERSE_DISTANCE-Nearby neighboring features have a larger influence
          on the computations for a target feature than features that are far
          away.

          * INVERSE_DISTANCE_SQUARED-Same as INVERSE_DISTANCE except that the
          slope is sharper, so influence drops off more quickly, and only a
          target feature's closest neighbors will exert substantial influence on
          computations for that feature.

          * FIXED_DISTANCE_BAND-Each feature is analyzed within the context of
          neighboring features. Neighboring features inside the specified
          critical distance (Distance_Band_or_Threshold_Distance) receive a
          weight of one and exert influence on computations for the target
          feature. Neighboring features outside the critical distance receive a
          weight of zero and have no influence on a target feature's
          computations.

          * ZONE_OF_INDIFFERENCE-Features within the specified critical distance
          (Distance_Band_or_Threshold_Distance) of a target feature receive a
          weight of one and influence computations for that feature. Once the
          critical distance is exceeded, weights (and the influence a
          neighboring feature has on target feature computations) diminish with
          distance.

          * K_NEAREST_NEIGHBORS-The closest k features are included in the
          analysis. The number of neighbors (k) is specified by the
          number_of_neighbors parameter.

          * CONTIGUITY_EDGES_ONLY-Only neighboring polygon features that share a
          boundary or overlap will influence computations for the target polygon
          feature.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share a boundary,
          share a node, or overlap will influence computations for the target
          polygon feature.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial relationships are defined by a
          specified spatial weights file. The path to the spatial weights file
          is specified by the Weights_Matrix_File parameter.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block); calculated by summing the
          (absolute) difference between the x- and y-coordinates
      Standardization (String):
          Row standardization is recommended whenever the distribution of your
          features is potentially biased due to sampling design or an imposed
          aggregation scheme.

          * NONE-No standardization of spatial weights is applied.

          * ROW-Spatial weights are standardized; each weight is divided by its
          row sum (the sum of the weights of all neighboring features).
      Distance_Band_or_Threshold_Distance {Double}:
          Specifies a cutoff distance for Inverse Distance and Fixed Distance
          options. Features outside the specified cutoff for a target feature
          are ignored in analyses for that feature. However, for Zone of
          Indifference, the influence of features outside the given distance is
          reduced with distance, while those inside the distance threshold are
          equally considered. The distance value entered should match that of
          the output coordinate system.For the Inverse Distance
          conceptualizations of spatial relationships,
          a value of 0 indicates that no threshold distance is applied; when
          this parameter is left blank, a default threshold value is computed
          and applied. This default value is the Euclidean distance that ensures
          every feature has at least one neighbor.This parameter has no effect
          when Polygon Contiguity or Get Spatial
          Weights From File spatial conceptualizations are selected.
      Weights_Matrix_File {File}:
          The path to a file containing weights that define spatial, and
          potentially temporal, relationships among features.
      Apply_False_Discovery_Rate__FDR__Correction {Boolean}:
          * APPLY_FDR-Statistical significance will be based on the False
          Discovery Rate correction for a 95 percent confidence level.

          * NO_FDR-Features with p-values less than 0.05 will appear in the
          COType field reflecting statistically significant clusters or outliers
          at a 95 percent confidence level (default).
      Number_of_Permutations {Long}:
          The number of random permutations for the calculation of pseudo
          p-values. The default number of permutations is 499. If you choose 0
          permutations, the standard p-value is calculated.

          * 0-Permutations are not used and a standard p-value is calculated.

          * 99-With 99 permutations, the smallest possible pseudo p-value is
          0.01 and all other pseudo p-values will be multiples of this value.

          * 199-With 199 permutations, the smallest possible pseudo p-value is
          0.005 and all other possible pseudo p-values will be multiples of this
          value.

          * 499-With 499 permutations, the smallest possible pseudo p-value is
          0.002 and all other pseudo p-values will be multiples of this value.

          * 999-With 999 permutations, the smallest possible pseudo p-value is
          0.001 and all other pseudo p-values will be multiples of this value.

          * 9999-With 9999 permutations, the smallest possible pseudo p-value is
          0.0001 and all other pseudo p-values will be multiples of this value.
      number_of_neighbors {Long}:
          The number of neighbors to include in the analysis.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          The output feature class to receive the results fields."""
    ...

@gptooldoc('DensityBasedClustering_stats', None)
def DensityBasedClustering(in_features=..., output_features=..., cluster_method=..., min_features_cluster=..., search_distance=..., cluster_sensitivity=..., time_field=..., search_time_interval=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DensityBasedClustering_stats(in_features, output_features, cluster_method, min_features_cluster, {search_distance}, {cluster_sensitivity}, {time_field}, {search_time_interval})

        Finds clusters of point features within surrounding noise based on
        their spatial distribution. Time can also be incorporated to find
        space-time clusters.

     INPUTS:
      in_features (Feature Layer):
          The point features for which density-based clustering will be
          performed.
      cluster_method (String):
          Specifies the method that will be used to define clusters.

          * DBSCAN-A specified distance will be used to separate dense clusters
          from sparser noise. DBSCAN is the fastest of the clustering methods
          but is only appropriate if there is a clear distance to use that works
          well to define all clusters that may be present. This results in
          clusters that have similar densities.

          * HDBSCAN-Varying distances will be used to separate clusters of
          varying densities from sparser noise. HDBSCAN is the most data-driven
          of the clustering methods and requires the least user input.

          * OPTICS-The distance between neighbors and a reachability plot will
          be used to separate clusters of varying densities from noise. OPTICS
          offers the most flexibility in fine-tuning the clusters that are
          detected, though it is computationally intensive, particularly with a
          large search distance.
      min_features_cluster (Long):
          The minimum number of points that will be considered a cluster. Any
          cluster with fewer points than the number provided will be considered
          noise.
      search_distance {Linear Unit}:
          The maximum distance that will be considered.For the cluster_method
          parameter's DBSCAN option, the
          min_features_cluster parameter value must be found within this
          distance for cluster membership. Individual clusters will be separated
          by at least this distance. If a point is located farther than this
          distance from the next closest point in the cluster, it will not be
          included in the cluster.For the cluster_method parameter's OPTICS
          option, this parameter is
          optional and is used as the maximum search distance when creating the
          reachability plot. For OPTICS, the reachability plot, combined with
          the cluster_sensitivity parameter value, determines cluster
          membership. If no distance is specified, the tool will search all
          distances, which will increase processing time.If left blank, the
          default distance used will be the highest core
          distance found in the dataset, excluding those core distances in the
          top 1 percent (he most extreme core distances). If the time_field
          parameter value is provided, a search distance must be provided and
          does not include a default value.
      cluster_sensitivity {Long}:
          An integer between 0 and 100 that determines the compactness of
          clusters. A number close to 100 will result in a higher number of
          dense clusters. A number close to 0 will result in fewer, less compact
          clusters. If left blank, the tool will find a sensitivity value using
          the Kullback-Leibler divergence that finds the value in which adding
          more clusters does not add additional information.
      time_field {Field}:
          The field containing the time stamp for each record in the dataset.
          This field must be of type Date. If provided, the tool will find
          clusters of points that are close to each other in space and time. The
          search_time_interval parameter value must be provided to determine
          whether a point is close enough in time to a cluster to be included in
          the cluster.
      search_time_interval {Time Unit}:
          The time interval that will be used to determine whether points form a
          space-time cluster. The search time interval spans before and after
          the time of each point; for example, an interval of 3 days around a
          point will include all points starting 3 days before and ending 3 days
          after the time of the point.

          * For the cluster_method parameter's DBSCAN option, the
          min_features_cluster value specified must be found within the search
          distance and the search time interval to be included in a cluster.

          * For the cluster_method parameter's OPTICS option, all points outside
          of the search time interval will be excluded when calculating core
          distances, neighbor-distances, and reachability distances.
          The search time interval does not control the overall time span of the
          resulting space-time clusters. The time span of points within a
          cluster can be larger than the search time interval as long as each
          point has neighbors within the cluster that are within the search time
          interval.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class that will receive the cluster results."""
    ...

@gptooldoc('GroupingAnalysis_stats', None)
def GroupingAnalysis(Input_Features=..., Unique_ID_Field=..., Output_Feature_Class=..., Number_of_Groups=..., Analysis_Fields=..., Spatial_Constraints=..., Distance_Method=..., Number_of_Neighbors=..., Weights_Matrix_File=..., Initialization_Method=..., Initialization_Field=..., Output_Report_File=..., Evaluate_Optimal_Number_of_Groups=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GroupingAnalysis_stats(Input_Features, Unique_ID_Field, Output_Feature_Class, Number_of_Groups, Analysis_Fields;Analysis_Fields..., Spatial_Constraints, {Distance_Method}, {Number_of_Neighbors}, {Weights_Matrix_File}, {Initialization_Method}, {Initialization_Field}, {Output_Report_File}, {Evaluate_Optimal_Number_of_Groups})

        Groups features based on feature attributes and optional spatial or
        temporal constraints.

     INPUTS:
      Input_Features (Feature Layer):
          The feature class or feature layer for which you want to create
          groups.
      Unique_ID_Field (Field):
          An integer field containing a different value for every feature in the
          input feature class. If you don't have a Unique ID field, you can
          create one by adding an integer field to your feature class table and
          calculating the field values to equal the FID or OBJECTID field.
      Number_of_Groups (Long):
          The number of groups to create. The Output Report parameter will be
          disabled for more than 15 groups.
      Analysis_Fields (Field):
          A list of fields you want to use to distinguish one group from
          another. The Output Report parameter will be disabled for more than 15
          fields.
      Spatial_Constraints (String):
          Specifies if and how spatial relationships among features should
          constrain the groups created.

          * CONTIGUITY_EDGES_ONLY-Groups contain contiguous polygon features.
          Only polygons that share an edge can be part of the same group.

          * CONTIGUITY_EDGES_CORNERS-Groups contain contiguous polygon features.
          Only polygons that share an edge or a vertex can be part of the same
          group.

          * DELAUNAY_TRIANGULATION-Features in the same group will have at least
          one natural neighbor in common with another feature in the group.
          Natural neighbor relationships are based on Delaunay Triangulation.
          Conceptually, Delaunay Triangulation creates a nonoverlapping mesh of
          triangles from feature centroids. Each feature is a triangle node and
          nodes that share edges are considered neighbors.

          * K_NEAREST_NEIGHBORS-Features in the same group will be near each
          other; each feature will be a neighbor of at least one other feature
          in the group. Neighbor relationships are based on the nearest K
          features where you specify an Integer value, K, for the
          Number_of_Neighbors parameter.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial, and optionally temporal,
          relationships are defined by a spatial weights file (.swm). Create the
          spatial weights matrix file using the Generate Spatial Weights Matrix
          tool or the Generate Network Spatial Weights tool.

          * NO_SPATIAL_CONSTRAINT-Features will be grouped using data space
          proximity only. Features do not have to be near each other in space or
          time to be part of the same group.
      Distance_Method {String}:
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN-The straight-line distance between two points (as the crow
          flies)

          * MANHATTAN-The distance between two points measured along axes at
          right angles (city block); calculated by summing the (absolute)
          difference between the x- and y-coordinates
      Number_of_Neighbors {Long}:
          This parameter may be specified whenever the Spatial_Constraints
          parameter is K_NEAREST_NEIGHBORS or one of the contiguity methods
          (CONTIGUITY_EDGES_ONLY or CONTIGUITY_EDGES_CORNERS). The default
          number of neighbors is 8 and cannot be smaller than 2 for
          K_NEAREST_NEIGHBORS. This value reflects the exact number of nearest
          neighbor candidates to consider when building groups. A feature will
          not be included in a group unless one of the other features in that
          group is a K nearest neighbor. The default for CONTIGUITY_EDGES_ONLY
          and CONTIGUITY_EDGES_CORNERS is 0. For the contiguity methods, this
          value reflects the minimum number of neighbor candidates to consider.
          Additional nearby neighbors for features with less than the
          Number_of_Neighbors specified will be based on feature centroid
          proximity.
      Weights_Matrix_File {File}:
          The path to a file containing spatial weights that define spatial
          relationships among features.
      Initialization_Method {String}:
          Specifies how initial seeds are obtained when the Spatial_Constraint
          parameter selected is NO_SPATIAL_CONSTRAINT. Seeds are used to grow
          groups. If you indicate you want three groups, for example, the
          analysis will begin with three seeds.

          * FIND_SEED_LOCATIONS-Seed features will be selected to optimize
          performance.

          * GET_SEEDS_FROM_FIELD-Nonzero entries in the Initialization Field
          will be used as starting points to grow groups.

          * USE_RANDOM_SEEDS-Initial seed features will be randomly selected.
      Initialization_Field {Field}:
          The numeric field identifying seed features. Features with a value of
          1 for this field will be used to grow groups.
      Evaluate_Optimal_Number_of_Groups {Boolean}:
          * EVALUATE-Groupings from 2 to 15 will be evaluated.

          * DO_NOT_EVALUATE-No evaluation of the number of groups will be
          performed. This is the default.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          The new output feature class created containing all features, the
          analysis fields specified, and a field indicating to which group each
          feature belongs.
      Output_Report_File {File}:
          The full path for the PDF report file to be created summarizing group
          characteristics. This report provides a number of graphs to help you
          compare the characteristics of each group. Creating the report file
          can add substantial processing time."""
    ...

@gptooldoc('HotSpotAnalysisComparison_stats', None)
def HotSpotAnalysisComparison(in_hot_spot_1=..., in_hot_spot_2=..., out_features=..., num_neighbors=..., num_perms=..., weighting_method=..., similarity_weights=..., in_weights_table=..., exclude_nonsig_features=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """HotSpotAnalysisComparison_stats(in_hot_spot_1, in_hot_spot_2, out_features, {num_neighbors}, {num_perms}, {weighting_method}, {similarity_weights;similarity_weights...}, {in_weights_table}, {exclude_nonsig_features})

        Compares two hot spot analysis result layers and measures their
        similarity and association.

     INPUTS:
      in_hot_spot_1 (Feature Layer):
          The first hot spot analysis result layer.
      in_hot_spot_2 (Feature Layer):
          The second hot spot analysis result layer.
      num_neighbors {Long}:
          The number of neighbors around each feature that will be used for
          distance weighting. Distance weighting is one component of the overall
          similarity, and any features with matching significance levels within
          the neighborhood will be considered partial matches when calculating
          similarity and association.
      num_perms {Long}:
          The number of permutations that will be used to estimate the expected
          similarity and kappa values. A larger number of simulations will
          increase the precision of the estimates but will also increase
          calculation time.

          * 99-The analysis will use 99 permutations.

          * 199-The analysis will use 199 permutations.

          * 499-The analysis will use 499 permutations. This is the default.

          * 999-The analysis will use 999 permutations.

          * 9999-The analysis will use 9,999 permutations.
      weighting_method {String}:
          Specifies how similarity weights between significance level categories
          will be defined. Similarity weights are numbers between 0 and 1 that
          define the categories of one result that are expected to match the
          categories of the other result. A value of 1 indicates that the
          categories will be considered exactly the same, and a value of 0
          indicates that the categories will be considered completely different.
          Values between 0 and 1 indicate degrees of partial similarity between
          the categories. For example, 99% significant hot spots can be
          considered perfectly similar to other 99% hot spots, partially similar
          to 95% hot spots, and completely dissimilar to 99% cold spots.

          * FUZZY-Similarity weights will be fuzzy (nonbinary) and determined by
          the closeness of significance levels. For example, 99% significant hot
          spots will be perfectly similar to other 99% significant hot spots
          (weight = 1), but they will be partially similar to 95% significant
          hot spots (weight=0.71) and 90% significant hot spots (weight = 0.55).
          The weight between 95% significant and 90% significant is 0.78. All
          hot spots will be completely dissimilar to all cold spots and
          nonsignificant features (weight = 0). This is the default.

          * EXACT_MATCH-Features must have the same significance level to be
          considered similar. For example, 99% significant hot spots will be
          considered completely dissimilar to 95% and 90% significant hot spots.

          * ABOVE_90-Features that are 90%, 95%, and 99% significant hot spots
          will be considered perfectly similar to each other, and all features
          that are 90%, 95%, and 99% significant cold spots will be considered
          perfectly similar to each other. This option treats all features at or
          above 90% significance as being the same (statistically significant)
          and all features below 90% confidence as being the same
          (nonsignificant). This option is recommended when the hot spot
          analyses were performed at a 90% significance level.

          * ABOVE_95-Features that are 95% and 99% significant hot (or cold)
          spots will be considered perfectly similar, and features that are 95%
          and 99% significant cold spots will be considered perfectly similar.
          For example, 90% significant hot and cold spots will be considered
          completely dissimilar to higher significance levels. This option
          treats all features at or above 95% confidence as being the same
          (statistically significant) and all features below 95% confidence as
          being the same (nonsignificant). This option is recommended when the
          hot spot analyses were performed at a 95% significance level.

          * ABOVE_99-Only features that are 99% significant hot (or cold) spots
          will be considered perfectly similar to each other. This option treats
          all features below 99% significance as being nonsignificant. This
          option is recommended when the hot spot analyses were performed at a
          99% significance level.

          * CUSTOM-Custom similarity weights provided in the similarity_weights
          parameter will be used.

          * TABLE-Similarity weights between significance levels will be defined
          by an input table. Provide the table in the in_weights_table
          parameter.

          * REVERSE-The default fuzzy weights will be used, but hot spots of the
          first hot spot result will be considered similar to the cold spots of
          the second hot spot result. For example, 99% significant hot spots in
          one result will be considered perfectly similar to 99% cold spots in
          the other result and partially similar to 95% and 90% cold spots in
          the other result. This option is recommended when the hot spot
          analysis variables have a negative relationship. For example, you can
          measure how closely hot spots of infant mortality correspond to cold
          spots of healthcare access.
      similarity_weights {Value Table}:
          The custom similarity weights between significance level categories.
          The weights are values between 0 and 1 and indicate how similar to
          consider the two categories. A value of 0 indicates the categories are
          completely dissimilar, a value of 1 indicates the values are perfectly
          similar, and values between 0 and 1 indicate the categories are
          partially similar.
      in_weights_table {Table View}:
          The table containing custom similarity weights for each combination of
          hot spot significance level categories. The table must contain
          CATEGORY1, CATEGORY2, and WEIGHT fields. Provide the significance
          level categories of the pair (the Gi_Bin field values of the input
          layers) in the category fields and the similarity weight between them
          in the weight field. If a combination is not provided in the table,
          the weight for the combination is assumed to be 0.
      exclude_nonsig_features {Boolean}:
          Specifies whether pairs of features will be excluded from the
          comparisons if both hot spot results are nonsignificant. If excluded,
          conditional similarity and kappa values will be calculated that
          compare only the statistically significant hot and cold spots.
          Excluding features is recommended when you are interested only in
          whether the hot and cold spots of the input layers align, not whether
          the nonsignificant areas align, such as comparing whether hot and cold
          spots of median income correspond to hot and cold spots of food
          access.

          * EXCLUDE-Nonsignificant features will be excluded, and the
          comparisons will be conditional on statistically significant hot and
          cold spots.

          * NO_EXCLUDE-Nonsignificant features will be included. This is the
          default.
          If any significance level categories are assigned a similarity weight
          of 1 to the nonsignificant category (indicating that the category will
          be treated the same as the nonsignificant category), features with
          that category will also be excluded from comparisons if they are
          paired with another nonsignificant feature.

     OUTPUTS:
      out_features (Feature Class):
          The output feature class that will contain the local measures of
          similarity and association."""
    ...

@gptooldoc('HotSpots_stats', None)
def HotSpots(Input_Feature_Class=..., Input_Field=..., Output_Feature_Class=..., Conceptualization_of_Spatial_Relationships=..., Distance_Method=..., Standardization=..., Distance_Band_or_Threshold_Distance=..., Self_Potential_Field=..., Weights_Matrix_File=..., Apply_False_Discovery_Rate__FDR__Correction=..., number_of_neighbors=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """HotSpots_stats(Input_Feature_Class, Input_Field, Output_Feature_Class, Conceptualization_of_Spatial_Relationships, Distance_Method, Standardization, {Distance_Band_or_Threshold_Distance}, {Self_Potential_Field}, {Weights_Matrix_File}, {Apply_False_Discovery_Rate__FDR__Correction}, {number_of_neighbors})

        Given a set of weighted features, identifies statistically significant
        hot spots and cold spots using the Getis-Ord Gi* statistic.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class for which hot spot analysis will be performed.
      Input_Field (Field):
          The numeric field (number of victims, crime rate, test scores, and so
          on) to be evaluated.
      Conceptualization_of_Spatial_Relationships (String):
          Specifies how spatial relationships among features will be defined.

          * INVERSE_DISTANCE-Nearby neighboring features will have a larger
          influence on the computations for a target feature than features that
          are far away.

          * INVERSE_DISTANCE_SQUARED-Same as INVERSE_DISTANCE except that the
          slope is sharper, so influence will drop off more quickly, and only a
          target feature's closest neighbors will exert substantial influence on
          computations for that feature.

          * FIXED_DISTANCE_BAND-Each feature will be analyzed within the context
          of neighboring features. Neighboring features inside the specified
          critical distance (Distance_Band_or_Threshold) will receive a weight
          of one and exert influence on computations for the target feature.
          Neighboring features outside the critical distance will receive a
          weight of zero and have no influence on a target feature's
          computations.

          * ZONE_OF_INDIFFERENCE-Features within the specified critical distance
          (Distance_Band_or_Threshold) of a target feature will receive a weight
          of one and influence computations for that feature. Once the critical
          distance is exceeded, weights (and the influence a neighboring feature
          has on target feature computations) will diminish with distance.

          * K_NEAREST_NEIGHBORS-The closest k features will be included in the
          analysis; k is a specified numeric parameter.

          * CONTIGUITY_EDGES_ONLY-Only neighboring polygon features that share a
          boundary or overlap will influence computations for the target polygon
          feature.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share a boundary,
          share a node, or overlap will influence computations for the target
          polygon feature.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial relationships will be defined
          by a specified spatial weights file. The path to the spatial weights
          file is specified by the Weights_Matrix_File parameter.
      Distance_Method (String):
          Specifies how distances will be calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block), calculated by summing the
          (absolute) difference between the x- and y-coordinates
      Standardization (String):
          Row standardization has no impact on this tool: results from Hot Spot
          Analysis (the Getis-Ord Gi* statistic) would be identical with or
          without row standardization. The parameter is disabled; it remains as
          a tool parameter only to support backward compatibility.

          * NONE-No standardization of spatial weights is applied.

          * ROW-No standardization of spatial weights is applied.
      Distance_Band_or_Threshold_Distance {Double}:
          Specifies a cutoff distance for the inverse distance and fixed
          distance options. Features outside the specified cutoff for a target
          feature will be ignored in analyses for that feature. However, for
          ZONE_OF_INDIFFERENCE, the influence of features outside the given
          distance will be reduced with distance, while those inside the
          distance threshold will be equally considered. The distance value
          entered should match that of the output coordinate system.For the
          inverse distance conceptualizations of spatial relationships,
          a value of 0 indicates that no threshold distance will be applied;
          when this parameter is left blank, a default threshold value will be
          computed and applied. The default value is the Euclidean distance,
          which ensures that every feature has at least one neighbor.This
          parameter has no effect when polygon contiguity
          (CONTIGUITY_EDGES_ONLY or CONTIGUITY_EDGES_CORNERS) or
          GET_SPATIAL_WEIGHTS_FROM_FILE spatial conceptualization is selected.
      Self_Potential_Field {Field}:
          The field representing self potential: the distance or weight between
          a feature and itself.
      Weights_Matrix_File {File}:
          The path to a file containing weights that define spatial, and
          potentially temporal, relationships among features.
      Apply_False_Discovery_Rate__FDR__Correction {Boolean}:
          * APPLY_FDR-Statistical significance will be based on the FDR
          correction.

          * NO_FDR-Statistical significance will not be based on the FDR
          correction; it will be based on the p-value and z-score fields. This
          is the default.
      number_of_neighbors {Long}:
          An integer specifying the number of neighbors to include in the
          analysis.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          The output feature class to receive the z-score and p-value results."""
    ...

@gptooldoc('MultivariateClustering_stats', None)
def MultivariateClustering(in_features=..., output_features=..., analysis_fields=..., clustering_method=..., initialization_method=..., initialization_field=..., number_of_clusters=..., output_table=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MultivariateClustering_stats(in_features, output_features, analysis_fields;analysis_fields..., {clustering_method}, {initialization_method}, {initialization_field}, {number_of_clusters}, {output_table})

        Finds natural clusters of features based solely on feature attribute
        values.

     INPUTS:
      in_features (Feature Layer):
          The feature class or feature layer for which clusters will be created.
      analysis_fields (Field):
          A list of fields that will be used to distinguish one cluster from
          another.
      clustering_method {String}:
          Specifies the clustering algorithm that will be used.The K_MEANS and
          K_MEDOIDS options generally produce similar results.
          However, K_MEDOIDS is more robust to noise and outliers in the
          in_features parameter value. K_MEANS is generally faster than
          K_MEDOIDS and is recommended for large data sets.

          * K_MEANS-The in_features parameter value will be clustered using the
          K means algorithm. This is the default.

          * K_MEDOIDS-The in_features parameter value will be clustered using
          the K medoids algorithm.
      initialization_method {String}:
          Specifies how initial seeds to grow clusters are obtained. If you
          indicate you want three clusters, for example, the analysis will begin
          with three seeds.

          * OPTIMIZED_SEED_LOCATIONS-Seed features will be selected to optimize
          analysis results and performance. This is the default.

          * USER_DEFINED_SEED_LOCATIONS-Nonzero entries in the
          initialization_field parameter value will be used as starting points
          to grow clusters.

          * RANDOM_SEED_LOCATIONS-Initial seed features will be randomly
          selected.
      initialization_field {Field}:
          The numeric field identifying seed features. Features with a value of
          1 for this field will be used to grow clusters. Each seed results in a
          cluster, so at least two seed features must be provided.
      number_of_clusters {Long}:
          The number of clusters that will be created. If you leave this
          parameter empty, the tool will evaluate the optimal number of clusters
          by computing a pseudo F-statistic for clustering solutions with 2
          through 30 clusters.This parameter is disabled if the seed locations
          were provided in an
          initialization field.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class that will be created containing all features,
          the analysis fields specified, and a field indicating to which cluster
          each feature belongs.
      output_table {Table}:
          The table containing the pseudo F-statistic for clustering solutions 2
          through 30, calculated to evaluate the optimal number of clusters. The
          chart created from this table can be accessed in the stand-alone
          tables section of the Contents pane."""
    ...

@gptooldoc('OptimizedHotSpotAnalysis_stats', None)
def OptimizedHotSpotAnalysis(Input_Features=..., Output_Features=..., Analysis_Field=..., Incident_Data_Aggregation_Method=..., Bounding_Polygons_Defining_Where_Incidents_Are_Possible=..., Polygons_For_Aggregating_Incidents_Into_Counts=..., Density_Surface=..., Cell_Size=..., Distance_Band=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """OptimizedHotSpotAnalysis_stats(Input_Features, Output_Features, {Analysis_Field}, {Incident_Data_Aggregation_Method}, {Bounding_Polygons_Defining_Where_Incidents_Are_Possible}, {Polygons_For_Aggregating_Incidents_Into_Counts}, {Density_Surface}, {Cell_Size}, {Distance_Band})

        Given incident points or weighted features (points or polygons),
        creates a map of statistically significant hot and cold spots using
        the Getis-Ord Gi* statistic. It evaluates the characteristics of the
        input feature class to produce optimal results.

     INPUTS:
      Input_Features (Feature Layer):
          The point or polygon feature class for which hot spot analysis will be
          performed.
      Analysis_Field {Field}:
          The numeric field (number of incidents, crime rates, test scores, and
          so on) to be evaluated.
      Incident_Data_Aggregation_Method {String}:
          The aggregation method to use to create weighted features for analysis
          from incident point data.

          * COUNT_INCIDENTS_WITHIN_FISHNET_POLYGONS-A fishnet polygon mesh will
          overlay the incident point data and the number of incidents within
          each polygon cell will be counted. If no bounding polygon is provided
          in the Bounding_Polygons_Defining_Where_Incidents_Are_Possible
          parameter, only cells with at least one incident will be used in the
          analysis; otherwise, all cells within the bounding polygons will be
          analyzed.

          * COUNT_INCIDENTS_WITHIN_HEXAGON_POLYGONS-A hexagon polygon mesh will
          overlay the incident point data and the number of incidents within
          each polygon cell will be counted. If no bounding polygon is provided
          in the Bounding_Polygons_Defining_Where_Incidents_Are_Possible
          parameter, only cells with at least one incident will be used in the
          analysis; otherwise, all cells within the bounding polygons will be
          analyzed.

          * COUNT_INCIDENTS_WITHIN_AGGREGATION_POLYGONS-You provide aggregation
          polygons to overlay the incident point data in the
          Polygons_For_Aggregating_Incidents_Into_Counts parameter. The
          incidents within each polygon are counted.

          * SNAP_NEARBY_INCIDENTS_TO_CREATE_WEIGHTED_POINTS-Nearby incidents
          will be aggregated together to create a single weighted point. The
          weight for each point is the number of aggregated incidents at that
          location.
      Bounding_Polygons_Defining_Where_Incidents_Are_Possible {Feature Layer}:
          A polygon feature class defining where the incident Input_Features
          could possibly occur.
      Polygons_For_Aggregating_Incidents_Into_Counts {Feature Layer}:
          The polygons to use to aggregate the incident Input_Features in order
          to get an incident count for each polygon feature.
      Cell_Size {Linear Unit}:
          The size of the grid cells used to aggregate the Input_Features. When
          aggregating into a hexagon grid, this distance is used as the height
          to construct the hexagon polygons.
      Distance_Band {Linear Unit}:
          The spatial extent of the analysis neighborhood. This value determines
          which features are analyzed together in order to assess local
          clustering.

     OUTPUTS:
      Output_Features (Feature Class):
          The output feature class to receive the z-score, p-value, and Gi_Bin
          results.
      Density_Surface {Raster Dataset}:
          The Density_Surface parameter is disabled; it remains as a tool
          parameter only to support backwards compatibility. The Kernel Density
          tool can be used if you would like a density surface visualization of
          your weighted points."""
    ...

@gptooldoc('OptimizedOutlierAnalysis_stats', None)
def OptimizedOutlierAnalysis(Input_Features=..., Output_Features=..., Analysis_Field=..., Incident_Data_Aggregation_Method=..., Bounding_Polygons_Defining_Where_Incidents_Are_Possible=..., Polygons_For_Aggregating_Incidents_Into_Counts=..., Performance_Adjustment=..., Cell_Size=..., Distance_Band=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """OptimizedOutlierAnalysis_stats(Input_Features, Output_Features, {Analysis_Field}, {Incident_Data_Aggregation_Method}, {Bounding_Polygons_Defining_Where_Incidents_Are_Possible}, {Polygons_For_Aggregating_Incidents_Into_Counts}, {Performance_Adjustment}, {Cell_Size}, {Distance_Band})

        Given incident points or weighted features (points or polygons),
        creates a map of statistically significant hot spots, cold spots, and
        spatial outliers using the Anselin Local Moran's I statistic. It
        evaluates the characteristics of the input feature class to produce
        optimal results.

     INPUTS:
      Input_Features (Feature Layer):
          The point or polygon feature class for which the cluster and outlier
          analysis will be performed.
      Analysis_Field {Field}:
          The numeric field (number of incidents, crime rates, test scores, and
          so on) to be evaluated.
      Incident_Data_Aggregation_Method {String}:
          The aggregation method to use to create weighted features for analysis
          from incident point data.

          * COUNT_INCIDENTS_WITHIN_FISHNET_POLYGONS-A fishnet polygon mesh will
          overlay the incident point data and the number of incidents within
          each polygon cell will be counted. If no bounding polygon is provided
          in the Bounding_Polygons_Defining_Where_Incidents_Are_Possible
          parameter, only cells with at least one incident will be used in the
          analysis; otherwise, all cells within the bounding polygons will be
          analyzed.

          * COUNT_INCIDENTS_WITHIN_HEXAGON_POLYGONS-A hexagon polygon mesh will
          overlay the incident point data and the number of incidents within
          each polygon cell will be counted. If no bounding polygon is provided
          in the Bounding_Polygons_Defining_Where_Incidents_Are_Possible
          parameter, only cells with at least one incident will be used in the
          analysis; otherwise, all cells within the bounding polygons will be
          analyzed.

          * COUNT_INCIDENTS_WITHIN_AGGREGATION_POLYGONS-You provide aggregation
          polygons to overlay the incident point data in the
          Polygons_For_Aggregating_Incidents_Into_Counts parameter. The
          incidents within each polygon are counted.

          * SNAP_NEARBY_INCIDENTS_TO_CREATE_WEIGHTED_POINTS-Nearby incidents
          will be aggregated together to create a single weighted point. The
          weight for each point is the number of aggregated incidents at that
          location.
      Bounding_Polygons_Defining_Where_Incidents_Are_Possible {Feature Layer}:
          A polygon feature class defining where the incident Input_Features
          could possibly occur.
      Polygons_For_Aggregating_Incidents_Into_Counts {Feature Layer}:
          The polygons to use to aggregate the incident Input_Features in order
          to get an incident count for each polygon feature.
      Performance_Adjustment {String}:
          This analysis utilizes permutations to create a reference
          distribution. Choosing the number of permutations is a balance between
          precision and increased processing time. Choose your preference for
          speed versus precision. More robust and precise results take longer to
          calculate.

          * QUICK_199-With 199 permutations, the smallest possible pseudo
          p-value is 0.005 and all other pseudo p-values will be even multiples
          of this value.

          * BALANCED_499-With 499 permutations, the smallest possible pseudo
          p-value is 0.002 and all other pseudo p-values will be even multiples
          of this value.

          * ROBUST_999-With 999 permutations, the smallest possible pseudo
          p-value is 0.001 and all other pseudo p-values will be even multiples
          of this value.
      Cell_Size {Linear Unit}:
          The size of the grid cells used to aggregate the Input_Features. When
          aggregating into a hexagon grid, this distance is used as the height
          to construct the hexagon polygons.
      Distance_Band {Linear Unit}:
          The spatial extent of the analysis neighborhood. This value determines
          which features are analyzed together in order to assess local
          clustering.

     OUTPUTS:
      Output_Features (Feature Class):
          The output feature class to receive the result fields."""
    ...

@gptooldoc('SimilaritySearch_stats', None)
def SimilaritySearch(Input_Features_To_Match=..., Candidate_Features=..., Output_Features=..., Collapse_Output_To_Points=..., Most_Or_Least_Similar=..., Match_Method=..., Number_Of_Results=..., Attributes_Of_Interest=..., Fields_To_Append_To_Output=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SimilaritySearch_stats(Input_Features_To_Match, Candidate_Features, Output_Features, Collapse_Output_To_Points, Most_Or_Least_Similar, Match_Method, Number_Of_Results, Attributes_Of_Interest;Attributes_Of_Interest..., {Fields_To_Append_To_Output;Fields_To_Append_To_Output...})

        Identifies which candidate features are most similar or most
        dissimilar to one or more input features based on feature attributes.

     INPUTS:
      Input_Features_To_Match (Feature Layer):
          The layer, or a selection on a layer, containing the features you want
          to match; you are searching for other features that look like these
          features. When more than one feature is provided, matching is based on
          attribute averages. When the Input Features To Match and
          Candidate Features
          values are from a single dataset layer, you can do the following:

          * Copy the layer to the Contents pane, making a duplicate layer.

          * Rename the duplicate layer.

          * On the renamed layer, make a selection or set a definition query for
          the reference features you want to match. Use the new layer created
          for the Input Features To Match parameter.

          * Apply a selection or set a definition query on the original layer so
          it excludes the reference features. This will be the layer to use for
          the Candidate Features parameter.
      Candidate_Features (Feature Layer):
          The layer, or a selection on a layer, containing candidate matching
          features. The tool will check for features most similar (or most
          dissimilar) to the Input_Features_To_Match values among these
          candidates.
      Collapse_Output_To_Points (Boolean):
          Specifies whether the geometry for the Output_Features parameter will
          be collapsed to points or will match the original geometry (lines or
          polygons) of the input features if the Input_Features_To_Match and
          Candidate_Features parameter values are both either lines or polygons.
          This parameter is only available with an Desktop Advanced license.
          Choosing COLLAPSE will improve tool performance for large line and
          polygon datasets.

          * COLLAPSE-The line and polygon features will be represented as
          feature centroids (points).

          * NO_COLLAPSE-The output geometry will match the line or polygon
          geometry of the input features. This is the default.
      Most_Or_Least_Similar (String):
          Specifies whether features that are most similar or most dissimilar to
          the Input_Features_To_Match values will be identified.

          * MOST_SIMILAR-Features that are most similar will be identified. This
          is the default.

          * LEAST_SIMILAR-Features that are most dissimilar will be identified.

          * BOTH-Features that are most similar and features that are most
          dissimilar will both be identified.
      Match_Method (String):
          Specifies whether matching will be based on values, ranks, or cosine
          relationships.

          * ATTRIBUTE_VALUES-Matching will be based on the sum of squared
          standardized attribute value differences for all of the Attributes Of
          Interest values. This is the default.

          * RANKED_ATTRIBUTE_VALUES-Matching will be based on the sum of squared
          rank differences for all of the Attributes Of Interest values.

          * ATTRIBUTE_PROFILES-Matching will be computed as a function of cosine
          similarity for all of the Attributes Of Interest values.
      Number_Of_Results (Long):
          The number of solution matches to find. Entering zero or a number
          larger than the total number of Candidate_Features values will return
          rankings for all the candidate features. The default is 10.
      Attributes_Of_Interest (Field):
          The numeric attributes representing the matching criteria.
      Fields_To_Append_To_Output {Field}:
          The fields to include with the Output_Features parameter. These fields
          are not used to determine similarity; they are only included in the
          Output_Features parameter for reference.

     OUTPUTS:
      Output_Features (Feature Class):
          The output feature class containing a record for each of the
          Input_Features_To_Match values and for all the solution-matching
          features found."""
    ...

@gptooldoc('SpatialOutlierDetection_stats', None)
def SpatialOutlierDetection(in_features=..., output_features=..., n_neighbors=..., percent_outlier=..., output_raster=..., outlier_type=..., sensitivity=..., keep_type=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SpatialOutlierDetection_stats(in_features, output_features, {n_neighbors}, {percent_outlier}, {output_raster}, {outlier_type}, {sensitivity}, {keep_type})

        Identifies global or local spatial outliers in point features.

     INPUTS:
      in_features (Feature Layer):
          The point features that will be used to build the spatial outlier
          detection model. Each point will be classified as an outlier or inlier
          based on its local outlier factor.
      n_neighbors {Long}:
          The number of neighbors that will be used to detect spatial outliers
          for each input point.For local outlier detection, the value must be at
          least 2, and all
          features within the neighborhood will be used as neighbors. If no
          value is specified, a value is estimated at run time and is displayed
          as a geoprocessing message.For global outlier detection, only the
          farthest neighbor in the
          neighborhood will be used, and the default is 1 (the closest
          neighbor). For example, a value of 3 indicates that global outliers
          are detected using distances to the third nearest neighbor of each
          point.
      percent_outlier {Double}:
          The percent of locations that will be identified as spatial outliers
          by defining the threshold of the local outlier factor. If no value is
          specified, a value is estimated at run time and is displayed as a
          geoprocessing message. A maximum of 50 percent of the features can be
          identified as spatial outliers.
      outlier_type {String}:
          Specifies the type of outlier that will be detected. A global outlier
          is a point that is far away from all other points in the feature
          class. A local outlier is a point that is farther away from its
          neighbors than would be expected by the density of points in the
          surrounding area.

          * GLOBAL-Global outliers of input points will be detected. This is the
          default.

          * LOCAL-Local outliers of input points will be detected.
      sensitivity {String}:
          Specifies the sensitivity level that will be used to detect global
          outliers. The higher the sensitivity, the more points that will be
          detected as outliers.The sensitivity value will determine the
          threshold, and any point with
          a neighbor distance larger than this threshold will be identified as a
          global outlier. The thresholds are determined using the box plot rule,
          in which the threshold for high sensitivity is one interquartile range
          above the third quartile. For medium sensitivity, the threshold is 1.5
          interquartile ranges above the third quartile. For low sensitivity,
          the threshold is two interquartile ranges above the third quartile.

          * LOW-Outliers will be detected using low sensitivity. This option
          will detect the fewest outliers.

          * MEDIUM-Outliers will be detected using moderate sensitivity. This is
          the default.

          * HIGH-Outliers will be detected using high sensitivity. This option
          will detect the most outliers.
      keep_type {Boolean}:
          Specifies whether the output features will contain all input features
          or only features identified as spatial outliers.

          * KEEP_OUTLIER-The output features will only contain features
          identified as spatial outliers.

          * KEEP_ALL-The output features will contain all input features. This
          is the default.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class containing the local outlier factor for each
          input feature as well as an indicator of whether the point is a
          spatial outlier.
      output_raster {Raster Dataset}:
          The output raster containing the local outlier factors at each cell,
          which is calculated based on the spatial distribution of the input
          features.This parameter is only available with a Desktop Advanced
          license."""
    ...

@gptooldoc('SpatiallyConstrainedMultivariateClustering_stats', None)
def SpatiallyConstrainedMultivariateClustering(in_features=..., output_features=..., analysis_fields=..., size_constraints=..., constraint_field=..., min_constraint=..., max_constraint=..., number_of_clusters=..., spatial_constraints=..., weights_matrix_file=..., number_of_permutations=..., output_table=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SpatiallyConstrainedMultivariateClustering_stats(in_features, output_features, analysis_fields;analysis_fields..., {size_constraints}, {constraint_field}, {min_constraint}, {max_constraint}, {number_of_clusters}, {spatial_constraints}, {weights_matrix_file}, {number_of_permutations}, {output_table})

        Finds spatially contiguous clusters of features based on a set of
        feature attribute values and optional cluster size limits.

     INPUTS:
      in_features (Feature Layer):
          The feature class or feature layer for which you want to create
          clusters.
      analysis_fields (Field):
          A list of fields that will be used to distinguish one cluster from
          another.
      size_constraints {String}:
          Specifies cluster size based on number of features per group or a
          target attribute value per group.

          * NONE-No cluster size constraints will be used. This is the default.

          * NUM_FEATURES-A minimum and maximum number of features per group will
          be used.

          * ATTRIBUTE_VALUE-A minimum and maximum attribute value per group will
          be used.
      constraint_field {Field}:
          The attribute value to be summed per cluster.
      min_constraint {Double}:
          The minimum number of features per cluster or the minimum attribute
          value per cluster. This must be a positive value.
      max_constraint {Double}:
          The maximum number of features per cluster or the maximum attribute
          value per cluster. If a maximum constraint is set, the
          number_of_clusters parameter is disabled. This must be a positive
          value.
      number_of_clusters {Long}:
          The number of clusters to create. If this parameter is empty, the tool
          will evaluate the optimal number of clusters by computing a pseudo
          F-statistic value for clustering solutions with 2 through 30
          clusters.This parameter will be disabled if a maximum number of
          features or
          maximum attribute value has been set.
      spatial_constraints {String}:
          Specifies how spatial relationships among features will be defined.

          * CONTIGUITY_EDGES_ONLY-Clusters will contain contiguous polygon
          features. Only polygons that share an edge can be part of the same
          cluster.

          * CONTIGUITY_EDGES_CORNERS-Clusters will contain contiguous polygon
          features. Only polygons that share an edge or a vertex can be part of
          the same cluster. This is the default for polygon features.

          * TRIMMED_DELAUNAY_TRIANGULATION-Features in the same cluster will
          have at least one natural neighbor in common with another feature in
          the cluster. Natural neighbor relationships are based on a trimmed
          Delaunay triangulation. Conceptually, Delaunay triangulation creates a
          nonoverlapping mesh of triangles from feature centroids. Each feature
          is a triangle node, and nodes that share edges are considered
          neighbors. These triangles are then clipped to a convex hull to ensure
          that features cannot be neighbors with any features outside the convex
          hull. This is the default for point features.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Spatial, and optionally temporal,
          relationships are defined by a specified spatial weights file (.swm).
          Create the spatial weights matrix using the Generate Spatial Weights
          Matrix or Generate Network Spatial Weights tool. The path to the
          spatial weights file is specified by the Weights_Matrix_File
          parameter.
      weights_matrix_file {File}:
          The path to a file containing spatial weights that define spatial, and
          potentially temporal, relationships among features.
      number_of_permutations {Long}:
          The number of random permutations for the calculation of membership
          stability scores. If 0 (zero) is chosen, probabilities will not be
          calculated. Calculating these probabilities uses permutations of
          random spanning trees and evidence accumulation.This calculation can
          take a significant time to run for larger
          datasets. It is recommended that you iterate and find the optimal
          number of clusters for your analysis first; then calculate
          probabilities for your analysis in a subsequent run. Setting the
          Parallel Processing Factor Environment setting to 50 may improve the
          run time of the tool.

     OUTPUTS:
      output_features (Feature Class):
          The new output feature class created containing all features, the
          analysis fields specified, and a field indicating to which cluster
          each feature belongs.
      output_table {Table}:
          The table created containing the results of the F-statistic values
          calculated to evaluate the optimal number of clusters. The chart
          created from this table can be accessed in the Contents pane under the
          output feature layer."""
    ...

@gptooldoc('CentralFeature_stats', None)
def CentralFeature(Input_Feature_Class=..., Output_Feature_Class=..., Distance_Method=..., Weight_Field=..., Self_Potential_Weight_Field=..., Case_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CentralFeature_stats(Input_Feature_Class, Output_Feature_Class, Distance_Method, {Weight_Field}, {Self_Potential_Weight_Field}, {Case_Field})

        Identifies the most centrally located feature in a point, line, or
        polygon feature class.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class containing a distribution of features from which to
          identify the most centrally located feature.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block); calculated by summing the
          (absolute) difference between the x- and y-coordinates
      Weight_Field {Field}:
          The numeric field used to weight distances in the origin-destination
          distance matrix.
      Self_Potential_Weight_Field {Field}:
          The field representing self-potential-the distance or weight between a
          feature and itself.
      Case_Field {Field}:
          Field used to group features for separate central feature
          computations. The case field can be of integer, date, or string type.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          The feature class that will contain the most centrally located feature
          in the Input Feature Class."""
    ...

@gptooldoc('DirectionalDistribution_stats', None)
def DirectionalDistribution(Input_Feature_Class=..., Output_Ellipse_Feature_Class=..., Ellipse_Size=..., Weight_Field=..., Case_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DirectionalDistribution_stats(Input_Feature_Class, Output_Ellipse_Feature_Class, Ellipse_Size, {Weight_Field}, {Case_Field})

        Creates standard deviational ellipses or ellipsoids to summarize the
        spatial characteristics of geographic features: central tendency,
        dispersion, and directional trends.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          A feature class containing a distribution of features for which the
          standard deviational ellipse or ellipsoid will be calculated.
      Ellipse_Size (String):
          The size of output ellipses in standard deviations. The default
          ellipse size is 1; valid choices are 1, 2, or 3 standard deviations.

          * 1_STANDARD_DEVIATION-1 standard deviation

          * 2_STANDARD_DEVIATIONS-2 standard deviations

          * 3_STANDARD_DEVIATIONS-3 standard deviations
      Weight_Field {Field}:
          The numeric field used to weight locations according to their relative
          importance.
      Case_Field {Field}:
          The field used to group features for separate directional distribution
          calculations. The case field can be of integer, date, or string type.

     OUTPUTS:
      Output_Ellipse_Feature_Class (Feature Class):
          A polygon feature class that will contain the output ellipse feature."""
    ...

@gptooldoc('DirectionalMean_stats', None)
def DirectionalMean(Input_Feature_Class=..., Output_Feature_Class=..., Orientation_Only=..., Case_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DirectionalMean_stats(Input_Feature_Class, Output_Feature_Class, Orientation_Only, {Case_Field})

        Identifies the mean direction, length, and geographic center for a set
        of lines.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class containing vectors for which the mean direction will
          be calculated.
      Orientation_Only (Boolean):
          Specifies whether to include direction (From and To nodes) information
          in the analysis.

          * DIRECTION-The From and To nodes are utilized in calculating the
          mean. This is the default.

          * ORIENTATION_ONLY-The From and To node information is ignored.
      Case_Field {Field}:
          Field used to group features for separate directional mean
          calculations. The case field can be of integer, date, or string type.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          A line feature class that will contain the features representing the
          mean directions of the input feature class."""
    ...

@gptooldoc('MeanCenter_stats', None)
def MeanCenter(Input_Feature_Class=..., Output_Feature_Class=..., Weight_Field=..., Case_Field=..., Dimension_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MeanCenter_stats(Input_Feature_Class, Output_Feature_Class, {Weight_Field}, {Case_Field}, {Dimension_Field})

        Identifies the geographic center (or the center of concentration) for
        a set of features.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          A feature class for which the mean center will be calculated.
      Weight_Field {Field}:
          The numeric field used to create a weighted mean center.
      Case_Field {Field}:
          Field used to group features for separate mean center calculations.
          The case field can be of integer, date, or string type.
      Dimension_Field {Field}:
          A numeric field containing attribute values from which an average
          value will be calculated.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          A point feature class that will contain the features representing the
          mean centers of the input feature class."""
    ...

@gptooldoc('MedianCenter_stats', None)
def MedianCenter(Input_Feature_Class=..., Output_Feature_Class=..., Weight_Field=..., Case_Field=..., Attribute_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MedianCenter_stats(Input_Feature_Class, Output_Feature_Class, {Weight_Field}, {Case_Field}, {Attribute_Field;Attribute_Field...})

        Identifies the location that minimizes overall Euclidean distance to
        the features in a dataset.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          A feature class for which the median center will be calculated.
      Weight_Field {Field}:
          The numeric field used to create a weighted median center.
      Case_Field {Field}:
          Field used to group features for separate median center calculations.
          The case field can be of integer, date, or string type.
      Attribute_Field {Field}:
          Numeric field(s) for which the data median value will be computed.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          A point feature class that will contain the features representing the
          median centers of the input feature class."""
    ...

@gptooldoc('NeighborhoodSummaryStatistics_stats', None)
def NeighborhoodSummaryStatistics(in_features=..., output_features=..., analysis_fields=..., local_summary_statistic=..., include_focal_feature=..., ignore_nulls=..., neighborhood_type=..., distance_band=..., number_of_neighbors=..., weights_matrix_file=..., local_weighting_scheme=..., kernel_bandwidth=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """NeighborhoodSummaryStatistics_stats(in_features, output_features, {analysis_fields;analysis_fields...}, {local_summary_statistic}, {include_focal_feature}, {ignore_nulls}, {neighborhood_type}, {distance_band}, {number_of_neighbors}, {weights_matrix_file}, {local_weighting_scheme}, {kernel_bandwidth})

        Calculates summary statistics of one or more numeric fields using
        local neighborhoods around each feature. The local statistics include
        mean (average), median, standard deviation, interquartile range,
        skewness, and quantile imbalance. All statistics can be geographically
        weighted using kernels to give more influence to neighbors closer to
        the focal feature. Various neighborhood types can be used, including
        distance band, number of neighbors, polygon contiguity, Delaunay
        triangulation, and spatial weights matrix files (.swm). Summary
        statistics are also calculated for the distances to the neighbors of
        each feature.

     INPUTS:
      in_features (Feature Layer):
          The point or polygon features that will be used to calculate the local
          statistics.
      analysis_fields {Field}:
          One or more fields for which local statistics will be calculated. If
          no analysis fields are provided, only local statistics based on
          distances to neighbors will be calculated.
      local_summary_statistic {String}:
          Specifies the local summary statistic that will be calculated for each
          analysis field.

          * ALL-All local statistics will be calculated. This is the default.

          * MEAN-The local mean (average) will be calculated.

          * MEDIAN-The local median will be calculated.

          * STD_DEV-The local standard deviation will be calculated.

          * IQR-The local interquartile range will be calculated.

          * SKEWNESS-The local skewness will be calculated.

          * QUANTILE_IMBALANCE-The local quantile imbalance will be calculated.
      include_focal_feature {Boolean}:
          Specifies whether the focal feature will be included when calculating
          local statistics for each feature.

          * INCLUDE_FOCAL-The focal feature and all of its neighbors will be
          included when calculating local statistics. This is the default.

          * EXCLUDE_FOCAL-The focal feature will not be included when
          calculating local statistics. Only neighbors of the feature will be
          included.
      ignore_nulls {Boolean}:
          Specifies whether null values in the analysis fields will be included
          or ignored in the calculations.

          * IGNORE_NULLS-Null values will be ignored in the local calculations.

          * INCLUDE_NULLS-Null values will be included in the local
          calculations.
      neighborhood_type {String}:
          Specifies how neighbors will be chosen for each input feature. To
          calculate local statistics, neighboring features must be identified
          for each input feature, and these neighbors are used to calculate the
          local statistics for each feature. For point features, the default is
          Delaunay triangulation. For polygon features, the default is
          Contiguity edges corners.The Delaunay triangulation option is only
          available with a Desktop
          Advanced license.

          * DISTANCE_BAND-Features within a specified critical distance of each
          feature will be included as neighbors.

          * NUMBER_OF_NEIGHBORS-The closest features will be included as
          neighbors.

          * CONTIGUITY_EDGES_ONLY-Polygon features that share an edge will be
          included as neighbors.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share an edge or a
          corner will be included as neighbors. This is the default for polygon
          features.

          * DELAUNAY_TRIANGULATION-Features whose Delaunay triangulation share
          an edge will be included as neighbors. This is the default for point
          features.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-Neighbors and weights will be defined
          by a specified spatial weights file.
      distance_band {Linear Unit}:
          All features within this distance will be included as neighbors. If no
          value is provided, one will be estimated during processing and
          included as a geoprocessing message. If the specified distance results
          in more than 1,000 neighbors, only the closest 1,000 features will be
          included as neighbors.
      number_of_neighbors {Long}:
          The number of neighbors that will be included for each local
          calculation. The number does not include the focal feature. If the
          focal feature is included in calculations, one additional neighbor
          will be used. The default is 8.
      weights_matrix_file {File}:
          The path and file name of the spatial weights matrix file that defines
          spatial, and potentially temporal, relationships among features.
      local_weighting_scheme {String}:
          Specifies the weighting scheme that will be applied to neighbors when
          calculating local statistics.

          * UNWEIGHTED-Neighbors will not be weighted. This is the default.

          * BISQUARE-Neighbors will be weighted using a bisquare kernel scheme.

          * GAUSSIAN-Neighbors will be weighted using a Gaussian kernel scheme.
      kernel_bandwidth {Linear Unit}:
          The bandwidth of the bisquare or Gaussian local weighting schemes. If
          no value is provided, one will be estimated during processing and
          included as a geoprocessing message.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class containing the local statistics as fields.
          Each statistic of each analysis field will be stored as an individual
          field."""
    ...

@gptooldoc('StandardDistance_stats', None)
def StandardDistance(Input_Feature_Class=..., Output_Standard_Distance_Feature_Class=..., Circle_Size=..., Weight_Field=..., Case_Field=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """StandardDistance_stats(Input_Feature_Class, Output_Standard_Distance_Feature_Class, Circle_Size, {Weight_Field}, {Case_Field})

        Measures the degree to which features are concentrated or dispersed
        around the geometric mean center.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          A feature class containing a distribution of features for which the
          standard distance will be calculated.
      Circle_Size (String):
          Specifies the size of output circles in standard deviations.

          * 1_STANDARD_DEVIATION-The output circles will be 1 standard
          deviation. This is the default.

          * 2_STANDARD_DEVIATIONS-The output circles will be 2 standard
          deviations.

          * 3_STANDARD_DEVIATIONS-The output circles will be 3 standard
          deviations.
      Weight_Field {Field}:
          The numeric field used to weight locations according to their relative
          importance.
      Case_Field {Field}:
          The field used to group features for separate standard distance
          calculations. The case field can be of integer, date, or string type.

     OUTPUTS:
      Output_Standard_Distance_Feature_Class (Feature Class):
          A polygon feature class that will contain a circle polygon for each
          input center. These circle polygons graphically portray the standard
          distance at each center point."""
    ...

@gptooldoc('ColocationAnalysis_stats', None)
def ColocationAnalysis(input_type=..., in_features_of_interest=..., output_features=..., field_of_interest=..., time_field_of_interest=..., category_of_interest=..., input_feature_for_comparison=..., field_for_comparison=..., time_field_for_comparison=..., category_for_comparison=..., neighborhood_type=..., number_of_neighbors=..., distance_band=..., weights_matrix_file=..., temporal_relationship_type=..., time_step_interval=..., number_of_permutations=..., local_weighting_scheme=..., output_table=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ColocationAnalysis_stats(input_type, in_features_of_interest, output_features, {field_of_interest}, {time_field_of_interest}, {category_of_interest}, {input_feature_for_comparison}, {field_for_comparison}, {time_field_for_comparison}, {category_for_comparison}, {neighborhood_type}, {number_of_neighbors}, {distance_band}, {weights_matrix_file}, {temporal_relationship_type}, {time_step_interval}, {number_of_permutations}, {local_weighting_scheme}, {output_table})

        Measures local patterns of spatial association, or colocation, between
        two categories of point features using the colocation quotient
        statistic.

     INPUTS:
      input_type (String):
          Specifies whether the in_features_of_interest parameter values will
          come from the same dataset with specified categories, different
          datasets with specified categories, or different datasets that will be
          treated as their own category (for example, one dataset with all
          points representing cheetahs and a second dataset in which all points
          represent gazelles).

          * SINGLE_DATASET-The categories to be analyzed exist in a field in a
          single dataset.

          * TWO_DATASETS-The categories to be analyzed exist in fields of
          separate datasets.

          * DATASETS_WITHOUT_CATEGORIES-Two separate datasets representing two
          categories will be analyzed.
      in_features_of_interest (Feature Layer):
          The feature class containing points with representative categories.
      field_of_interest {Field}:
          The field containing the category or categories to be analyzed.
      time_field_of_interest {Field}:
          A date field with an optional time stamp for each feature to analyze
          points using a space-time window. Features near each other in space
          and time will be considered neighbors and will be analyzed together.
      category_of_interest {String}:
          The base category for the analysis. The tool will identify, for each
          category_of_interest value, the degree to which the base category is
          attracted to or colocated with the neighboring_category parameter
          value.
      input_feature_for_comparison {Feature Layer}:
          The input feature class containing the points with the categories that
          will be compared.
      field_for_comparison {Field}:
          The field from the input_feature_for_comparison parameter containing
          the category to be compared.
      time_field_for_comparison {Field}:
          A date field with a time stamp for each feature to analyze your points
          using a space-time window. Features near each other in space and time
          will be considered neighbors and will be analyzed together.
      category_for_comparison {String}:
          The neighboring category for the analysis. The tool will identify the
          degree to which the category_of_interest parameter value is attracted
          to or isolated from the category_for_comparison value.
      neighborhood_type {String}:
          Specifies how the spatial relationships among features will be
          defined.

          * DISTANCE_BAND-Each feature will be analyzed within the context of
          neighboring features. Neighboring features inside the specified
          critical distance specified by the distance_band parameter receive a
          weight of one and exert influence on computations for the target
          feature. Neighboring features outside the critical distance receive a
          weight of zero and have no influence on a target feature's
          computations.

          * K_NEAREST_NEIGHBORS-The closest k features will be included in the
          analysis as neighbors. The number of neighbors is specified by the
          number_of_neighbors parameter. The neighbor's influence in the
          analysis is weighted based on the distance to the farthest neighbor.
          This is the default.

          * GET_SPATIAL_WEIGHTS_FROM_FILE-When SINGLE_DATASET is used as the
          input_type, spatial relationships will be defined by a specified
          spatial weights matrix file. The neighbor's influence in the analysis
          is weighted based on the distance to the farthest neighbor. The path
          to the spatial weights file is specified by the weights_matrix_file
          parameter.
      number_of_neighbors {Long}:
          The number of neighbors around each feature that will be used to test
          for local relationships between categories. If no value is provided,
          the default of 8 is used. The provided value must be large enough to
          detect the relationships between features but small enough to still
          identify local patterns.
      distance_band {Linear Unit}:
          The neighborhood size is a constant or fixed distance for each
          feature. All features within this distance will be used to test for
          local relationships between categories. If no value is provided, the
          distance used will be the average distance at which each feature has
          at least eight neighbors.
      weights_matrix_file {File}:
          The path to a file containing weights that define spatial, and
          potentially temporal, relationships among features.
      temporal_relationship_type {String}:
          Specifies how temporal relationships among features will be defined.

          * BEFORE-The time window will extend back in time for each of the
          in_features_of_interest values. Neighboring features must have a
          date/time stamp that occurs before the date/time stamp of the feature
          of interest to be included in the analysis. This is the default.

          * AFTER-The time window will extend forward in time for each of the
          in_features_of_interest values. Neighboring features must have a
          date/time stamp that occurs after the date/time stamp of the feature
          of interest to be included in the analysis.

          * SPAN-The time window will extend both back and forward in time for
          each of the in_features_of_interest values. Neighboring features that
          have a date/time stamp that occurs within the time_step_interval value
          before or after the date/time stamp of the feature of interest will be
          included in the analysis. For example, if the time_step_interval
          parameter is set to 1 week, the window will look 1 week before and 1
          week after the target feature.
      time_step_interval {Time Unit}:
          An integer and unit of measurement representing the number of time
          units composing the time window.
      number_of_permutations {Long}:
          The number of permutations that will be used to create a reference
          distribution. Choosing the number of permutations is a balance between
          precision and increased processing time. Choose your preference of
          speed versus precision. More robust and precise results take longer to
          calculate.

          * 99-The analysis will use 99 permutations. With 99 permutations, the
          smallest possible pseudo p-value is 0.02 and all other pseudo p-values
          will be multiples of this value. This is the default.

          * 199-The analysis will use 199 permutations. With 199 permutations,
          the smallest possible pseudo p-value is 0.01 and all other pseudo
          p-values will be even multiples of this value.

          * 499-The analysis will use 499 permutations. With 499 permutations,
          the smallest possible pseudo p-value is 0.004 and all other pseudo
          p-values will be even multiples of this value.

          * 999-The analysis will use 999 permutations. With 999 permutations,
          the smallest possible pseudo p-value is 0.002 and all other pseudo
          p-values will be even multiples of this value.

          * 9999-The analysis will use 9,999 permutations. With 9,999
          permutations, the smallest possible pseudo p-value is 0.0002 and all
          other pseudo p-values will be even multiples of this value.
      local_weighting_scheme {String}:
          Specifies the kernel type that will be used to provide the spatial
          weighting. The kernel defines how each feature is related to other
          features within its neighborhood.

          * BISQUARE-Features will be weighted based on the distance to the
          farthest neighbor or the edge of the distance band, and a weight of 0
          will be assigned to any feature outside the neighborhood specified.

          * GAUSSIAN-Features will be weighted based on the distance to the
          farthest neighbor or the edge of the distance band but drop off more
          quickly than the Bisquare option. A weight of 0 will be assigned to
          any feature outside the neighborhood specified. This is the default.

          * NONE-No weighting scheme will be applied, and all features within
          the neighborhood will be given a weight of 1 and contribute equally.
          All features outside the neighborhood will be given a weight of 0.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class containing all the in_features parameter
          values with fields representing the local colocation quotient scores
          and p-values.
      output_table {Table}:
          A table that includes the global colocation quotients between all the
          categories in the Field of Interest parameter and all the categories
          in the Field Containing Neighboring Category parameter. This table can
          help you determine the local categories to analyze.If Datasets without
          categories is used as the Input Type parameter
          value, global colocation quotients will be calculated for each dataset
          and between each dataset."""
    ...

@gptooldoc('ExploratoryRegression_stats', None)
def ExploratoryRegression(Input_Features=..., Dependent_Variable=..., Candidate_Explanatory_Variables=..., Weights_Matrix_File=..., Output_Report_File=..., Output_Results_Table=..., Maximum_Number_of_Explanatory_Variables=..., Minimum_Number_of_Explanatory_Variables=..., Minimum_Acceptable_Adj_R_Squared=..., Maximum_Coefficient_p_value_Cutoff=..., Maximum_VIF_Value_Cutoff=..., Minimum_Acceptable_Jarque_Bera_p_value=..., Minimum_Acceptable_Spatial_Autocorrelation_p_value=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExploratoryRegression_stats(Input_Features, Dependent_Variable, Candidate_Explanatory_Variables;Candidate_Explanatory_Variables..., {Weights_Matrix_File}, {Output_Report_File}, {Output_Results_Table}, {Maximum_Number_of_Explanatory_Variables}, {Minimum_Number_of_Explanatory_Variables}, {Minimum_Acceptable_Adj_R_Squared}, {Maximum_Coefficient_p_value_Cutoff}, {Maximum_VIF_Value_Cutoff}, {Minimum_Acceptable_Jarque_Bera_p_value}, {Minimum_Acceptable_Spatial_Autocorrelation_p_value})

        Evaluates all possible combinations of the input candidate explanatory
        variables, looking for OLS models that best explain the dependent
        variable within the context of user-specified criteria.

     INPUTS:
      Input_Features (Feature Layer):
          The feature class or feature layer containing the dependent and
          candidate explanatory variables to analyze.
      Dependent_Variable (Field):
          The numeric field containing the observed values you want to model
          using OLS.
      Candidate_Explanatory_Variables (Field):
          A list of fields to try as OLS model explanatory variables.
      Weights_Matrix_File {File}:
          A file containing spatial weights that define the spatial
          relationships among your input features. This file is used to assess
          spatial autocorrelation among regression residuals. You can use the
          Generate Spatial Weights Matrix File tool to create this. When you do
          not provide a spatial weights matrix file, residuals are assessed for
          spatial autocorrelation based on each feature's 8 nearest
          neighbors.Note: The spatial weights matrix file is only used to
          analyze spatial
          structure in model residuals; it is not used to build or to calibrate
          any of the OLS models.
      Maximum_Number_of_Explanatory_Variables {Long}:
          All models with explanatory variables up to the value entered here
          will be assessed. If, for example, the
          Minimum_Number_of_Explanatory_Variables is 2 and the
          Maximum_Number_of_Explanatory_Variables is 3, the Exploratory
          Regression tool will try all models with every combination of two
          explanatory variables, and all models with every combination of three
          explanatory variables.
      Minimum_Number_of_Explanatory_Variables {Long}:
          This value represents the minimum number of explanatory variables for
          models evaluated. If, for example, the
          Minimum_Number_of_Explanatory_Variables is 2 and the
          Maximum_Number_of_Explanatory_Variables is 3, the Exploratory
          Regression tool will try all models with every combination of two
          explanatory variables, and all models with every combination of three
          explanatory variables.
      Minimum_Acceptable_Adj_R_Squared {Double}:
          This is the lowest Adjusted R-Squared value you consider a passing
          model. If a model passes all of your other search criteria, but has an
          Adjusted R-Squared value smaller than the value entered here, it will
          not show up as a Passing Model in the Output_Report_File. Valid values
          for this parameter range from 0.0 to 1.0. The default value is 0.5,
          indicating that passing models will explain at least fifty percent of
          the variation in the dependent variable.
      Maximum_Coefficient_p_value_Cutoff {Double}:
          For each model evaluated, OLS computes explanatory variable
          coefficient p-values. The cutoff p-value you enter here represents the
          confidence level you require for all coefficients in the model in
          order to consider the model passing. Small p-values reflect a stronger
          confidence level. Valid values for this parameter range from 1.0 down
          to 0.0, but will most likely be 0.1, 0.05, 0.01, 0.001, and so on. The
          default value is 0.05, indicating passing models will only contain
          explanatory variables whose coefficients are statistically at the 95
          percent confidence level (p-values smaller than 0.05). To relax this
          default you would enter a larger p-value cutoff, such as 0.1. If you
          are getting lots of passing models, you will likely want to make this
          search criteria more stringent by decreasing the default p-value
          cutoff from 0.05 to 0.01 or smaller.
      Maximum_VIF_Value_Cutoff {Double}:
          This value reflects how much redundancy (multicollinearity) among
          model explanatory variables you will tolerate. When the VIF (Variance
          Inflation Factor) value is higher than about 7.5, multicollinearity
          can make a model unstable; consequently, 7.5 is the default value
          here. If you want your passing models to have less redundancy, you
          would enter a smaller value, such as 5.0, for this parameter.
      Minimum_Acceptable_Jarque_Bera_p_value {Double}:
          The p-value returned by the Jarque-Bera diagnostic test indicates
          whether the model residuals are normally distributed. If the p-value
          is statistically significant (small), the model residuals are not
          normal and the model is biased. Passing models should have large
          Jarque-Bera p-values. The default minimum acceptable p-value is 0.1.
          Only models returning p-values larger than this minimum will be
          considered passing. If you are having trouble finding unbiased passing
          models, and decide to relax this criterion, you might enter a smaller
          minimum p-value such as 0.05.
      Minimum_Acceptable_Spatial_Autocorrelation_p_value {Double}:
          For models that pass all of the other search criteria, the Exploratory
          Regression tool will check model residuals for spatial clustering
          using Global Moran's I. When the p-value for this diagnostic test is
          statistically significant (small), it indicates the model is very
          likely missing key explanatory variables (it isn't telling the whole
          story). Unfortunately, if you have spatial autocorrelation in your
          regression residuals, your model is misspecified, so you cannot trust
          your results. Passing models should have large p-values for this
          diagnostic test. The default minimum p-value is 0.1. Only models
          returning p-values larger than this minimum will be considered
          passing. If you are having trouble finding properly specified models
          because of this diagnostic test, and decide to relax this search
          criteria, you might enter a smaller minimum such as 0.05.

     OUTPUTS:
      Output_Report_File {File}:
          The report file contains tool results, including details about any
          models found that passed all the search criteria you entered. This
          output file also contains diagnostics to help you fix common
          regression problems in the case that you don't find any passing
          models.
      Output_Results_Table {Table}:
          The optional output table created containing the explanatory variables
          and diagnostics for all of the models within the Coefficient p-value
          and VIF value cutoffs."""
    ...

@gptooldoc('Forest_stats', None)
def Forest(prediction_type=..., in_features=..., variable_predict=..., treat_variable_as_categorical=..., explanatory_variables=..., distance_features=..., explanatory_rasters=..., features_to_predict=..., output_features=..., output_raster=..., explanatory_variable_matching=..., explanatory_distance_matching=..., explanatory_rasters_matching=..., output_trained_features=..., output_importance_table=..., use_raster_values=..., number_of_trees=..., minimum_leaf_size=..., maximum_depth=..., sample_size=..., random_variables=..., percentage_for_training=..., output_classification_table=..., output_validation_table=..., compensate_sparse_categories=..., number_validation_runs=..., calculate_uncertainty=..., output_trained_model=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """Forest_stats(prediction_type, in_features, {variable_predict}, {treat_variable_as_categorical}, {explanatory_variables;explanatory_variables...}, {distance_features;distance_features...}, {explanatory_rasters;explanatory_rasters...}, {features_to_predict}, {output_features}, {output_raster}, {explanatory_variable_matching;explanatory_variable_matching...}, {explanatory_distance_matching;explanatory_distance_matching...}, {explanatory_rasters_matching;explanatory_rasters_matching...}, {output_trained_features}, {output_importance_table}, {use_raster_values}, {number_of_trees}, {minimum_leaf_size}, {maximum_depth}, {sample_size}, {random_variables}, {percentage_for_training}, {output_classification_table}, {output_validation_table}, {compensate_sparse_categories}, {number_validation_runs}, {calculate_uncertainty}, {output_trained_model})

        Creates models and generates predictions using an adaptation of the
        random forest algorithm, which is a supervised machine learning method
        developed by Leo Breiman and Adele Cutler. Predictions can be
        performed for both categorical variables (classification) and
        continuous variables (regression). Explanatory variables can take the
        form of fields in the attribute table of the training features, raster
        datasets, and distance features used to calculate proximity values for
        use as additional variables. In addition to validation of model
        performance based on the training data, predictions can be made to
        either features or a prediction raster.

     INPUTS:
      prediction_type (String):
          Specifies the operation mode of the tool. The tool can be run to train
          a model to only assess performance, predict features, or create a
          prediction surface.

          * TRAIN-A model will be trained, but no predictions will be generated.
          Use this option to assess the accuracy of the model before generating
          predictions. This option will output model diagnostics in the messages
          window and a chart of variable importance. This is the default

          * PREDICT_FEATURES-Predictions or classifications will be generated
          for features. Explanatory variables must be provided for both the
          training features and the features to be predicted. The output of this
          option will be a feature class, model diagnostics in the messages
          window, and an optional table and chart of variable importance.

          * PREDICT_RASTER-A prediction raster will be generated for the area
          where the explanatory rasters intersect. Explanatory rasters must be
          provided for both the training area and the area to be predicted. The
          output of this option will be a prediction surface, model diagnostics
          in the messages window, and an optional table and chart of variable
          importance.
      in_features (Feature Layer):
          The feature class containing the variable_predict parameter value and,
          optionally, the explanatory training variables from fields.
      variable_predict {Field}:
          The variable from the in_features parameter containing the values to
          be used to train the model. This field contains known (training)
          values of the variable that will be used to predict at unknown
          locations.
      treat_variable_as_categorical {Boolean}:
          * CATEGORICAL-The variable_predict value is a categorical variable and
          the tool will perform classification.

          * NUMERIC-The variable_predict value is continuous and the tool will
          perform regression. This is the default
      explanatory_variables {Value Table}:
          A list of fields representing the explanatory variables that help
          predict the value or category of the variable_predict value. Use the
          treat_variable_as_categorical parameter for any variables that
          represent classes or categories (such as land cover or presence or
          absence). Specify the variable as CATEGORICAL if it represents classes
          or categories such as land cover or presence or absence and NUMERIC if
          it is continuous.
      distance_features {Feature Layer}:
          The explanatory training distance features. Explanatory variables will
          be automatically created by calculating a distance from the provided
          features to the in_features values. Distances will be calculated from
          each of the input distance_features values to the nearest in_features
          value. If the input distance_features values are polygons or lines,
          the distance attributes will be calculated as the distance between the
          closest segments of the pair of features.
      explanatory_rasters {Value Table}:
          The explanatory training variables extracted from rasters. Explanatory
          training variables will be automatically created by extracting raster
          cell values. For each feature in the in_features parameter, the value
          of the raster cell is extracted at that exact location. Bilinear
          raster resampling is used when extracting the raster value unless it
          is specified as categorical, in which case nearest neighbor assignment
          is used. Specify the raster as CATEGORICAL if it represents classes or
          categories such as land cover or presence or absence and NUMERIC if it
          is continuous.
      features_to_predict {Feature Layer}:
          A feature class representing locations where predictions will be made.
          This feature class must also contain any explanatory variables
          provided as fields that correspond to those used from the training
          data.
      explanatory_variable_matching {Value Table}:
          A list of the explanatory_variables values specified from the
          in_features parameter on the right and corresponding fields from the
          features_to_predict parameter on the left, for example,
          [["LandCover2000", "LandCover2010"], ["Income", "PerCapitaIncome"]].
      explanatory_distance_matching {Value Table}:
          A list of the distance_features values specified for the in_features
          parameter on the right and corresponding feature sets from the
          features_to_predict parameter on the left.The
          explanatory_distance_features values that are more appropriate for
          the features_to_predict parameter can be provided if those used for
          training are in a different study area or time period.
      explanatory_rasters_matching {Value Table}:
          A list of the explanatory_rasters values specified for the in_features
          on the right and corresponding rasters from the features_to_predict
          parameter or output_raster parameter to be created on the left.The
          explanatory_rasters values that are more appropriate for the
          features_to_predict parameter can be provided if those used for
          training are in a different study area or time period.
      use_raster_values {Boolean}:
          Specifies how polygons will be treated when training the model if the
          in_features values are polygons with a categorical variable_predict
          value and only explanatory_rasters values have been specified.

          * TRUE-The polygon will be divided into all of the raster cells with
          centroids falling within the polygon. The raster values at each
          centroid will be extracted and used to train the model. The model will
          no longer be trained on the polygon; it will be trained on the raster
          values extracted for each cell centroid. This is the default.

          * FALSE-Each polygon will be assigned the average value of the
          underlying continuous rasters and the majority for underlying
          categorical rasters.
      number_of_trees {Long}:
          The number of trees that will be created in the forest model. More
          trees generally result in more accurate model prediction, but the
          model will take longer to calculate. The default number of trees is
          100.
      minimum_leaf_size {Long}:
          The minimum number of observations required to keep a leaf (that is,
          the terminal node on a tree without further splits). The default
          minimum for regression is 5 and the default for classification is 1.
          For very large data, increasing these numbers will decrease the run
          time of the tool.
      maximum_depth {Long}:
          The maximum number of splits that will be made down a tree. Using a
          large maximum depth, more splits will be created, which may increase
          the chances of overfitting the model. The default is data driven and
          depends on the number of trees created and the number of variables
          included.
      sample_size {Long}:
          The percentage of the in_features values that will be used for each
          decision tree. The default is 100 percent of the data. Samples for
          each tree are taken randomly from two-thirds of the data
          specified.Each decision tree in the forest is created using a random
          sample or
          subset (approximately two-thirds) of the training data available.
          Using a lower percentage of the input data for each decision tree
          increases the speed of the tool for very large datasets.
      random_variables {Long}:
          The number of explanatory variables that will be used to create each
          decision tree.Each of the decision trees in the forest is created
          using a random
          subset of the explanatory variables specified. Increasing the number
          of variables used in each decision tree will increase the chances of
          overfitting the model particularly if there is one or more dominant
          variables. A common practice is to use the square root of the total
          number of explanatory variables (fields, distances, and rasters
          combined) if the variable_predict value is categorical or divide the
          total number of explanatory variables (fields, distances, and rasters
          combined) by 3 if the variable_predict value is numeric.
      percentage_for_training {Double}:
          The percentage (between 10 percent and 50 percent) of the in_features
          values that will be reserved as the test dataset for validation. The
          model will be trained without this random subset of data, and the
          observed values for those features will be compared to the predicted
          value. The default is 10 percent.
      compensate_sparse_categories {Boolean}:
          Specifies whether each category in the training dataset, regardless of
          its frequency, will be represented in each tree.

          * TRUE-Each tree will include every category that is represented in
          the training dataset.

          * FALSE-Each tree will be created based on a random sample of the
          categories in the training dataset. This is the default.
      number_validation_runs {Long}:
          The number of iterations of the tool. The distribution of the R2 for
          each run can be displayed using the Output Validation Table parameter.
          When this is set and predictions are being generated, only the model
          that produced the highest R2 value will be used for predictions.
      calculate_uncertainty {Boolean}:
          Specifies whether prediction uncertainty will be calculated when
          training, predicting to features, or predicting to raster.

          * TRUE-A prediction uncertainty interval will be calculated.

          * FALSE-Uncertainty will not be calculated. This is the default.

     OUTPUTS:
      output_features {Feature Class}:
          The output feature class containing the prediction results.
      output_raster {Raster Dataset}:
          The output raster containing the prediction results. The default cell
          size will be the maximum cell size of the raster inputs. To set a
          different cell size, use the Cell Size environment setting.
      output_trained_features {Feature Class}:
          The explanatory variables used for training (including sampled raster
          values and distance calculations), as well as the observed
          variable_to_predict field and accompanying predictions that will be
          used to further assess performance of the trained model.
      output_importance_table {Table}:
          The table that will contain information describing the importance of
          each explanatory variable (fields, distance features, and rasters)
          used in the model created.
      output_classification_table {Table}:
          A confusion matrix for classification summarizing the performance of
          the model created. This table can be used to calculate other
          diagnostics in addition to the accuracy and sensitivity measures the
          tool calculates in the output messages.
      output_validation_table {Table}:
          If the Number of Runs for Validation value is greater than 2, this
          table creates a chart of the distribution of R2 for each model. This
          distribution can be used to assess the stability of the model.
      output_trained_model {File}:
          An output model file that will save the trained model, which can be
          used later for prediction."""
    ...

@gptooldoc('GWR_stats', None)
def GWR(in_features=..., dependent_variable=..., model_type=..., explanatory_variables=..., output_features=..., neighborhood_type=..., neighborhood_selection_method=..., minimum_number_of_neighbors=..., maximum_number_of_neighbors=..., minimum_search_distance=..., maximum_search_distance=..., number_of_neighbors_increment=..., search_distance_increment=..., number_of_increments=..., number_of_neighbors=..., distance_band=..., prediction_locations=..., explanatory_variables_to_match=..., output_predicted_features=..., robust_prediction=..., local_weighting_scheme=..., coefficient_raster_workspace=..., scale=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GWR_stats(in_features, dependent_variable, model_type, explanatory_variables;explanatory_variables..., output_features, neighborhood_type, neighborhood_selection_method, {minimum_number_of_neighbors}, {maximum_number_of_neighbors}, {minimum_search_distance}, {maximum_search_distance}, {number_of_neighbors_increment}, {search_distance_increment}, {number_of_increments}, {number_of_neighbors}, {distance_band}, {prediction_locations}, {explanatory_variables_to_match;explanatory_variables_to_match...}, {output_predicted_features}, {robust_prediction}, {local_weighting_scheme}, {coefficient_raster_workspace}, {scale})

        Performs Geographically Weighted Regression, which is a local form of
        linear regression that is used to model spatially varying
        relationships.

     INPUTS:
      in_features (Feature Layer):
          The feature class containing the dependent and explanatory variables.
      dependent_variable (Field):
          The numeric field containing the observed values that will be modeled.
      model_type (String):
          Specifies the type of data that will be modeled.

          * CONTINUOUS-The dependent_variable value is continuous. The Gaussian
          model will be used, and the tool will perform ordinary least squares
          regression.

          * BINARY-The dependent_variable value represents presence or absence.
          This can be either conventional 1s and 0s or continuous data that has
          been coded based on a threshold value. The Logistic regression model
          will be used.

          * COUNT-The dependent_variable value is discrete and represents
          events, such as crime counts, disease incidents, or traffic accidents.
          The Poisson regression model will be used.
      explanatory_variables (Field):
          A list of fields representing independent explanatory variables in the
          regression model.
      neighborhood_type (String):
          Specifies whether the neighborhood used is constructed as a fixed
          distance or allowed to vary in spatial extent depending on the density
          of the features.

          * NUMBER_OF_NEIGHBORS-The neighborhood size is a function of a
          specified number of neighbors included in calculations for each
          feature. Where features are dense, the spatial extent of the
          neighborhood is smaller; where features are sparse, the spatial extent
          of the neighborhood is larger.

          * DISTANCE_BAND-The neighborhood size is a constant or fixed distance
          for each feature.
      neighborhood_selection_method (String):
          Specifies how the neighborhood size will be determined. The
          neighborhood selected with the GOLDEN_SEARCH and MANUAL_INTERVALS
          options is based on minimizing the AICc value.

          * GOLDEN_SEARCH-The tool will identify an optimal distance or number
          of neighbors based on the characteristics of the data using the golden
          section search method.

          * MANUAL_INTERVALS-The neighborhoods tested will be defined by the
          values specified in the minimum_number_of_neighbors and
          number_of_neighbors_increment parameters when NUMBER_OF_NEIGHBORS is
          chosen for the neighborhood_type parameter, or the
          minimum_search_distance and search_distance_increment parameters when
          DISTANCE_BAND is chosen for the neighborhood_type parameter, as well
          as the number_of_increments parameter.

          * USER_DEFINED-The neighborhood size will be specified by either the
          number_of_neighbors or distance_band parameter.
      minimum_number_of_neighbors {Long}:
          The minimum number of neighbors each feature will include in its
          calculations. It is recommended that you use at least 30 neighbors.
      maximum_number_of_neighbors {Long}:
          The maximum number of neighbors (up to 1000) each feature will include
          in its calculations.
      minimum_search_distance {Linear Unit}:
          The minimum neighborhood search distance. It is recommended that you
          use a distance at which each feature has at least 30 neighbors.
      maximum_search_distance {Linear Unit}:
          The maximum neighborhood search distance. If a distance results in
          features with more than 1000 neighbors, the tool will use the first
          1000 in calculations for the target feature.
      number_of_neighbors_increment {Long}:
          The number of neighbors by which manual intervals will increase for
          each neighborhood test.
      search_distance_increment {Linear Unit}:
          The distance by which manual intervals will increase for each
          neighborhood test.
      number_of_increments {Long}:
          The number of neighborhood sizes to test starting with the
          minimum_number_of_neighbors or minimum_search_distance parameter
          value.
      number_of_neighbors {Long}:
          The closest number of neighbors (up to 1000) to consider for each
          feature. The number must be an integer between 2 and 1000.
      distance_band {Linear Unit}:
          The spatial extent of the neighborhood.
      prediction_locations {Feature Layer}:
          A feature class containing features representing locations where
          estimates will be computed. Each feature in this dataset should
          contain values for all the explanatory variables specified. The
          dependent variable for these features will be estimated using the
          model calibrated for the input feature class data. To be predicted,
          these feature locations should be within the same study area as the
          in_features value or be close (within the extent plus 15 percent).A
          feature class containing features representing locations where
          estimates will be computed. Each feature in this dataset should
          contain values for all the explanatory variables specified. The
          dependent variable for these features will be estimated using the
          model calibrated for the input feature class data. To be predicted,
          these feature locations should be within the same study area as the
          Input Features values or be close (within the extent plus 15 percent).
      explanatory_variables_to_match {Value Table}:
          The explanatory variables from the prediction_locations parameter that
          match corresponding explanatory variables from the in_features
          parameter. [["LandCover2000", "LandCover2010"], ["Income",
          "PerCapitaIncome"]] are examples.
      robust_prediction {Boolean}:
          Specifies the features that will be used in prediction calculations.

          * ROBUST-Features with values more than three standard deviations from
          the mean (value outliers) and features with weights of 0 (spatial
          outliers) will be excluded from prediction calculations but will
          receive predictions in the output feature class. This is the default.

          * NON_ROBUST-All features will be used in prediction calculations
      local_weighting_scheme {String}:
          Specifies the kernel type that will be used to provide the spatial
          weighting in the model. The kernel defines how each feature is related
          to other features within its neighborhood.

          * BISQUARE-A weight of 0 will be assigned to any feature outside the
          neighborhood specified. This is the default.

          * GAUSSIAN-All features will receive weights, but weights become
          exponentially smaller the farther away they are from the target
          feature.
      coefficient_raster_workspace {Workspace}:
          The workspace where the coefficient rasters will be created. When this
          workspace is provided, rasters are created for the intercept and every
          explanatory variable. This parameter is only available with a Desktop
          Advanced license.
      scale {Boolean}:
          Specifies whether the values of the explanatory and dependent
          variables will be scaled to have mean zero and standard deviation one
          prior to fitting the model.

          * SCALE_DATA-The values of the variables will be scaled. The results
          will contain scaled and unscaled versions of the explanatory variable
          coefficients.

          * NO_SCALE_DATA-The values of the variables will not be scaled. All
          coefficients will be unscaled and in original data units.

     OUTPUTS:
      output_features (Feature Class):
          The new feature class containing the dependent variable estimates and
          residuals.
      output_predicted_features {Feature Class}:
          The output feature class that will receive dependent variable
          estimates for each prediction_location value."""
    ...

@gptooldoc('GeneralizedLinearRegression_stats', None)
def GeneralizedLinearRegression(in_features=..., dependent_variable=..., model_type=..., output_features=..., explanatory_variables=..., distance_features=..., prediction_locations=..., explanatory_variables_to_match=..., explanatory_distance_matching=..., output_predicted_features=..., output_trained_model=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeneralizedLinearRegression_stats(in_features, dependent_variable, model_type, output_features, {explanatory_variables;explanatory_variables...}, {distance_features;distance_features...}, {prediction_locations}, {explanatory_variables_to_match;explanatory_variables_to_match...}, {explanatory_distance_matching;explanatory_distance_matching...}, {output_predicted_features}, {output_trained_model})

        Performs generalized linear regression (GLR) to generate predictions
        or to model a dependent variable in terms of its relationship to a set
        of explanatory variables. This tool can be used to fit continuous
        (OLS), binary (logistic), and count (Poisson) models.

     INPUTS:
      in_features (Feature Layer):
          The feature class containing the dependent and independent variables.
      dependent_variable (Field):
          The numeric field containing the observed values to be modeled.
      model_type (String):
          Specifies the type of data that will be modeled.

          * CONTINUOUS-The dependent_variable value is continuous. The model
          used is Gaussian, and the tool performs ordinary least squares
          regression.

          * BINARY-The dependent_variable value represents presence or absence.
          This can be either conventional 1s and 0s, or continuous data that has
          been recoded based on a threshold value. The model used is Logistic
          Regression.

          * COUNT-The dependent_variable value is discrete and represents
          events-for example, crime counts, disease incidents, or traffic
          accidents. The model used is Poisson regression.
      explanatory_variables {Field}:
          A list of fields representing independent explanatory variables in the
          regression model.
      distance_features {Feature Layer}:
          Automatically creates explanatory variables by calculating a distance
          from the provided features to the in_features values. Distances will
          be calculated from each of the input distance_features values to the
          nearest in_features value. If the input distance_features values are
          polygons or lines, the distance attributes will be calculated as the
          distance between the closest segments of the pair of features.
      prediction_locations {Feature Layer}:
          A feature class containing features representing locations where
          estimates will be computed. Each feature in this dataset should
          contain values for all the explanatory variables specified. The
          dependent variable for these features will be estimated using the
          model calibrated for the input feature class data.
      explanatory_variables_to_match {Value Table}:
          Matches the explanatory variables in the prediction_locations
          parameter to corresponding explanatory variables from the in_features
          parameter.
      explanatory_distance_matching {Value Table}:
          Matches the distance features specified for the features_to_predict
          parameter on the left to the corresponding distance features for the
          in_features parameter on the right.

     OUTPUTS:
      output_features (Feature Class):
          The new feature class that will contain the dependent variable
          estimates and residuals.
      output_predicted_features {Feature Class}:
          The output feature class that will receive dependent variable
          estimates for each prediction_location value.The output feature class
          that will receive dependent variable
          estimates for each Prediction Location value.
      output_trained_model {File}:
          An output model file that will save the trained model, which can be
          used later for prediction."""
    ...

@gptooldoc('GenerateNetworkSWM_stats', None)
def GenerateNetworkSWM(Input_Feature_Class=..., Unique_ID_Field=..., Output_Spatial_Weights_Matrix_File=..., Input_Network_Data_Source=..., Travel_Mode=..., Impedance_Distance_Cutoff=..., Impedance_Temporal_Cutoff=..., Impedance_Cost_Cutoff=..., Maximum_Number_of_Neighbors=..., Time_of_Day=..., Time_Zone=..., Barriers=..., Search_Tolerance=..., Conceptualization_of_Spatial_Relationships=..., Exponent=..., Row_Standardization=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateNetworkSWM_stats(Input_Feature_Class, Unique_ID_Field, Output_Spatial_Weights_Matrix_File, Input_Network_Data_Source, Travel_Mode, {Impedance_Distance_Cutoff}, {Impedance_Temporal_Cutoff}, {Impedance_Cost_Cutoff}, {Maximum_Number_of_Neighbors}, {Time_of_Day}, {Time_Zone}, {Barriers}, {Search_Tolerance}, {Conceptualization_of_Spatial_Relationships}, {Exponent}, {Row_Standardization})

        Constructs a spatial weights matrix file (.swm) using a network
        dataset, defining spatial relationships in terms of the underlying
        network structure.

     INPUTS:
      Input_Feature_Class (Feature Class):
          The point feature class representing locations on the network. For
          each feature, neighbors and weights are calculated and stored in the
          output spatial weights matrix file.
      Unique_ID_Field (Field):
          An integer field containing a unique value for each feature in the
          input feature class. If you don't have a field with unique ID values,
          you can create one by adding an integer field to your feature class
          table and calculating the field values to equal the FID or OBJECTID
          field.
      Input_Network_Data_Source (Network Data Source):
          The network dataset used to find neighbors of each input feature.
          Network datasets usually represent street networks but can also
          represent other kinds of transportation networks such as railroads or
          walking paths. The network dataset must include at least one attribute
          related to distance, travel time, or cost.
      Travel_Mode (String):
          The mode of transportation for the analysis. A travel mode defines how
          a pedestrian, car, truck, or other medium of transportation moves
          through the network and represents a collection of network settings,
          such as travel restrictions and U-turn policies.An arcpy.na.TravelMode
          object and a string containing the valid JSON
          representation of a travel mode can also be used as input to the
          parameter.
      Impedance_Distance_Cutoff {Linear Unit}:
          The maximum impedance distance allowed for neighbors of a feature. Any
          feature whose distance is farther than this value will not be used as
          a neighbor. By default, no distance cutoff is used.
      Impedance_Temporal_Cutoff {Time Unit}:
          The maximum impedance travel time allowed for neighbors of a feature.
          Any feature whose travel time is longer than this value will not be
          used as a neighbor. By default, no temporal cutoff is used.
      Impedance_Cost_Cutoff {Double}:
          The maximum impedance cost allowed for neighbors of a feature. Any
          feature whose cost of travel is larger than this value will not be
          used as a neighbor. By default, no cost cutoff is used.
      Maximum_Number_of_Neighbors {Long}:
          An integer reflecting the maximum number of neighbors for each
          feature. The actual number of neighbors used for each feature may be
          smaller due to impedance cutoffs.
      Time_of_Day {Date}:
          The time of day traffic conditions will be considered in the analysis.
          Traffic conditions can impact the distance that can be traveled over a
          given time. If no date or time is provided, the analysis will not
          consider the impact of traffic.Instead of using a particular date, you
          can specify a day of the week
          using the following dates:

          * Today-12/30/1899

          * Sunday-12/31/1899

          * Monday-1/1/1900

          * Tuesday-1/2/1900

          * Wednesday-1/3/1900

          * Thursday-1/4/1900

          * Friday-1/5/1900

          * Saturday-1/6/1900
          For example, to specify that travel should begin at 5:00 p.m. on
          Tuesday, specify the parameter value as 1/2/1900 5:00 PM.
      Time_Zone {String}:
          Specifies the time zone for the Time_of_Day parameter.

          * LOCAL_TIME_AT_LOCATIONS-The time zone in which the
          Input_Feature_Class is located will be used. This is the default.

          * UTC-Coordinated universal time (UTC) will be used.
      Barriers {Feature Layer}:
          The features that represent blocked intersections, road closures,
          accident sites, or other locations where travel is blocked along the
          network.
      Search_Tolerance {Linear Unit}:
          The maximum distance used to assign each input feature to a location
          on the network. If any of the input points do not fall exactly on a
          line of the network, they will be assigned to the closest location on
          the network for the analysis. However, if the feature is farther than
          the search tolerance value from any location on the network, it will
          not be assigned to the network and will not be included in the
          analysis.
      Conceptualization_of_Spatial_Relationships {String}:
          Specifies how weights will be defined for each neighbor.

          * INVERSE-Features farther in distance, time, or cost will have a
          smaller weight than features nearby. The weights decrease by their
          inverse to an exponent.

          * FIXED-All neighbors will be given equal weight. This is the default.
      Exponent {Double}:
          The exponent used when INVERSE is specified for the
          Conceptualization_of_Spatial_Relationships parameter. The weights
          assigned to each neighbor are calculated by taking the inverse
          distance, time, or cost to the power of the exponent. The default
          value is 1, and the value must be between 0.01 and 4. Weights drop off
          more rapidly as the exponent increases.
      Row_Standardization {Boolean}:
          Specifies whether row standardization will be applied. Row
          standardization is recommended when the locations of the input points
          are potentially biased due to sampling design or an imposed
          aggregation scheme. It is also recommended that you standardize rows
          when weighting neighbors based on inverse distance, time, or cost.

          * ROW_STANDARDIZATION-Spatial weights will be standardized by row.
          Each weight is divided by its row sum. This is the default.

          * NO_STANDARDIZATION-No standardization of spatial weights will be
          applied.

     OUTPUTS:
      Output_Spatial_Weights_Matrix_File (File):
          The output network spatial weights matrix file (.swm) that will store
          the neighbors and weights for each input feature."""
    ...

@gptooldoc('GenerateNetworkSpatialWeights_stats', None)
def GenerateNetworkSpatialWeights(Input_Feature_Class=..., Unique_ID_Field=..., Output_Spatial_Weights_Matrix_File=..., Input_Network=..., Impedance_Attribute=..., Impedance_Cutoff=..., Maximum_Number_of_Neighbors=..., Barriers=..., U_turn_Policy=..., Restrictions=..., Use_Hierarchy_in_Analysis=..., Search_Tolerance=..., Conceptualization_of_Spatial_Relationships=..., Exponent=..., Row_Standardization=..., Travel_Mode=..., Time_of_Day=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateNetworkSpatialWeights_stats(Input_Feature_Class, Unique_ID_Field, Output_Spatial_Weights_Matrix_File, Input_Network, Impedance_Attribute, {Impedance_Cutoff}, {Maximum_Number_of_Neighbors}, {Barriers}, {U_turn_Policy}, {Restrictions;Restrictions...}, {Use_Hierarchy_in_Analysis}, {Search_Tolerance}, {Conceptualization_of_Spatial_Relationships}, {Exponent}, {Row_Standardization}, {Travel_Mode}, {Time_of_Day})

        Constructs a spatial weights matrix file (.swm) using a Network
        dataset, defining feature spatial relationships in terms of the
        underlying network structure.

     INPUTS:
      Input_Feature_Class (Feature Class):
          The point feature class for which network spatial relationships among
          features will be assessed.
      Unique_ID_Field (Field):
          An integer field containing a different value for every feature in the
          input feature class. If you don't have a Unique ID field, you can
          create one by adding an integer field to your feature class table and
          calculating the field values to equal the FID or OBJECTID field.
      Input_Network (Network Dataset Layer):
          The network dataset for which spatial relationships among features in
          the input feature class will be defined. Network datasets most often
          represent street networks but may represent other kinds of
          transportation networks as well. The network dataset needs at least
          one time-based and one distance-based cost attribute.
      Impedance_Attribute (String):
          The type of cost units to use as impedance in the analysis.
      Impedance_Cutoff {Double}:
          Specifies a cutoff value for INVERSE and FIXED conceptualizations of
          spatial relationships. Enter this value using the units specified by
          the Impedance_Attribute parameter.A value of zero indicates that no
          threshold is applied. When this
          parameter is left blank, a default threshold value is computed based
          on input feature class extent and the number of features.
      Maximum_Number_of_Neighbors {Long}:
          An integer reflecting the maximum number of neighbors to find for each
          feature.
      Barriers {Feature Layer}:
          The name of a point feature class with features representing blocked
          intersections, road closures, accident sites, or other locations where
          travel is blocked along the network.
      U-turn_Policy {String}:
          Specifies optional U-turn restrictions.

          * ALLOW_UTURNS-U-turns will be allowed anywhere. This is the default.

          * NO_UTURNS-No U-turns will be allowed during navigation.

          * ALLOW_DEAD_ENDS_ONLY-U-turns will be allowed only at dead ends (that
          is, single-valent junctions).

          * ALLOW_DEAD_ENDS_AND_INTERSECTIONS_ONLY-U-turns will be allowed only
          at dead ends and intersections.
      Restrictions {String}:
          A list of restrictions. Check the restrictions to be honored in
          spatial relationship computations.
      Use_Hierarchy_in_Analysis {Boolean}:
          Specifies whether to use a hierarchy in the analysis.

          * USE_HIERARCHY-The network dataset's hierarchy attribute will be used
          in a heuristic path algorithm to speed analysis.

          * NO_HIERARCHY-An exact path algorithm will be used instead. If there
          is no hierarchy attribute, this option does not affect analysis.
      Search_Tolerance {Linear Unit}:
          The search threshold used to locate features in the
          Input_Feature_Class onto the network dataset. This parameter includes
          a search value and the units for the tolerance.
      Conceptualization_of_Spatial_Relationships {String}:
          Specifies how the weighting associated with each spatial relationship
          is specified.

          * INVERSE-Features farther away have a smaller weight than features
          nearby.

          * FIXED-Features within the Impendance_Cutoff are neighbors (weight of
          1); features outside the Impendance_Cutoff are not weighted (weight of
          0).
      Exponent {Double}:
          Parameter for the INVERSE Conceptualization_of_Spatial_Relationships
          calculation. Typical values are 1 or 2. Weights drop off quicker with
          distance as this exponent value increases.
      Row_Standardization {Boolean}:
          Specifies whether row standardization is applied. Row standardization
          is recommended whenever feature distribution is potentially biased due
          to sampling design or to an imposed aggregation scheme.

          * ROW_STANDARDIZATION-Spatial weights are standardized by row. Each
          weight is divided by its row sum.

          * NO_STANDARDIZATION-No standardization of spatial weights is applied.
      Travel_Mode {String}:
          The mode of transportation for the analysis. Custom is always a
          choice. For other travel modes to appear, they must be present in the
          network dataset specified in the Network Dataset parameter.A travel
          mode is defined on a network dataset and provides override
          values for parameters that model car, truck, pedestrian, or other
          modes of travel.
      Time_of_Day {Date}:
          Specifies whether travel times should consider traffic conditions.
          Especially in urbanized areas, traffic conditions can significantly
          impact the area covered within a specified travel time. If no date or
          time is specified, the distance covered during a specified travel time
          will not be impacted by traffic.

     OUTPUTS:
      Output_Spatial_Weights_Matrix_File (File):
          The output network spatial weights matrix (.swm) file."""
    ...

@gptooldoc('GenerateSpatialWeightsMatrix_stats', None)
def GenerateSpatialWeightsMatrix(Input_Feature_Class=..., Unique_ID_Field=..., Output_Spatial_Weights_Matrix_File=..., Conceptualization_of_Spatial_Relationships=..., Distance_Method=..., Exponent=..., Threshold_Distance=..., Number_of_Neighbors=..., Row_Standardization=..., Input_Table=..., Date_Time_Field=..., Date_Time_Interval_Type=..., Date_Time_Interval_Value=..., Use_Z_values=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GenerateSpatialWeightsMatrix_stats(Input_Feature_Class, Unique_ID_Field, Output_Spatial_Weights_Matrix_File, Conceptualization_of_Spatial_Relationships, {Distance_Method}, {Exponent}, {Threshold_Distance}, {Number_of_Neighbors}, {Row_Standardization}, {Input_Table}, {Date_Time_Field}, {Date_Time_Interval_Type}, {Date_Time_Interval_Value}, {Use_Z_values})

        Generates a spatial weights matrix file (.swm) to represent the
        spatial relationships among features in a dataset.

     INPUTS:
      Input_Feature_Class (Feature Class):
          The feature class for which spatial relationships of features will be
          assessed.
      Unique_ID_Field (Field):
          An integer field containing a different value for every feature in the
          input feature class. If you don't have a Unique ID field, you can
          create one by adding an integer field to your feature class table and
          calculating the field values to equal the FID or OBJECTID field.
      Conceptualization_of_Spatial_Relationships (String):
          Specifies how spatial relationships among features will be
          conceptualized.

          * INVERSE_DISTANCE-The impact of one feature on another feature will
          decrease with distance.

          * FIXED_DISTANCE-Everything within a specified critical distance of
          each feature will be included in the analysis; everything outside the
          critical distance will be excluded.

          * K_NEAREST_NEIGHBORS-The closest k features will be included in the
          analysis; k is a specified numeric parameter.

          * CONTIGUITY_EDGES_ONLY-Polygon features that share a boundary will be
          neighbors.

          * CONTIGUITY_EDGES_CORNERS-Polygon features that share a boundary or
          share a node will be neighbors.

          * DELAUNAY_TRIANGULATION-A mesh of nonoverlapping triangles will be
          created from feature centroids, and features associated with triangle
          nodes that share edges will be neighbors.

          * SPACE_TIME_WINDOW-Features within a specified critical distance and
          specified time interval of each other will be neighbors.

          * CONVERT_TABLE-Spatial relationships will be defined in a table.
      Distance_Method {String}:
          Specifies how distances will be calculated from each feature to
          neighboring features.

          * EUCLIDEAN-The straight-line distance between two points (as the crow
          flies) will be calculated. This is the default.

          * MANHATTAN-The distance between two points measured along axes at
          right angles (city block) will be calculated by summing the (absolute)
          difference between the x- and y-coordinates.
      Exponent {Double}:
          The value for inverse distance calculation. A typical value is 1 or 2.
      Threshold_Distance {Double}:
          The cutoff distance for the Conceptualization_of_Spatial_Relationships
          parameter's INVERSE_DISTANCE and FIXED_DISTANCE options. Enter this
          value using the units specified in the environment output coordinate
          system. This defines the size of the space window for the
          SPACE_TIME_WINDOW option.When this parameter is left blank, a default
          threshold value is
          computed based on the output feature class extent and the number of
          features. For the inverse distance conceptualization of spatial
          relationships, a value of zero indicates that no threshold distance
          will be applied and all features will be neighbors of every other
          feature.
      Number_of_Neighbors {Long}:
          An integer reflecting either the minimum or the exact number of
          neighbors. When the Conceptualization_of_Spatial_Relationships
          parameter is set to K_NEAREST_NEIGHBORS, each feature will have
          exactly this specified number of neighbors. For the INVERSE_DISTANCE
          or FIXED_DISTANCE option, each feature will have at least this many
          neighbors (the threshold distance will be temporarily extended to
          ensure this many neighbors, if necessary). When CONTIGUITY_EDGES_ONLY
          or CONTIGUITY_EDGES_CORNERS option is chosen, each polygon will be
          assigned this minimum number of neighbors. For polygons with fewer
          than this number of contiguous neighbors, additional neighbors will be
          based on feature centroid proximity.
      Row_Standardization {Boolean}:
          Specifies whether spatial weights will be standardized by row. Row
          standardization is recommended whenever feature distribution is
          potentially biased due to sampling design or to an imposed aggregation
          scheme.

          * ROW_STANDARDIZATION-Spatial weights will be standardized by row.
          Each weight is divided by its row sum. This is the default.

          * NO_STANDARDIZATION-No standardization of spatial weights will be
          applied.
      Input_Table {Table}:
          A table containing numeric weights relating every feature to every
          other feature in the input feature class. Required fields for the
          table are the Unique ID Field parameter value, NID (neighbor ID), and
          WEIGHT.
      Date_Time_Field {Field}:
          A date field with a time stamp for each feature.
      Date_Time_Interval_Type {String}:
          Specifies the units that will be used for measuring time.

          * SECONDS-The unit will be seconds.

          * MINUTES-The unit will be minutes.

          * HOURS-The unit will be hours.

          * DAYS-The unit will be days.

          * WEEKS-The unit will be weeks.

          * MONTHS-The unit will be 30 days.

          * YEARS-The unit will be years.
      Date_Time_Interval_Value {Long}:
          An integer reflecting the number of time units comprising the time
          window.
      Use_Z_values {Boolean}:
          Specifies whether z-coordinates will be used in the construction of
          the spatial weights matrix if the input features are z-enabled.

          * USE_Z_VALUES-Z-values will be used in the construction of the
          spatial weights matrix.

          * DO_NOT_USE_Z_VALUES-Z-values will not be used; they will be ignored
          and only x- and y-coordinates will be considered in the construction
          of the spatial weights matrix. This is the default.

     OUTPUTS:
      Output_Spatial_Weights_Matrix_File (File):
          The full path for the output spatial weights matrix file (.swm)."""
    ...

@gptooldoc('GeographicallyWeightedRegression_stats', None)
def GeographicallyWeightedRegression(in_features=..., dependent_field=..., explanatory_field=..., out_featureclass=..., kernel_type=..., bandwidth_method=..., distance=..., number_of_neighbors=..., weight_field=..., coefficient_raster_workspace=..., cell_size=..., in_prediction_locations=..., prediction_explanatory_field=..., out_prediction_featureclass=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """GeographicallyWeightedRegression_stats(in_features, dependent_field, explanatory_field;explanatory_field..., out_featureclass, kernel_type, bandwidth_method, {distance}, {number_of_neighbors}, {weight_field}, {coefficient_raster_workspace}, {cell_size}, {in_prediction_locations}, {prediction_explanatory_field;prediction_explanatory_field...}, {out_prediction_featureclass})

        Performs Geographically Weighted Regression (GWR), a local form of
        linear regression used to model spatially varying relationships.

     INPUTS:
      in_features (Feature Layer):
          The feature class containing the dependent and independent variables.
      dependent_field (Field):
          The numeric field containing the values that will be modeled.
      explanatory_field (Field):
          A list of fields representing independent explanatory variables in the
          regression model.
      kernel_type (String):
          Specifies whether the kernel is constructed as a fixed distance, or if
          it is allowed to vary in extent as a function of feature density.

          * FIXED-The spatial context (the Gaussian kernel) used to solve each
          local regression analysis is a fixed distance.

          * ADAPTIVE-The spatial context (the Gaussian kernel) is a function of
          a specified number of neighbors. Where feature distribution is dense,
          the spatial context is smaller; where feature distribution is sparse,
          the spatial context is larger.
      bandwidth_method (String):
          Specifies how the extent of the kernel will be determined. When AICc
          or CV is selected, the tool will find the optimal distance or number
          of neighbors. Typically, you will select either AICc or CV when you
          aren't sure what to use for the distance or number_of_neighbors
          parameter. Once the tool determines the optimal distance or number of
          neighbors, however, you'll use the BANDWIDTH_PARAMETER option.

          * AICc-The extent of the kernel is determined using the Akaike
          Information Criterion (AICc).

          * CV-The extent of the kernel is determined using Cross Validation.

          * BANDWIDTH_PARAMETER-The extent of the kernel is determined by a
          fixed distance or a fixed number of neighbors. You must specify a
          value for either the distance or number_of_neighbors parameter.
      distance {Double}:
          The distance to use when kernel_type is FIXED and bandwidth_method is
          BANDWIDTH_PARAMETER.
      number_of_neighbors {Long}:
          The exact number of neighbors to include in the local bandwidth of the
          Gaussian kernel when kernel_type is ADAPTIVE and bandwidth_method is
          BANDWIDTH_PARAMETER.
      weight_field {Field}:
          The numeric field containing a spatial weighting for individual
          features. This weight field allows some features to be more important
          in the model calibration process than others. This is useful when the
          number of samples taken at different locations varies, values for the
          dependent and independent variables are averaged, and places with more
          samples are more reliable (should be weighted higher). If you have an
          average of 25 different samples for one location but an average of
          only 2 samples for another location, for example, you can use the
          number of samples as your weight field so that locations with more
          samples have a larger influence on model calibration than locations
          with few samples.
      coefficient_raster_workspace {Workspace}:
          The full path to the workspace where the coefficient rasters will be
          created. When this workspace is provided, rasters are created for the
          intercept and every explanatory variable.
      cell_size {Analysis Cell Size}:
          The cell size (a number) or reference to the cell size (a path to a
          raster dataset) to use when creating the coefficient rasters.The
          default cell size is the shortest of the width or height of the
          extent specified in the geoprocessing environment output coordinate
          system, divided by 250.
      in_prediction_locations {Feature Layer}:
          A feature class containing features representing locations where
          estimates should be computed. Each feature in this dataset should
          contain values for all of the explanatory variables specified; the
          dependent variable for these features will be estimated using the
          model calibrated for the input feature class data.
      prediction_explanatory_field {Field}:
          A list of fields representing explanatory variables in the Prediction
          locations feature class. These field names should be provided in the
          same order (a one-to-one correspondence) as those listed for the input
          feature class Explanatory variables parameter. If no prediction
          explanatory variables are given, the output prediction feature class
          will only contain computed coefficient values for each prediction
          location.

     OUTPUTS:
      out_featureclass (Feature Class):
          The output feature class that will receive dependent variable
          estimates and residuals.
      out_prediction_featureclass {Feature Class}:
          The output feature class to receive dependent variable estimates for
          each feature in the Prediction locations feature class."""
    ...

@gptooldoc('LocalBivariateRelationships_stats', None)
def LocalBivariateRelationships(in_features=..., dependent_variable=..., explanatory_variable=..., output_features=..., number_of_neighbors=..., number_of_permutations=..., enable_local_scatterplot_popups=..., level_of_confidence=..., apply_false_discovery_rate_fdr_correction=..., scaling_factor=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """LocalBivariateRelationships_stats(in_features, dependent_variable, explanatory_variable, output_features, {number_of_neighbors}, {number_of_permutations}, {enable_local_scatterplot_popups}, {level_of_confidence}, {apply_false_discovery_rate_fdr_correction}, {scaling_factor})

        Analyzes two variables for statistically significant relationships
        using local entropy. Each feature is classified into one of six
        categories based on the type of relationship. The output can be used
        to visualize areas where the variables are related and explore how
        their relationship changes across the study area.

     INPUTS:
      in_features (Feature Layer):
          The feature class containing fields representing the
          dependent_variable and explanatory_variable values.
      dependent_variable (Field):
          The numeric field representing the values of the dependent variable.
          When categorizing the relationships, the explanatory_variable value is
          used to predict the dependent_variable value.
      explanatory_variable (Field):
          The numeric field representing the values of the explanatory variable.
          When categorizing the relationships, the explanatory_variable value is
          used to predict the dependent_variable value.
      number_of_neighbors {Long}:
          The number of neighbors around each feature (including the feature)
          that will be used to test for a local relationship between the
          variables. The number of neighbors must be between 30 and 1,000, and
          the default is 30. The provided value should be large enough to detect
          the relationship between features, but small enough to still identify
          local patterns.
      number_of_permutations {Long}:
          Specifies the number of permutations that will be used to calculate
          the pseudo p-value for each feature. Choosing a number of permutations
          is a balance between precision in the pseudo p-value and increased
          processing time.

          * 99-With 99 permutations, the smallest possible pseudo p-value is
          0.01, and all other pseudo p-values will be multiples of this value.

          * 199-With 199 permutations, the smallest possible pseudo p-value is
          0.005, and all other pseudo p-values will be multiples of this value.
          This is the default.

          * 499-With 499 permutations, the smallest possible pseudo p-value is
          0.002, and all other pseudo p-values will be multiples of this value.

          * 999-With 999 permutations, the smallest possible pseudo p-value is
          0.001, and all other pseudo p-values will be multiples of this value.
      enable_local_scatterplot_popups {Boolean}:
          Specifies whether scatterplot pop-ups will be generated for each
          output feature. Each scatterplot displays the values of the
          explanatory (horizontal axis) and dependent (vertical axis) variables
          in the local neighborhood along with a fitted line or curve
          visualizing the form of the relationship. Scatterplot charts are not
          supported for shapefile outputs.

          * CREATE_POPUP-Local scatterplot pop-ups will be generated for each
          feature in the dataset. This is the default.

          * NO_POPUP-Local scatterplot pop-ups will not be generated.
      level_of_confidence {String}:
          Specifies a confidence level of the hypothesis test for significant
          relationships.

          * 90%-The confidence level is 90 percent. This is the default.

          * 95%-The confidence level is 95 percent.

          * 99%-The confidence level is 99 percent.
      apply_false_discovery_rate_fdr_correction {Boolean}:
          Specifies whether False Discover Rate (FDR) correction will be applied
          to the pseudo p-values.

          * APPLY_FDR-Statistical significance will be based on the FDR
          correction. This is the default.

          * NO_FDR-Statistical significance will be based on the pseudo p-value.
      scaling_factor {Double}:
          The level of sensitivity to subtle relationships between the
          variables. Larger values (closer to one) can detect relatively weak
          relationships, while smaller values (closer to zero) will only detect
          strong relationships. Smaller values are also more robust to outliers.
          The value must be between 0.01 and 1, and the default is 0.5.

     OUTPUTS:
      output_features (Feature Class):
          The output feature class containing all input features with fields
          representing the dependent_variable value, explanatory_variable value,
          entropy score, pseudo p-value, level of significance, type of
          categorized relationship, and diagnostics related to the
          categorization."""
    ...

@gptooldoc('MGWR_stats', None)
def MGWR(in_features=..., dependent_variable=..., model_type=..., explanatory_variables=..., output_features=..., neighborhood_type=..., neighborhood_selection_method=..., minimum_number_of_neighbors=..., maximum_number_of_neighbors=..., distance_unit=..., minimum_search_distance=..., maximum_search_distance=..., number_of_neighbors_increment=..., search_distance_increment=..., number_of_increments=..., number_of_neighbors=..., distance_band=..., number_of_neighbors_golden=..., number_of_neighbors_manual=..., number_of_neighbors_defined=..., distance_golden=..., distance_manual=..., distance_defined=..., prediction_locations=..., explanatory_variables_to_match=..., output_predicted_features=..., robust_prediction=..., local_weighting_scheme=..., output_table=..., coefficient_raster_workspace=..., scale=..., number_of_neighbors_gradient=..., distance_gradient=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """MGWR_stats(in_features, dependent_variable, model_type, explanatory_variables;explanatory_variables..., output_features, neighborhood_type, neighborhood_selection_method, {minimum_number_of_neighbors}, {maximum_number_of_neighbors}, {distance_unit}, {minimum_search_distance}, {maximum_search_distance}, {number_of_neighbors_increment}, {search_distance_increment}, {number_of_increments}, {number_of_neighbors}, {distance_band}, {number_of_neighbors_golden;number_of_neighbors_golden...}, {number_of_neighbors_manual;number_of_neighbors_manual...}, {number_of_neighbors_defined;number_of_neighbors_defined...}, {distance_golden;distance_golden...}, {distance_manual;distance_manual...}, {distance_defined;distance_defined...}, {prediction_locations}, {explanatory_variables_to_match;explanatory_variables_to_match...}, {output_predicted_features}, {robust_prediction}, {local_weighting_scheme}, {output_table}, {coefficient_raster_workspace}, {scale}, {number_of_neighbors_gradient;number_of_neighbors_gradient...}, {distance_gradient;distance_gradient...})

        Performs Multiscale Geographically Weighted Regression (MGWR), which
        is a local form of linear regression that models spatially varying
        relationships.

     INPUTS:
      in_features (Feature Layer):
          The feature class containing the dependent and explanatory variables.
      dependent_variable (Field):
          The numeric field containing the observed values that will be modeled.
      model_type (String):
          Specifies the regression model based on the values of the dependent
          variable. Currently, only continuous data is supported, and the
          parameter is hidden in the Geoprocessing pane. Do not use categorical,
          count, or binary dependent variables.

          * CONTINUOUS-The dependent variable represents continuous values. This
          is the default.
      explanatory_variables (Field):
          A list of fields that will be used as independent explanatory
          variables in the regression model.
      neighborhood_type (String):
          Specifies whether the neighborhood will be a fixed distance or allowed
          to vary spatially depending on the density of the features.

          * NUMBER_OF_NEIGHBORS-The neighborhood size will be a specified
          number of closest neighbors for each feature. Where features are
          dense, the spatial extent of the neighborhood will be smaller; where
          features are sparse, the spatial extent of the neighborhood will be
          larger.

          * DISTANCE_BAND-The neighborhood size will be a constant or fixed
          distance for each feature.
      neighborhood_selection_method (String):
          Specifies how the neighborhood size will be determined.

          * GOLDEN_SEARCH-An optimal distance or number of neighbors will be
          identified by minimizing the AICc value using the Golden Search
          algorithm. This option takes the longest to calculate, especially for
          large or high-dimensional datasets.

          * GRADIENT_SEARCH-An optimal distance or number of neighbors will be
          identified by minimizing the AICc value using the gradient-based
          optimization algorithm. This option runs the fastest and requires
          significantly less memory usage than Golden Search.

          * MANUAL_INTERVALS-A distance or number of neighbors will be
          identified by testing a range of values and determining the value with
          the smallest AICc. If the neighborhood_type parameter is set to
          DISTANCE_BAND, the minimum value of this range is provided by the
          minimum_search_distance parameter. The minimum value is then
          incremented by the value specified in the search_distance_increment
          parameter. This is repeated the number of times specified by the
          number_of_increments parameter. If the neighborhood_type parameter is
          set to NUMBER_OF_NEIGHBORS, the minimum value, increment size, and
          number of increments are provided by the minimum_number_of_neighbors,
          number_of_neighbors_increment, and number_of_increments parameters,
          respectively.

          * USER_DEFINED-The neighborhood size will be specified by either the
          number_of_neighbors parameter value or the distance_band parameter
          value.
      minimum_number_of_neighbors {Long}:
          The minimum number of neighbors that each feature will include in its
          calculation. It is recommended that you use at least 30 neighbors.
      maximum_number_of_neighbors {Long}:
          The maximum number of neighbors that each feature will include in its
          calculations.
      distance_unit {String}:
          Specifies the unit of distance that will be used to measure the
          distances between features.

          * FEETINT-Distances will be measured in international feet.

          * MILESINT-Distances will be measured in statute miles.

          * FEET-Distances will be measured in US survey feet.

          * METERS-Distances will be measured in meters.

          * KILOMETERS-Distances will be measured in kilometers.

          * MILES-Distances will be measured in US survey miles.
      minimum_search_distance {Double}:
          The minimum search distance that will be applied to every explanatory
          variable. It is recommended that you provide a minimum distance that
          includes at least 30 neighbors for each feature.
      maximum_search_distance {Double}:
          The maximum neighborhood search distance that will be applied to all
          variables.
      number_of_neighbors_increment {Long}:
          The number of neighbors by which manual intervals will increase for
          each neighborhood test.
      search_distance_increment {Double}:
          The distance by which manual intervals will increase for each
          neighborhood test.
      number_of_increments {Long}:
          The number of neighborhood sizes to test when using manual intervals.
          The first neighborhood size is the value of the
          minimum_number_of_neighbors or minimum_search_distance parameter.
      number_of_neighbors {Long}:
          The number of neighbors that will be used for the user-defined
          neighborhood type.
      distance_band {Double}:
          The size of the distance band that will be used for the user-defined
          neighborhood type. All features within this distance will be included
          as neighbors in the local models.
      number_of_neighbors_golden {Value Table}:
          The customized Golden Search options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          variable, the minimum number of neighbors, and the maximum number of
          neighbors in the columns.
      number_of_neighbors_manual {Value Table}:
          The customized manual intervals options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          minimum number of neighbors, number of neighbors increment, and number
          of increments in the columns.
      number_of_neighbors_defined {Value Table}:
          The customized user-defined options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          number of neighbors.
      distance_golden {Value Table}:
          The customized Golden Search options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          variable, the minimum search distance, and the maximum search distance
          in the columns.
      distance_manual {Value Table}:
          The customized manual intervals options for individual explanatory
          variables. For each variable to be customized, provide the variable,
          the minimum search distance, search distance increments, and number of
          increments in the columns.
      distance_defined {Value Table}:
          The customized user-defined options for individual explanatory
          variables. For each variable to be customized, provide the variable
          and the distance band in the columns.
      prediction_locations {Feature Layer}:
          A feature class with the locations where estimates will be computed.
          Each feature in this dataset should contain a value for every
          explanatory variables specified. The dependent variable for these
          features will be estimated using the model calibrated for the input
          feature class data. These feature locations should be close to (within
          115 percent of the extent) or within the same study area as the input
          features.
      explanatory_variables_to_match {Value Table}:
          The explanatory variables from the prediction locations that match
          corresponding explanatory variables from the input features.
      robust_prediction {Boolean}:
          Specifies the features that will be used in the prediction
          calculations.

          * ROBUST-Features with values greater than three standard deviations
          from the mean (value outliers) and features with weights of 0 (spatial
          outliers) will be excluded from the prediction calculations but will
          receive predictions in the output feature class. This is the default.

          * NON_ROBUST-Every feature will be used in the prediction
          calculations.
      local_weighting_scheme {String}:
          Specifies the kernel type that will be used to provide the spatial
          weighting in the model. The kernel defines how each feature is related
          to other features within its neighborhood.

          * BISQUARE-A weight of zero will be assigned to any feature outside
          the neighborhood specified. This is the default.

          * GAUSSIAN-All features will receive weights, but weights become
          exponentially smaller the farther away they are from the target
          feature.
      coefficient_raster_workspace {Workspace}:
          The workspace where the coefficient rasters will be created. When this
          workspace is provided, rasters are created for the intercept and every
          explanatory variable. This parameter is only available with a Desktop
          Advanced license. If a directory is provided, the rasters will be TIFF
          (.tif) raster type.
      scale {Boolean}:
          Specifies whether the values of the explanatory and dependent
          variables will be scaled to have mean zero and standard deviation one
          prior to fitting the model.

          * SCALE_DATA-The values of the variables will be scaled. The results
          will contain scaled and unscaled versions of the explanatory variable
          coefficients.

          * NO_SCALE_DATA-The values of the variables will not be scaled. All
          coefficients will be unscaled and in original data units.
      number_of_neighbors_gradient {Value Table}:
          The customized Gradient Search options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          variable, the minimum number of neighbors, and the maximum number of
          neighbors in the columns.
      distance_gradient {Value Table}:
          The customized Gradient Search options for individual explanatory
          variables. For each explanatory variable to be customized, provide the
          variable, the minimum search distance, and the maximum search distance
          in the columns.

     OUTPUTS:
      output_features (Feature Class):
          The new feature class containing the coefficients, residuals, and
          significance levels of the MGWR model.
      output_predicted_features {Feature Class}:
          The output feature class that will receive dependent variable
          estimates for every prediction location.
      output_table {Table}:
          A table containing the output statistics of the MGWR model. A bar
          chart of estimated bandwidths or numbers of neighbors will be included
          with the output."""
    ...

@gptooldoc('OrdinaryLeastSquares_stats', None)
def OrdinaryLeastSquares(Input_Feature_Class=..., Unique_ID_Field=..., Output_Feature_Class=..., Dependent_Variable=..., Explanatory_Variables=..., Coefficient_Output_Table=..., Diagnostic_Output_Table=..., Output_Report_File=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """OrdinaryLeastSquares_stats(Input_Feature_Class, Unique_ID_Field, Output_Feature_Class, Dependent_Variable, Explanatory_Variables;Explanatory_Variables..., {Coefficient_Output_Table}, {Diagnostic_Output_Table}, {Output_Report_File})

        Performs global Ordinary Least Squares (OLS) linear regression to
        generate predictions or to model a dependent variable in terms of its
        relationships to a set of explanatory variables.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class containing the dependent and independent variables
          for analysis.
      Unique_ID_Field (Field):
          An integer field containing a different value for every feature in the
          Input Feature Class.
      Dependent_Variable (Field):
          The numeric field containing values for what you are trying to model.
      Explanatory_Variables (Field):
          A list of fields representing explanatory variables in your regression
          model.

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          The output feature class that will receive dependent variable
          estimates and residuals.
      Coefficient_Output_Table {Table}:
          The full path to an optional table that will receive model
          coefficients, standardized coefficients, standard errors, and
          probabilities for each explanatory variable.
      Diagnostic_Output_Table {Table}:
          The full path to an optional table that will receive model summary
          diagnostics.
      Output_Report_File {File}:
          The path to the optional PDF file the tool will create. This report
          file includes model diagnostics, graphs, and notes to help you
          interpret the OLS results."""
    ...

@gptooldoc('PredictUsingSSMFile_stats', None)
def PredictUsingSSMFile(input_model=..., prediction_type=..., features_to_predict=..., output_features=..., output_raster=..., explanatory_variable_matching=..., explanatory_distance_matching=..., explanatory_rasters_matching=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PredictUsingSSMFile_stats(input_model, prediction_type, {features_to_predict}, {output_features}, {output_raster}, {explanatory_variable_matching;explanatory_variable_matching...}, {explanatory_distance_matching;explanatory_distance_matching...}, {explanatory_rasters_matching;explanatory_rasters_matching...})

        Predicts continuous or categorical values using a trained spatial
        statistics model (.ssm file).

     INPUTS:
      input_model (File):
          The spatial statistics model file that will be used to make new
          predictions.
      prediction_type (String):
          Specifies the operation mode that will be used. The tool can predict
          new features or create a prediction raster surface.

          * PREDICT_FEATURES-Predictions or classifications will be generated
          for features. Explanatory variables must be provided to match
          variables used to train the input model file. The output of this
          option will be a feature class and model diagnostics in the messages
          window.

          * PREDICT_RASTER-A prediction raster will be generated for the area
          where the explanatory rasters intersect. Explanatory rasters must be
          provided to match the rasters used to train the input model file. The
          output of this option will be a prediction surface and model
          diagnostics in the messages window
      features_to_predict {Feature Layer}:
          The feature class representing locations where predictions will be
          made. This feature class must also contain any explanatory variables
          provided as fields that correspond to those used to train the input
          model.
      explanatory_variable_matching {Value Table}:
          A list of the explanatory variables of the input model and
          corresponding fields of the input prediction features. For each
          explanatory variable in the Training column, provide the corresponding
          prediction field in the Prediction column. The Categorical column
          specifies whether the variable is categorical or continuous.
      explanatory_distance_matching {Value Table}:
          A list of the explanatory distance features of the input model and
          corresponding prediction distance features. For each explanatory
          distance feature in the Training column, provide the corresponding
          prediction distance feature in the Prediction column.
      explanatory_rasters_matching {Value Table}:
          A list of the explanatory rasters of the input model and corresponding
          prediction rasters. For each explanatory raster in the Training
          column, provide the corresponding prediction raster in the Prediction
          column. The Categorical column specifies whether the raster is
          categorical or continuous.

     OUTPUTS:
      output_features {Feature Class}:
          The output feature class containing the prediction results.
      output_raster {Raster Dataset}:
          The output raster containing the prediction results. The default cell
          size will be the maximum cell size of the input rasters."""
    ...

@gptooldoc('PresenceOnlyPrediction_stats', None)
def PresenceOnlyPrediction(input_point_features=..., contains_background=..., presence_indicator_field=..., explanatory_variables=..., distance_features=..., explanatory_rasters=..., basis_expansion_functions=..., number_knots=..., study_area_type=..., study_area_polygon=..., spatial_thinning=..., thinning_distance_band=..., number_of_iterations=..., relative_weight=..., link_function=..., presence_probability_cutoff=..., output_trained_features=..., output_trained_raster=..., output_response_curve_table=..., output_sensitivity_table=..., features_to_predict=..., output_pred_features=..., output_pred_raster=..., explanatory_variable_matching=..., explanatory_distance_matching=..., explanatory_rasters_matching=..., allow_predictions_outside_of_data_ranges=..., resampling_scheme=..., number_of_groups=..., output_trained_model=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """PresenceOnlyPrediction_stats(input_point_features, {contains_background}, {presence_indicator_field}, {explanatory_variables;explanatory_variables...}, {distance_features;distance_features...}, {explanatory_rasters;explanatory_rasters...}, {basis_expansion_functions;basis_expansion_functions...}, {number_knots}, {study_area_type}, {study_area_polygon}, {spatial_thinning}, {thinning_distance_band}, {number_of_iterations}, {relative_weight}, {link_function}, {presence_probability_cutoff}, {output_trained_features}, {output_trained_raster}, {output_response_curve_table}, {output_sensitivity_table}, {features_to_predict}, {output_pred_features}, {output_pred_raster}, {explanatory_variable_matching;explanatory_variable_matching...}, {explanatory_distance_matching;explanatory_distance_matching...}, {explanatory_rasters_matching;explanatory_rasters_matching...}, {allow_predictions_outside_of_data_ranges}, {resampling_scheme}, {number_of_groups}, {output_trained_model})

        Models the presence of a phenomenon given known presence locations and
        explanatory variables using a maximum entropy approach (MaxEnt). The
        tool provides output features and rasters that include the probability
        of presence and can be applied to problems in which only presence is
        known and absence is not known.

     INPUTS:
      input_point_features (Feature Layer):
          The point features representing locations where presence of a
          phenomenon of interest is known to occur.
      contains_background {Boolean}:
          Specifies whether the input point features contain background
          points.If the input points do not contain background points, the tool
          will
          generate background points using cells in the explanatory training
          rasters. The tool uses background points to model the characteristics
          of the landscape in unknown locations and compare them to landscape
          characteristics in known presence locations. Therefore, background
          points can be considered as the study area. Generally, these are
          locations where presence of a phenomenon of interest is unknown.
          However, if any information is known about the background points, the
          relative_weight parameter can be used to indicate this.

          * PRESENCE_AND_BACKGROUND_POINTS-The input point features include
          background points.

          * PRESENCE_ONLY_POINTS-The input point features do not include
          background points. This is the default.
      presence_indicator_field {Field}:
          The field from the input point features containing binary values that
          indicate each point as presence (1) or background (0). The field must
          be numeric (Short, Long, Float, or Double types).
      explanatory_variables {Value Table}:
          A list of fields representing the explanatory variables that will help
          predict the probability of presence. You can specify whether each
          variable is categorical or numeric. Specify the CATEGORICAL option for
          each variable that represents a class or category (such as land
          cover).
      distance_features {Feature Layer}:
          A list of feature layers or feature classes that will be used to
          automatically create explanatory variables that represent the distance
          from the input point features to the nearest provided distance
          features. If the input explanatory training distance features are
          polygons or lines, the distance attributes are calculated as the
          distance between the closest segment and the point.
      explanatory_rasters {Value Table}:
          A list of rasters that will be used to automatically create
          explanatory training variables in the model whose values are extracted
          from rasters. For each feature (presence and background points) in the
          input point features, the value of the raster cell will be extracted
          at that exact location.Bilinear raster resampling will be used when
          extracting the raster
          value for continuous rasters. Nearest neighbor assignment will be used
          when extracting a raster value from categorical rasters.You can
          specify whether each raster value is categorical or numeric.
          Specify the CATEGORICAL option for each raster that represents a class
          or category (such as land cover).
      basis_expansion_functions {String}:
          Specifies the basis function that will be used to transform the
          provided explanatory variables for use in the model. If multiple basis
          functions are selected, the tool will produce multiple transformed
          variables and attempt to use them in the model.

          * LINEAR-A linear transformation to the input variables will be
          applied. This is the default

          * PRODUCT-A pairwise multiplication on continuous explanatory
          variables will be used, yielding interaction variables. This option is
          only available when multiple explanatory variables have been provided.

          * HINGE-The continuous explanatory variable values will be converted
          into two segments, a static segment (composed of zeroes or ones) and a
          linear function segment (increasing or decreasing).

          * THRESHOLD-The continuous explanatory variable values will be
          converted into a binary variable composed of zeroes and ones.

          * QUADRATIC-The square of each continuous explanatory variable value
          will be returned.
      number_knots {Long}:
          The number of knots that will be used by the hinge and threshold
          explanatory variable expansions. The value controls how many
          thresholds are created, which are used to create multiple explanatory
          variable expansions using each threshold. The value must be between 2
          and 50. The default is 10.
      study_area_type {String}:
          Specifies the type of study area that will be used to define where
          presence is possible when the input point features do not contain
          background points.

          * CONVEX_HULL-The smallest convex polygon that encloses all the
          presence points in the input point features will be used. This is the
          default

          * RASTER_EXTENT-The extent of the intersection of the explanatory
          training rasters will be used.

          * STUDY_POLYGON-A custom study area that is defined by a polygon
          feature class will be used.
      study_area_polygon {Feature Layer}:
          A feature class containing the polygons that define a custom study
          area. The input point features must be located within the custom study
          area covered by the polygon features. A study area can be composed of
          multiple polygons.
      spatial_thinning {Boolean}:
          Specifies whether spatial thinning will be applied to presence and
          background points before training the model.Spatial thinning helps to
          reduce sampling bias by removing points and
          ensuring that remaining points have a minimum nearest-neighbor
          distance, set in the thinning_distance_band parameter. Spatial
          thinning is also applied to background points whether they are
          provided in input point features or generated by the tool.

          * THINNING-Spatial thinning will be applied.

          * NO_THINNING-Spatial thinning will not be applied. This is the
          default.
      thinning_distance_band {Linear Unit}:
          The minimum distance between any two presence points or any two
          background points when spatial thinning is applied.
      number_of_iterations {Long}:
          The number of runs that will be used to find the optimal spatial
          thinning solution, seeking to maintain as many presence and background
          points as possible while ensuring that no two presence or two
          background points are within the specified thinning_distance_band
          parameter value. The minimum possible is 1 iteration and the maximum
          possible is 50 iterations. The default is 10.This parameter is only
          applicable for spatial thinning applied to
          presence and background points in the input point features. Spatial
          thinning that is applied to background points generated from raster
          cells undergo spatial thinning by resampling the raster cells to the
          specified thinning_distance_band parameter value, without needing to
          iterate for an optimal solution.
      relative_weight {Long}:
          A value between 1 and 100 that specifies the relative information
          weight of presence points to background points. The default is 100.A
          higher value indicates that presence points are the primary source
          of information; it is unknown whether background points represent
          presence or absence and background points receive lower weight in the
          model. A lower value indicates that background points also contribute
          valuable information that can be used in conjunction with presence
          points; there is greater confidence that background points represent
          absence and their information can be used in the model as absence
          locations.
      link_function {String}:
          Specifies the function that will convert the unbounded outputs of the
          model to a number between 0 and 1. This value can be interpreted as
          the probability of presence at the location. Each option converts the
          same continuous value to a different probability.

          * CLOGLOG-The C-log-log link function will be used to convert the
          predictions to probabilities. This option is recommended when the
          presence and location of a phenomenon is unambiguous, for example,
          when modeling the presence of an immobile plant species. This is the
          default.

          * LOGISTIC-The logistic link function will be used to convert
          predictions to probabilities. This option is recommended when the
          presence and location of a phenomenon is ambiguous, for example, when
          modeling the presence of a migratory animal species.
      presence_probability_cutoff {Double}:
          A cutoff value between 0.01 and 0.99 that establishes which
          probabilities correspond with presence in the resulting
          classification. The cutoff value is used to help evaluate the model's
          performance using training data and known presence points.
          Classification diagnostics are provided in geoprocessing messages and
          in the output trained features.
      features_to_predict {Feature Layer}:
          The feature class representing locations where predictions will be
          made. The feature class must contain any provided explanatory variable
          fields that were used from the input point features.When using spatial
          thinning, you can use the original input point
          features as input prediction features to receive a prediction for the
          entire dataset.
      explanatory_variable_matching {Value Table}:
          The matching explanatory variable fields for the input point features
          and input prediction features.
      explanatory_distance_matching {Value Table}:
          The matching distance features for the training and prediction.
      explanatory_rasters_matching {Value Table}:
          The matching rasters for the training and prediction.
      allow_predictions_outside_of_data_ranges {Boolean}:
          * ALLOWED-The prediction will allow extrapolation beyond the range of
          values used in training. This is the default.

          * NOT_ALLOWED-The prediction will not allow extrapolation beyond the
          range of values used in training.
      resampling_scheme {String}:
          Specifies the method that will be used to perform cross validation of
          the prediction model. Cross validation excludes a portion of the data
          during training of the model and uses it to test the model's
          performance after it is trained.

          * NONE-Cross validation will not be performed. This is the default

          * RANDOM-The points will be randomly divided into groups, and each
          group will be left out once when performing cross validation. The
          number of groups is specified in the number_of_groups parameter.
      number_of_groups {Long}:
          The number of groups that will be used in cross validation for the
          random resampling scheme. A field in the output trained features
          indicates the group that each point was assigned to. The default is 3.
          A minimum of 2 groups and a maximum of 10 groups are allowed.

     OUTPUTS:
      output_trained_features {Feature Class}:
          An output feature class that will contain all features and explanatory
          variables used in the training of the model.
      output_trained_raster {Raster Dataset}:
          The output raster with cell values indicating the probability of
          presence using the selected link function. The default cell size is
          the maximum of the cell sizes of the explanatory training rasters. An
          output trained raster can only be created if the input point features
          do not contain background points.
      output_response_curve_table {Table}:
          The output table that will contain diagnostics from the training model
          that indicate the effect of each explanatory variable on the
          probability of presence after accounting for the average effects of
          all other explanatory variables in the model.The table will have up to
          two derived charts of partial dependence
          plots: one set of line charts for continuous variables and one set of
          bar charts for categorical variables.
      output_sensitivity_table {Table}:
          The output table that will contain diagnostics of training model
          accuracy as the probability presence cutoff changes from 0 to 1.
      output_pred_features {Feature Class}:
          The output feature class that will contain the results of the
          prediction model applied to the input prediction features.
      output_pred_raster {Raster Dataset}:
          The output raster containing the prediction results at each cell of
          the matched explanatory rasters. The default cell size is the maximum
          of the cell sizes of the explanatory training rasters.
      output_trained_model {File}:
          An output model file that will save the trained model, which can be
          used later for prediction."""
    ...

@gptooldoc('SpatialAssociationBetweenZones_stats', None)
def SpatialAssociationBetweenZones(input_feature_or_raster=..., categorical_zone_field=..., overlay_feature_or_raster=..., categorical_overlay_zone_field=..., output_features=..., output_raster=..., correspondence_overlay_to_input=..., correspondence_input_to_overlay=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SpatialAssociationBetweenZones_stats(input_feature_or_raster, categorical_zone_field, overlay_feature_or_raster, categorical_overlay_zone_field, {output_features}, {output_raster}, {correspondence_overlay_to_input}, {correspondence_input_to_overlay})

        Measures the degree of spatial association between two
        regionalizations of the same study area in which each regionalization
        is composed of a set of categories, called zones. The association
        between the regionalizations is determined by the area overlap between
        zones of each regionalization. The association is highest when each
        zone of one regionalization closely corresponds to a zone of the other
        regionalization. Similarly, spatial association is lowest when the
        zones of one regionalization have large overlap with many different
        zones of the other regionalization. The primary output of the tool is
        a global measure of spatial association between the categorical
        variables: a single number ranging from 0 (no correspondence) to 1
        (perfect spatial alignment of zones). Optionally, this global
        association can be calculated and visualized for specific zones of
        either regionalization or for specific combinations of zones between
        regionalizations.

     INPUTS:
      input_feature_or_raster (Feature Layer / Raster Layer / Image Service):
          The dataset representing the zones of the first regionalization. The
          zones can be defined using polygon features or a raster.
      categorical_zone_field (Field):
          The field representing the zone category of the input zones. Each
          unique value of this field defines an individual zone. For features,
          the field must be integer or text. For rasters, the VALUE field is
          also supported.
      overlay_feature_or_raster (Feature Layer / Raster Layer / Image Service):
          The dataset representing the zones of the second regionalization. The
          zones can be polygon features or a raster.
      categorical_overlay_zone_field (Field):
          The field representing the zone category of the overlay zones. Each
          unique value of this field defines an individual zone. For features,
          the field must be integer or text. For rasters, the VALUE field is
          also supported.

     OUTPUTS:
      output_features {Feature Class}:
          The output polygon features containing spatial association measures at
          all intersections of the input and overlay zones.The output features
          can be used to measure the association between
          specific combinations of input and overlay zones, such as the
          association between areas of corn production (crop type) and areas of
          well-drained soil (soil drainage class). This parameter is only
          enabled if the input and overlay zones are both polygon features.
      output_raster {Raster Dataset}:
          The output raster containing spatial association measures between the
          input and overlay zones.The output raster will have three fields to
          indicate the spatial
          association measures for intersections of the input and overlay zones,
          correspondence of overlay zones within input zones, and correspondence
          of input zones within overlay zones. This parameter is only enabled if
          at least one of the input and overlay zones is a raster.
      correspondence_overlay_to_input {Feature Class}:
          The output polygon features containing the correspondence measures of
          the overlay zones within the input zones.This output will have the
          same geometry as the input zones and can be
          used to identify which input zones closely correspond overall to the
          overlay zones. Specific zone combinations can then be investigated
          with the output features. This parameter is only enabled if the input
          and overlay zones are both polygon features.
      correspondence_input_to_overlay {Feature Class}:
          The output polygon features containing the correspondence measures of
          the input zones within the overlay zones.This output will have the
          same geometry as the overlay zones and can
          be used to identify which overlay zones closely correspond overall to
          the input zones. Specific zone combinations can then be investigated
          with the output features. This parameter is only enabled if the input
          and overlay zones are both polygon features."""
    ...

@gptooldoc('CalculateAreas_stats', None)
def CalculateAreas(Input_Feature_Class=..., Output_Feature_Class=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateAreas_stats(Input_Feature_Class, Output_Feature_Class)

        ‌

     INPUTS:
      Input_Feature_Class (Feature Layer):
          Input Feature Class

     OUTPUTS:
      Output_Feature_Class (Feature Class):
          Output Feature Class"""
    ...

@gptooldoc('CalculateDistanceBand_stats', None)
def CalculateDistanceBand(Input_Features=..., Neighbors=..., Distance_Method=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CalculateDistanceBand_stats(Input_Features, Neighbors, Distance_Method)

        Returns the minimum, the maximum, and the average distance to the
        specified Nth nearest neighbor (N is an input parameter) for a set of
        features. Results are written as tool execution messages.

     INPUTS:
      Input_Features (Feature Layer):
          The feature class or layer used to calculate distance statistics.
      Neighbors (Long):
          The number of neighbors (N) to consider for each feature. This number
          should be any integer between one and the total number of features in
          the feature class. A list of distances between each feature and its
          Nth neighbor is compiled, and the maximum, minimum, and average
          distances are output to the Results window.
      Distance_Method (String):
          Specifies how distances are calculated from each feature to
          neighboring features.

          * EUCLIDEAN_DISTANCE-The straight-line distance between two points (as
          the crow flies)

          * MANHATTAN_DISTANCE-The distance between two points measured along
          axes at right angles (city block); calculated by summing the
          (absolute) difference between the x- and y-coordinates"""
    ...

@gptooldoc('CollectEvents_stats', None)
def CollectEvents(Input_Incident_Features=..., Output_Weighted_Point_Feature_Class=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """CollectEvents_stats(Input_Incident_Features, Output_Weighted_Point_Feature_Class)

        Converts event data, such as crime or disease incidents, to weighted
        point data.

     INPUTS:
      Input_Incident_Features (Feature Layer):
          The features representing event or incident data.

     OUTPUTS:
      Output_Weighted_Point_Feature_Class (Feature Class):
          The output feature class that will contain the weighted point data."""
    ...

@gptooldoc('ConvertSpatialWeightsMatrixtoTable_stats', None)
def ConvertSpatialWeightsMatrixtoTable(Input_Spatial_Weights_Matrix_File=..., Output_Table=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ConvertSpatialWeightsMatrixtoTable_stats(Input_Spatial_Weights_Matrix_File, Output_Table)

        Converts a binary spatial weights matrix file (.swm) to a table.

     INPUTS:
      Input_Spatial_Weights_Matrix_File (File):
          The full pathname for the spatial weights matrix file (.swm) you want
          to convert.

     OUTPUTS:
      Output_Table (Table):
          A full pathname to the table you want to create."""
    ...

@gptooldoc('DescribeSSMFile_stats', None)
def DescribeSSMFile(input_model=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DescribeSSMFile_stats(input_model)

        Describes the contents and diagnostics of a spatial statistics model
        file.

     INPUTS:
      input_model (File):
          The spatial statistics model file that will be described."""
    ...

@gptooldoc('DimensionReduction_stats', None)
def DimensionReduction(in_table=..., output_data=..., fields=..., method=..., scale=..., categorical_field=..., min_variance=..., min_components=..., append_fields=..., output_eigenvalues_table=..., output_eigenvectors_table=..., number_of_permutations=..., append_to_input=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """DimensionReduction_stats(in_table, {output_data}, fields;fields..., {method}, {scale}, {categorical_field}, {min_variance}, {min_components}, {append_fields}, {output_eigenvalues_table}, {output_eigenvectors_table}, {number_of_permutations}, {append_to_input})

        Reduces the number of dimensions of a set of continuous variables by
        aggregating the highest possible amount of variance into fewer
        components using Principal Component Analysis (PCA) or Reduced-Rank
        Linear Discriminant Analysis (LDA).

     INPUTS:
      in_table (Table View):
          The table or features containing the fields with the dimension that
          will be reduced.
      fields (Field):
          The fields representing the data with the dimension that will be
          reduced.
      method {String}:
          Specifies the method that will be used to reduce the dimensions of the
          analysis fields.

          * PCA-The analysis fields will be partitioned into components that
          each maintain the maximum proportion of the total variance. This is
          the default.

          * LDA-The analysis fields will be partitioned into components that
          each maintain the maximum between-category separability of a
          categorical variable.
      scale {Boolean}:
          Specifies whether the values of each analysis will be scaled to have a
          variance equal to one. This scaling ensures that each analysis field
          is given equal priority in the components. Scaling also removes the
          effect of linear units; for example, the same data measured in meters
          and feet will result in equivalent components. The values of the
          analysis fields will be shifted to have mean zero for both options.

          * SCALE_DATA-The values of each analysis field will be scaled to have
          a variance of one by dividing each value by the standard deviation of
          the analysis field. This is the default.

          * NO_SCALE_DATA-The variance of each analysis field will not be
          scaled.
      categorical_field {Field}:
          The field representing the categorical variable for LDA. The
          components will maintain the maximum amount of information needed to
          classify each input record into these categories.
      min_variance {Double}:
          The minimum percent of total variance of the analysis fields that must
          be maintained in the components. The total variance depends on whether
          the analysis fields were scaled using the Scale Data parameter(scale
          in Python).
      min_components {Long}:
          The minimum number of components.
      append_fields {Boolean}:
          Specifies whether all fields from the input table or features will be
          copied and appended to the output table or feature class. The fields
          provided in the fields parameter will be copied to the output
          regardless of the value of this parameter.

          * APPEND-All fields from the input table or features will be copied
          and appended to the output table or feature class.

          * NO_APPEND-Only the analysis fields will be included in the output
          table or feature class. This is the default.
      number_of_permutations {Long}:
          The number of permutations to be used when determining the optimal
          number of components. The default value is 0, which indicates that no
          permutation test will be performed. The provided value must be equal
          to 0, 99, 199, 499, or 999. If any other value is provided, 0 will be
          used and no permutation test will be performed.
      append_to_input {Boolean}:
          Specifies whether the component fields will be appended to the input
          dataset or saved to an output table or feature class. If you append
          the fields to the input, the output coordinate system environment will
          be ignored.

          * APPEND_TO_INPUT-The fields containing the components will be
          appended to the input features. This option modifies the input data.

          * NEW_OUTPUT-An output table or feature class will be created
          containing the component fields. This is the default.

     OUTPUTS:
      output_data {Table}:
          The output table or feature class containing the resulting components
          of the dimension reduction.
      output_eigenvalues_table {Table}:
          The output table containing the eigenvalues of each component.
      output_eigenvectors_table {Table}:
          The output table containing the eigenvectors of each component."""
    ...

@gptooldoc('ExportXYv_stats', None)
def ExportXYv(Input_Feature_Class=..., Value_Field=..., Delimiter=..., Output_ASCII_File=..., Add_Field_Names_to_Output=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """ExportXYv_stats(Input_Feature_Class, Value_Field;Value_Field..., Delimiter, Output_ASCII_File, Add_Field_Names_to_Output)

        Exports feature class coordinates and attribute values to a space-,
        comma-, tab-, or semicolon-delimited ASCII text file.

     INPUTS:
      Input_Feature_Class (Feature Layer):
          The feature class from which the feature coordinates and attribute
          values will be exported.
      Value_Field (Field):
          The field or fields in the input feature class containing the values
          to export to an ASCII text file.
      Delimiter (String):
          Specifies how feature coordinates and attribute values will be
          separated in the output ASCII file.

          * SPACE-Feature coordinates and attribute values will be separated by
          a space in the output. This is the default.

          * COMMA-Feature coordinates and attribute values will be separated by
          a comma in the output.

          * SEMI-COLON-Feature coordinates and attribute values will be
          separated by a semicolon in the output.

          * TAB-Feature coordinates and attribute values will be separated by a
          tab in the output.
      Add_Field_Names_to_Output (Boolean):
          Specifies whether field names will be included as the first line in
          the output text file.

          * ADD_FIELD_NAMES-Field names will be written to the output text file.

          * NO_FIELD_NAMES-Field names will not be written to the output text
          file. This is the default.

     OUTPUTS:
      Output_ASCII_File (File):
          The ASCII text file that will contain the feature coordinates and
          attribute values."""
    ...

@gptooldoc('SetSSMFileProperties_stats', None)
def SetSSMFileProperties(input_model=..., variable_predict=..., explanatory_variables=..., distance_features=..., explanatory_rasters=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """SetSSMFileProperties_stats(input_model, {variable_predict;variable_predict...}, {explanatory_variables;explanatory_variables...}, {distance_features;distance_features...}, {explanatory_rasters;explanatory_rasters...})

        Adds descriptions and units to the variables stored in a spatial
        statistics model file.

     INPUTS:
      input_model (File):
          The spatial statistics model file.
      variable_predict {Value Table}:
          The name, description, and unit of the variable that will be predicted
          at new locations.
      explanatory_variables {Value Table}:
          The name, description, and unit of the explanatory variables that will
          be used to train the input model.
      distance_features {Value Table}:
          The name, description, and unit of the explanatory training distance
          features that will be used to train the input model.
      explanatory_rasters {Value Table}:
          The name, description, and unit of the explanatory training rasters
          that will be used to train the input model."""
    ...

@gptooldoc('TimeSeriesSmoothing_stats', None)
def TimeSeriesSmoothing(in_features=..., time_field=..., analysis_field=..., group_method=..., method=..., time_window=..., append_to_input=..., output_features=..., id_field=..., apply_shorter_window=..., enable_time_series_popups=...): # -> conversion | int | float | complex | basestring | list[Unknown] | tuple[Unknown, ...] | dict[Unknown, Unknown]:
    """TimeSeriesSmoothing_stats(in_features, time_field, analysis_field, {group_method}, {method}, {time_window}, {append_to_input}, {output_features}, {id_field}, {apply_shorter_window}, {enable_time_series_popups})

        Smooths a numeric variable of one or more time series using centered,
        forward, and backward moving averages, as well as an adaptive method
        based on local linear regression. After smoothing short-term
        fluctuations, longer-term trends or cycles often become apparent.

     INPUTS:
      in_features (Table View):
          The features or table containing the time series data and the field to
          smooth.
      time_field (Field):
          The field containing the time of each record.
      analysis_field (Field):
          The field containing the values that will be smoothed.
      group_method {String}:
          Specifies the method that will be used to group records into different
          time series. Smoothing is performed independently for each time
          series.

          * LOCATION-Features at the same location will be grouped in the same
          time series. This is the default.

          * ID_FIELD-Records with the same value of the ID field will be grouped
          in the same time series.

          * NONE-All records will be in the same time series.
      method {String}:
          Specifies the smoothing method that will be used.

          * BACKWARD-The smoothed value is the average of the record and the
          values within the time window before it. This is the default.

          * CENTERED-The smoothed value is the average of the record and the
          values before and after it. Half of the time window is used before the
          time of the record, and half is used after.

          * FORWARD-The smoothed value is the average of the record and the
          values within the time window after it.

          * ADAPTIVE-The smoothed value is the result of a local linear
          regression centered at the record. The size of the time window is
          optimized for each record.
      time_window {Time Unit}:
          The length of the time window. The value can be provided in seconds,
          minutes, hours, days, weeks, months, or years. For backward, forward,
          and centered moving averages, the value and unit must be provided. For
          adaptive bandwidth local linear regression, the value can be left
          empty and a time window will be estimated independently for each
          value. Values that fall on the border of the time window are included
          within the window. For example, if you have daily data and you use a
          backward moving average with a time window of four days, five values
          will be included in the window when smoothing a record: the value of
          the record and the values of the four previous days.
      append_to_input {Boolean}:
          Specifies whether the output fields will be appended to the input
          dataset or saved as a new output table or feature class. If you choose
          to append the fields to the input, the output coordinate system
          environment will be ignored.

          * APPEND_TO_INPUT-The output fields will be appended to the input
          features. This option modifies the input data.

          * NEW_OUTPUT-The output fields will not be appended to the input. An
          output table or a feature class will be created containing the output
          fields. This is the default.
      id_field {Field}:
          The integer or text field containing a unique ID for each time series.
          All records with the same value of this field are part of the same
          time series.
      apply_shorter_window {Boolean}:
          Specifies whether the time window will be shortened at the beginning
          and end of each time series.

          * APPLY_SHORTER_WINDOW-The time window will be shortened at the start
          and end of the time series so that the time window does not extend
          before the start or after the end of the time series.

          * NOT_APPLY_SHORTER_WINDOW-The time window will not be shortened. If
          the time window extends before the start or after the end of the time
          series, the smoothed value will be null. This is the default.
      enable_time_series_popups {Boolean}:
          Specifies whether the output features or table will include pop-up
          charts showing the original and smoothed values of the time series.

          * CREATE_POPUP-The output will include pop-up charts. This is the
          default.

          * NO_POPUP-The output will not include pop-up charts.

     OUTPUTS:
      output_features {Table}:
          The output features containing the smoothed values as well as fields
          for the time window and number of neighbors."""
    ...

